{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lenovo e560\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import BaggingRegressor, AdaBoostRegressor,GradientBoostingRegressor, RandomForestRegressor \n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('ENB2012_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 10 columns):\n",
      "X1    768 non-null float64\n",
      "X2    768 non-null float64\n",
      "X3    768 non-null float64\n",
      "X4    768 non-null float64\n",
      "X5    768 non-null float64\n",
      "X6    768 non-null int64\n",
      "X7    768 non-null float64\n",
      "X8    768 non-null int64\n",
      "Y1    768 non-null float64\n",
      "Y2    768 non-null float64\n",
      "dtypes: float64(8), int64(2)\n",
      "memory usage: 60.1 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>Y1</th>\n",
       "      <th>Y2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.00000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.00000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.764167</td>\n",
       "      <td>671.708333</td>\n",
       "      <td>318.500000</td>\n",
       "      <td>176.604167</td>\n",
       "      <td>5.25000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.234375</td>\n",
       "      <td>2.81250</td>\n",
       "      <td>22.307201</td>\n",
       "      <td>24.587760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.105777</td>\n",
       "      <td>88.086116</td>\n",
       "      <td>43.626481</td>\n",
       "      <td>45.165950</td>\n",
       "      <td>1.75114</td>\n",
       "      <td>1.118763</td>\n",
       "      <td>0.133221</td>\n",
       "      <td>1.55096</td>\n",
       "      <td>10.090196</td>\n",
       "      <td>9.513306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.620000</td>\n",
       "      <td>514.500000</td>\n",
       "      <td>245.000000</td>\n",
       "      <td>110.250000</td>\n",
       "      <td>3.50000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>6.010000</td>\n",
       "      <td>10.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.682500</td>\n",
       "      <td>606.375000</td>\n",
       "      <td>294.000000</td>\n",
       "      <td>140.875000</td>\n",
       "      <td>3.50000</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.75000</td>\n",
       "      <td>12.992500</td>\n",
       "      <td>15.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>673.750000</td>\n",
       "      <td>318.500000</td>\n",
       "      <td>183.750000</td>\n",
       "      <td>5.25000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>18.950000</td>\n",
       "      <td>22.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.830000</td>\n",
       "      <td>741.125000</td>\n",
       "      <td>343.000000</td>\n",
       "      <td>220.500000</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>31.667500</td>\n",
       "      <td>33.132500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.980000</td>\n",
       "      <td>808.500000</td>\n",
       "      <td>416.500000</td>\n",
       "      <td>220.500000</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>43.100000</td>\n",
       "      <td>48.030000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               X1          X2          X3          X4         X5          X6  \\\n",
       "count  768.000000  768.000000  768.000000  768.000000  768.00000  768.000000   \n",
       "mean     0.764167  671.708333  318.500000  176.604167    5.25000    3.500000   \n",
       "std      0.105777   88.086116   43.626481   45.165950    1.75114    1.118763   \n",
       "min      0.620000  514.500000  245.000000  110.250000    3.50000    2.000000   \n",
       "25%      0.682500  606.375000  294.000000  140.875000    3.50000    2.750000   \n",
       "50%      0.750000  673.750000  318.500000  183.750000    5.25000    3.500000   \n",
       "75%      0.830000  741.125000  343.000000  220.500000    7.00000    4.250000   \n",
       "max      0.980000  808.500000  416.500000  220.500000    7.00000    5.000000   \n",
       "\n",
       "               X7         X8          Y1          Y2  \n",
       "count  768.000000  768.00000  768.000000  768.000000  \n",
       "mean     0.234375    2.81250   22.307201   24.587760  \n",
       "std      0.133221    1.55096   10.090196    9.513306  \n",
       "min      0.000000    0.00000    6.010000   10.900000  \n",
       "25%      0.100000    1.75000   12.992500   15.620000  \n",
       "50%      0.250000    3.00000   18.950000   22.080000  \n",
       "75%      0.400000    4.00000   31.667500   33.132500  \n",
       "max      0.400000    5.00000   43.100000   48.030000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>Y1</th>\n",
       "      <th>Y2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.90</td>\n",
       "      <td>563.5</td>\n",
       "      <td>318.5</td>\n",
       "      <td>122.50</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>20.84</td>\n",
       "      <td>28.28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     X1     X2     X3      X4   X5  X6   X7  X8     Y1     Y2\n",
       "0  0.98  514.5  294.0  110.25  7.0   2  0.0   0  15.55  21.33\n",
       "1  0.98  514.5  294.0  110.25  7.0   3  0.0   0  15.55  21.33\n",
       "2  0.98  514.5  294.0  110.25  7.0   4  0.0   0  15.55  21.33\n",
       "3  0.98  514.5  294.0  110.25  7.0   5  0.0   0  15.55  21.33\n",
       "4  0.90  563.5  318.5  122.50  7.0   2  0.0   0  20.84  28.28"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing & Feature Selection(OLS)\n",
    "\n",
    "There are two target variables: y1 for heating load, and y2 for cooling load. Two set of feature variables are used accordingly.\n",
    "\n",
    "Using statistics summary, insignificant variables are detected by P-value. For heating load, X6 is removed. For cooling load, both X6 and X8 are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X= data.iloc[:,0:8]\n",
    "y1=data['Y1']\n",
    "y2=data['Y2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y=y1+y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.90</td>\n",
       "      <td>563.5</td>\n",
       "      <td>318.5</td>\n",
       "      <td>122.50</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     X1     X2     X3      X4   X5  X6   X7  X8\n",
       "0  0.98  514.5  294.0  110.25  7.0   2  0.0   0\n",
       "1  0.98  514.5  294.0  110.25  7.0   3  0.0   0\n",
       "2  0.98  514.5  294.0  110.25  7.0   4  0.0   0\n",
       "3  0.98  514.5  294.0  110.25  7.0   5  0.0   0\n",
       "4  0.90  563.5  318.5  122.50  7.0   2  0.0   0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                     Y1   R-squared:                       0.916\n",
      "Model:                            OLS   Adj. R-squared:                  0.915\n",
      "Method:                 Least Squares   F-statistic:                     1187.\n",
      "Date:                Wed, 25 Apr 2018   Prob (F-statistic):               0.00\n",
      "Time:                        17:52:51   Log-Likelihood:                -1912.5\n",
      "No. Observations:                 768   AIC:                             3841.\n",
      "Df Residuals:                     760   BIC:                             3878.\n",
      "Df Model:                           7                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         84.0145     19.034      4.414      0.000      46.650     121.379\n",
      "X1           -64.7740     10.289     -6.295      0.000     -84.973     -44.575\n",
      "X2            -0.0626      0.013     -4.670      0.000      -0.089      -0.036\n",
      "X3             0.0361      0.004      9.386      0.000       0.029       0.044\n",
      "X4            -0.0494      0.008     -6.569      0.000      -0.064      -0.035\n",
      "X5             4.1699      0.338     12.337      0.000       3.506       4.833\n",
      "X6            -0.0233      0.095     -0.246      0.805      -0.209       0.163\n",
      "X7            19.9327      0.814     24.488      0.000      18.335      21.531\n",
      "X8             0.2038      0.070      2.914      0.004       0.067       0.341\n",
      "==============================================================================\n",
      "Omnibus:                       18.648   Durbin-Watson:                   0.654\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               37.708\n",
      "Skew:                           0.044   Prob(JB):                     6.48e-09\n",
      "Kurtosis:                       4.082   Cond. No.                     2.85e+16\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 5.61e-25. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "Xtemp = sm.add_constant(X)\n",
    "est = sm.OLS(y1, Xtemp)\n",
    "est = est.fit()\n",
    "print(est.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                     Y2   R-squared:                       0.888\n",
      "Model:                            OLS   Adj. R-squared:                  0.887\n",
      "Method:                 Least Squares   F-statistic:                     859.1\n",
      "Date:                Wed, 25 Apr 2018   Prob (F-statistic):               0.00\n",
      "Time:                        17:52:51   Log-Likelihood:                -1979.3\n",
      "No. Observations:                 768   AIC:                             3975.\n",
      "Df Residuals:                     760   BIC:                             4012.\n",
      "Df Model:                           7                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         97.2457     20.765      4.683      0.000      56.483     138.009\n",
      "X1           -70.7877     11.225     -6.306      0.000     -92.824     -48.751\n",
      "X2            -0.0661      0.015     -4.519      0.000      -0.095      -0.037\n",
      "X3             0.0225      0.004      5.365      0.000       0.014       0.031\n",
      "X4            -0.0443      0.008     -5.404      0.000      -0.060      -0.028\n",
      "X5             4.2838      0.369     11.618      0.000       3.560       5.008\n",
      "X6             0.1215      0.103      1.176      0.240      -0.081       0.324\n",
      "X7            14.7171      0.888     16.573      0.000      12.974      16.460\n",
      "X8             0.0407      0.076      0.534      0.594      -0.109       0.190\n",
      "==============================================================================\n",
      "Omnibus:                      104.668   Durbin-Watson:                   1.094\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              230.547\n",
      "Skew:                           0.767   Prob(JB):                     8.65e-51\n",
      "Kurtosis:                       5.203   Cond. No.                     2.85e+16\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 5.61e-25. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "est2 = sm.OLS(y2, Xtemp)\n",
    "est2 = est2.fit()\n",
    "print(est2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X1= X.drop(['X6'], axis=1)\n",
    "X2= X.drop(['X6','X8'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.90</td>\n",
       "      <td>563.5</td>\n",
       "      <td>318.5</td>\n",
       "      <td>122.50</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     X1     X2     X3      X4   X5   X7  X8\n",
       "0  0.98  514.5  294.0  110.25  7.0  0.0   0\n",
       "1  0.98  514.5  294.0  110.25  7.0  0.0   0\n",
       "2  0.98  514.5  294.0  110.25  7.0  0.0   0\n",
       "3  0.98  514.5  294.0  110.25  7.0  0.0   0\n",
       "4  0.90  563.5  318.5  122.50  7.0  0.0   0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.90</td>\n",
       "      <td>563.5</td>\n",
       "      <td>318.5</td>\n",
       "      <td>122.50</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     X1     X2     X3      X4   X5   X7\n",
       "0  0.98  514.5  294.0  110.25  7.0  0.0\n",
       "1  0.98  514.5  294.0  110.25  7.0  0.0\n",
       "2  0.98  514.5  294.0  110.25  7.0  0.0\n",
       "3  0.98  514.5  294.0  110.25  7.0  0.0\n",
       "4  0.90  563.5  318.5  122.50  7.0  0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Regression- Deep Learning  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense \n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "np.random.seed(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heating Load Y1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Keras GridSearch \n",
    "GridSearchCV is used to find the best parameters for Deep learning model( Epoch 5, batch size 700). \n",
    "\n",
    "R-squared and prediction are required outputs from such configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1,y1, random_state = 10)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "def create_model():\n",
    "    #create model\n",
    "    model = Sequential() \n",
    "    model.add(Dense(7, input_dim=7, kernel_initializer='normal', activation='relu')) # by default, kernel_init has uniform weights\n",
    "    model.add(Dense(5, kernel_initializer='uniform', activation='sigmoid')) # hidden layer\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    #compile model\n",
    "    model.compile(loss='mse', optimizer='adam' , metrics = ['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = {'epochs':[500,700] , 'batch_size':[5,20]}\n",
    "keras_regressor = KerasRegressor(build_fn = create_model , verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 36min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "grid_search = GridSearchCV(keras_regressor, param_grid , cv=5, scoring='r2' )\n",
    "grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'batch_size': 5, 'epochs': 700}\n",
      "Best cross-validation score: 0.915546\n",
      "Best estimator:\n",
      "<keras.wrappers.scikit_learn.KerasRegressor object at 0x0000024763CC1FD0>\n",
      "0.922134354642 0.940054934711\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters: {}\".format(grid_search.best_params_)) \n",
    "print(\"Best cross-validation score: {:.6f}\".format(grid_search.best_score_)) \n",
    "print(\"Best estimator:\\n{}\".format(grid_search.best_estimator_))\n",
    "\n",
    "train_score = grid_search.score(X_train,y_train)\n",
    "test_score = grid_search.score(X_test,y_test)\n",
    "print(train_score, test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prediction of Heating Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create model\n",
    "model = Sequential() \n",
    "model.add(Dense(7, input_dim=7, kernel_initializer='normal', activation='relu')) # by default, kernel_init has uniform weights\n",
    "model.add(Dense(5, kernel_initializer='uniform', activation='sigmoid')) # hidden layer\n",
    "model.add(Dense(1, kernel_initializer='normal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile model \n",
    "model.compile(loss='mse', optimizer='adam' , metrics = ['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/700\n",
      "576/576 [==============================] - 6s 10ms/step - loss: 605.3467 - mean_squared_error: 605.3467\n",
      "Epoch 2/700\n",
      "576/576 [==============================] - 0s 674us/step - loss: 576.2592 - mean_squared_error: 576.2592\n",
      "Epoch 3/700\n",
      "576/576 [==============================] - 0s 653us/step - loss: 536.0256 - mean_squared_error: 536.0256\n",
      "Epoch 4/700\n",
      "576/576 [==============================] - 0s 660us/step - loss: 501.5971 - mean_squared_error: 501.5971\n",
      "Epoch 5/700\n",
      "576/576 [==============================] - 0s 654us/step - loss: 472.0968 - mean_squared_error: 472.0968\n",
      "Epoch 6/700\n",
      "576/576 [==============================] - 0s 661us/step - loss: 445.3707 - mean_squared_error: 445.3707\n",
      "Epoch 7/700\n",
      "576/576 [==============================] - 0s 651us/step - loss: 420.8596 - mean_squared_error: 420.8596\n",
      "Epoch 8/700\n",
      "576/576 [==============================] - 0s 663us/step - loss: 397.9891 - mean_squared_error: 397.9891\n",
      "Epoch 9/700\n",
      "576/576 [==============================] - 0s 672us/step - loss: 376.5553 - mean_squared_error: 376.5553\n",
      "Epoch 10/700\n",
      "576/576 [==============================] - 0s 678us/step - loss: 356.3166 - mean_squared_error: 356.3166\n",
      "Epoch 11/700\n",
      "576/576 [==============================] - 0s 678us/step - loss: 337.4436 - mean_squared_error: 337.4436\n",
      "Epoch 12/700\n",
      "576/576 [==============================] - 0s 684us/step - loss: 319.7153 - mean_squared_error: 319.7153\n",
      "Epoch 13/700\n",
      "576/576 [==============================] - 0s 675us/step - loss: 303.0166 - mean_squared_error: 303.0166\n",
      "Epoch 14/700\n",
      "576/576 [==============================] - 0s 646us/step - loss: 287.2173 - mean_squared_error: 287.2173\n",
      "Epoch 15/700\n",
      "576/576 [==============================] - 0s 672us/step - loss: 272.4420 - mean_squared_error: 272.4420\n",
      "Epoch 16/700\n",
      "576/576 [==============================] - 0s 655us/step - loss: 258.6073 - mean_squared_error: 258.6073\n",
      "Epoch 17/700\n",
      "576/576 [==============================] - 0s 684us/step - loss: 245.4747 - mean_squared_error: 245.4747\n",
      "Epoch 18/700\n",
      "576/576 [==============================] - 0s 665us/step - loss: 233.1955 - mean_squared_error: 233.1955\n",
      "Epoch 19/700\n",
      "576/576 [==============================] - 0s 663us/step - loss: 221.8182 - mean_squared_error: 221.8182\n",
      "Epoch 20/700\n",
      "576/576 [==============================] - 0s 661us/step - loss: 211.1779 - mean_squared_error: 211.1779\n",
      "Epoch 21/700\n",
      "576/576 [==============================] - 0s 677us/step - loss: 201.2725 - mean_squared_error: 201.2725\n",
      "Epoch 22/700\n",
      "576/576 [==============================] - 0s 670us/step - loss: 191.9217 - mean_squared_error: 191.9217\n",
      "Epoch 23/700\n",
      "576/576 [==============================] - 0s 651us/step - loss: 183.3622 - mean_squared_error: 183.3622\n",
      "Epoch 24/700\n",
      "576/576 [==============================] - 0s 641us/step - loss: 175.2764 - mean_squared_error: 175.2764\n",
      "Epoch 25/700\n",
      "576/576 [==============================] - 0s 653us/step - loss: 167.9139 - mean_squared_error: 167.9139\n",
      "Epoch 26/700\n",
      "576/576 [==============================] - 0s 646us/step - loss: 161.0210 - mean_squared_error: 161.0210\n",
      "Epoch 27/700\n",
      "576/576 [==============================] - 0s 649us/step - loss: 154.6929 - mean_squared_error: 154.6929\n",
      "Epoch 28/700\n",
      "576/576 [==============================] - 0s 639us/step - loss: 148.9605 - mean_squared_error: 148.9605\n",
      "Epoch 29/700\n",
      "576/576 [==============================] - 0s 642us/step - loss: 143.6244 - mean_squared_error: 143.6244\n",
      "Epoch 30/700\n",
      "576/576 [==============================] - 0s 660us/step - loss: 138.8098 - mean_squared_error: 138.8098\n",
      "Epoch 31/700\n",
      "576/576 [==============================] - 0s 667us/step - loss: 134.4339 - mean_squared_error: 134.4339\n",
      "Epoch 32/700\n",
      "576/576 [==============================] - 0s 642us/step - loss: 130.3726 - mean_squared_error: 130.3726\n",
      "Epoch 33/700\n",
      "576/576 [==============================] - 0s 676us/step - loss: 126.7387 - mean_squared_error: 126.7387\n",
      "Epoch 34/700\n",
      "576/576 [==============================] - 0s 743us/step - loss: 123.4296 - mean_squared_error: 123.4296\n",
      "Epoch 35/700\n",
      "576/576 [==============================] - 0s 775us/step - loss: 120.4749 - mean_squared_error: 120.4749\n",
      "Epoch 36/700\n",
      "576/576 [==============================] - 0s 722us/step - loss: 116.4946 - mean_squared_error: 116.4946\n",
      "Epoch 37/700\n",
      "576/576 [==============================] - 0s 646us/step - loss: 104.1523 - mean_squared_error: 104.1523\n",
      "Epoch 38/700\n",
      "576/576 [==============================] - 0s 665us/step - loss: 93.7529 - mean_squared_error: 93.7529\n",
      "Epoch 39/700\n",
      "576/576 [==============================] - 0s 650us/step - loss: 87.1664 - mean_squared_error: 87.1664\n",
      "Epoch 40/700\n",
      "576/576 [==============================] - 0s 627us/step - loss: 81.4598 - mean_squared_error: 81.4598\n",
      "Epoch 41/700\n",
      "576/576 [==============================] - 0s 614us/step - loss: 76.2901 - mean_squared_error: 76.2901\n",
      "Epoch 42/700\n",
      "576/576 [==============================] - 0s 628us/step - loss: 71.4696 - mean_squared_error: 71.4696\n",
      "Epoch 43/700\n",
      "576/576 [==============================] - 0s 677us/step - loss: 67.0279 - mean_squared_error: 67.0279\n",
      "Epoch 44/700\n",
      "576/576 [==============================] - 0s 628us/step - loss: 62.8077 - mean_squared_error: 62.8077\n",
      "Epoch 45/700\n",
      "576/576 [==============================] - 0s 602us/step - loss: 58.9309 - mean_squared_error: 58.9309\n",
      "Epoch 46/700\n",
      "576/576 [==============================] - 0s 644us/step - loss: 55.2760 - mean_squared_error: 55.2760\n",
      "Epoch 47/700\n",
      "576/576 [==============================] - 0s 642us/step - loss: 51.8359 - mean_squared_error: 51.8359\n",
      "Epoch 48/700\n",
      "576/576 [==============================] - 0s 661us/step - loss: 48.7188 - mean_squared_error: 48.7188\n",
      "Epoch 49/700\n",
      "576/576 [==============================] - 0s 642us/step - loss: 45.8127 - mean_squared_error: 45.8127\n",
      "Epoch 50/700\n",
      "576/576 [==============================] - 0s 605us/step - loss: 43.1210 - mean_squared_error: 43.1210\n",
      "Epoch 51/700\n",
      "576/576 [==============================] - 0s 631us/step - loss: 40.6475 - mean_squared_error: 40.6475\n",
      "Epoch 52/700\n",
      "576/576 [==============================] - 0s 604us/step - loss: 38.3722 - mean_squared_error: 38.3722\n",
      "Epoch 53/700\n",
      "576/576 [==============================] - 0s 653us/step - loss: 36.2655 - mean_squared_error: 36.2655\n",
      "Epoch 54/700\n",
      "576/576 [==============================] - 0s 611us/step - loss: 34.3292 - mean_squared_error: 34.3292\n",
      "Epoch 55/700\n",
      "576/576 [==============================] - 0s 566us/step - loss: 32.5841 - mean_squared_error: 32.5841\n",
      "Epoch 56/700\n",
      "576/576 [==============================] - 0s 606us/step - loss: 30.9909 - mean_squared_error: 30.9909\n",
      "Epoch 57/700\n",
      "576/576 [==============================] - 0s 620us/step - loss: 29.5706 - mean_squared_error: 29.5706\n",
      "Epoch 58/700\n",
      "576/576 [==============================] - 0s 573us/step - loss: 28.2773 - mean_squared_error: 28.2773\n",
      "Epoch 59/700\n",
      "576/576 [==============================] - 0s 601us/step - loss: 27.1178 - mean_squared_error: 27.1178\n",
      "Epoch 60/700\n",
      "576/576 [==============================] - 0s 615us/step - loss: 26.0982 - mean_squared_error: 26.0982\n",
      "Epoch 61/700\n",
      "576/576 [==============================] - 0s 646us/step - loss: 25.2077 - mean_squared_error: 25.2077\n",
      "Epoch 62/700\n",
      "576/576 [==============================] - 0s 601us/step - loss: 24.4255 - mean_squared_error: 24.4255\n",
      "Epoch 63/700\n",
      "576/576 [==============================] - 0s 608us/step - loss: 23.7246 - mean_squared_error: 23.7246\n",
      "Epoch 64/700\n",
      "576/576 [==============================] - 0s 621us/step - loss: 23.1123 - mean_squared_error: 23.1123\n",
      "Epoch 65/700\n",
      "576/576 [==============================] - 0s 623us/step - loss: 22.5601 - mean_squared_error: 22.5601\n",
      "Epoch 66/700\n",
      "576/576 [==============================] - 0s 611us/step - loss: 22.0843 - mean_squared_error: 22.0843\n",
      "Epoch 67/700\n",
      "576/576 [==============================] - 0s 621us/step - loss: 21.7227 - mean_squared_error: 21.7227\n",
      "Epoch 68/700\n",
      "576/576 [==============================] - 0s 613us/step - loss: 21.3752 - mean_squared_error: 21.3752\n",
      "Epoch 69/700\n",
      "576/576 [==============================] - 0s 587us/step - loss: 21.1206 - mean_squared_error: 21.1206\n",
      "Epoch 70/700\n",
      "576/576 [==============================] - 0s 594us/step - loss: 20.8375 - mean_squared_error: 20.8375\n",
      "Epoch 71/700\n",
      "576/576 [==============================] - 0s 639us/step - loss: 20.6510 - mean_squared_error: 20.6510\n",
      "Epoch 72/700\n",
      "576/576 [==============================] - 0s 614us/step - loss: 20.5164 - mean_squared_error: 20.5164\n",
      "Epoch 73/700\n",
      "576/576 [==============================] - 0s 617us/step - loss: 20.3617 - mean_squared_error: 20.3617\n",
      "Epoch 74/700\n",
      "576/576 [==============================] - 0s 620us/step - loss: 20.2376 - mean_squared_error: 20.2376\n",
      "Epoch 75/700\n",
      "576/576 [==============================] - 0s 632us/step - loss: 20.1578 - mean_squared_error: 20.1578\n",
      "Epoch 76/700\n",
      "576/576 [==============================] - 0s 629us/step - loss: 20.0793 - mean_squared_error: 20.0793\n",
      "Epoch 77/700\n",
      "576/576 [==============================] - 0s 668us/step - loss: 20.0257 - mean_squared_error: 20.0257\n",
      "Epoch 78/700\n",
      "576/576 [==============================] - 0s 703us/step - loss: 19.9906 - mean_squared_error: 19.9906\n",
      "Epoch 79/700\n",
      "576/576 [==============================] - 0s 722us/step - loss: 19.9225 - mean_squared_error: 19.9225\n",
      "Epoch 80/700\n",
      "576/576 [==============================] - 0s 684us/step - loss: 19.8933 - mean_squared_error: 19.8933\n",
      "Epoch 81/700\n",
      "576/576 [==============================] - 0s 602us/step - loss: 19.8720 - mean_squared_error: 19.8720\n",
      "Epoch 82/700\n",
      "576/576 [==============================] - 0s 616us/step - loss: 19.8467 - mean_squared_error: 19.8467\n",
      "Epoch 83/700\n",
      "576/576 [==============================] - 0s 667us/step - loss: 19.8251 - mean_squared_error: 19.8251\n",
      "Epoch 84/700\n",
      "576/576 [==============================] - 0s 677us/step - loss: 19.7979 - mean_squared_error: 19.7979\n",
      "Epoch 85/700\n",
      "576/576 [==============================] - 0s 625us/step - loss: 19.7764 - mean_squared_error: 19.7764\n",
      "Epoch 86/700\n",
      "576/576 [==============================] - 0s 614us/step - loss: 19.7524 - mean_squared_error: 19.7524\n",
      "Epoch 87/700\n",
      "576/576 [==============================] - 0s 608us/step - loss: 19.7257 - mean_squared_error: 19.7257\n",
      "Epoch 88/700\n",
      "576/576 [==============================] - 0s 620us/step - loss: 19.6946 - mean_squared_error: 19.6946\n",
      "Epoch 89/700\n",
      "576/576 [==============================] - 0s 601us/step - loss: 19.6821 - mean_squared_error: 19.6821\n",
      "Epoch 90/700\n",
      "576/576 [==============================] - 0s 608us/step - loss: 19.5747 - mean_squared_error: 19.5747\n",
      "Epoch 91/700\n",
      "576/576 [==============================] - 0s 602us/step - loss: 19.4785 - mean_squared_error: 19.4785\n",
      "Epoch 92/700\n",
      "576/576 [==============================] - 0s 605us/step - loss: 19.3078 - mean_squared_error: 19.3078\n",
      "Epoch 93/700\n",
      "576/576 [==============================] - 0s 607us/step - loss: 19.0702 - mean_squared_error: 19.0702\n",
      "Epoch 94/700\n",
      "576/576 [==============================] - 0s 609us/step - loss: 18.7971 - mean_squared_error: 18.7971\n",
      "Epoch 95/700\n",
      "576/576 [==============================] - 0s 604us/step - loss: 18.4772 - mean_squared_error: 18.4772\n",
      "Epoch 96/700\n",
      "576/576 [==============================] - 0s 588us/step - loss: 18.1667 - mean_squared_error: 18.1667\n",
      "Epoch 97/700\n",
      "576/576 [==============================] - 0s 614us/step - loss: 17.8819 - mean_squared_error: 17.8819\n",
      "Epoch 98/700\n",
      "576/576 [==============================] - 0s 590us/step - loss: 17.4868 - mean_squared_error: 17.4868\n",
      "Epoch 99/700\n",
      "576/576 [==============================] - 0s 601us/step - loss: 17.1081 - mean_squared_error: 17.1081\n",
      "Epoch 100/700\n",
      "576/576 [==============================] - 0s 594us/step - loss: 16.8262 - mean_squared_error: 16.8262\n",
      "Epoch 101/700\n",
      "576/576 [==============================] - 0s 597us/step - loss: 16.4383 - mean_squared_error: 16.4383\n",
      "Epoch 102/700\n",
      "576/576 [==============================] - 0s 594us/step - loss: 16.0896 - mean_squared_error: 16.0896\n",
      "Epoch 103/700\n",
      "576/576 [==============================] - 0s 609us/step - loss: 15.7267 - mean_squared_error: 15.7267\n",
      "Epoch 104/700\n",
      "576/576 [==============================] - 0s 601us/step - loss: 15.4712 - mean_squared_error: 15.4712\n",
      "Epoch 105/700\n",
      "576/576 [==============================] - 0s 595us/step - loss: 15.0502 - mean_squared_error: 15.0502\n",
      "Epoch 106/700\n",
      "576/576 [==============================] - 0s 578us/step - loss: 14.7104 - mean_squared_error: 14.7104\n",
      "Epoch 107/700\n",
      "576/576 [==============================] - 0s 601us/step - loss: 14.4257 - mean_squared_error: 14.4257\n",
      "Epoch 108/700\n",
      "576/576 [==============================] - 0s 581us/step - loss: 14.1298 - mean_squared_error: 14.1298\n",
      "Epoch 109/700\n",
      "576/576 [==============================] - 0s 590us/step - loss: 13.7869 - mean_squared_error: 13.7869\n",
      "Epoch 110/700\n",
      "576/576 [==============================] - 0s 661us/step - loss: 13.5567 - mean_squared_error: 13.5567\n",
      "Epoch 111/700\n",
      "576/576 [==============================] - 0s 645us/step - loss: 13.2834 - mean_squared_error: 13.2834\n",
      "Epoch 112/700\n",
      "576/576 [==============================] - 0s 599us/step - loss: 13.0304 - mean_squared_error: 13.0304\n",
      "Epoch 113/700\n",
      "576/576 [==============================] - 0s 608us/step - loss: 12.7711 - mean_squared_error: 12.7711\n",
      "Epoch 114/700\n",
      "576/576 [==============================] - 0s 574us/step - loss: 12.5262 - mean_squared_error: 12.5262\n",
      "Epoch 115/700\n",
      "576/576 [==============================] - 0s 574us/step - loss: 12.3026 - mean_squared_error: 12.3026\n",
      "Epoch 116/700\n",
      "576/576 [==============================] - 0s 578us/step - loss: 12.0557 - mean_squared_error: 12.0557\n",
      "Epoch 117/700\n",
      "576/576 [==============================] - 0s 576us/step - loss: 11.8024 - mean_squared_error: 11.8024\n",
      "Epoch 118/700\n",
      "576/576 [==============================] - 0s 581us/step - loss: 11.6500 - mean_squared_error: 11.6500\n",
      "Epoch 119/700\n",
      "576/576 [==============================] - 0s 581us/step - loss: 11.3312 - mean_squared_error: 11.3312\n",
      "Epoch 120/700\n",
      "576/576 [==============================] - 0s 580us/step - loss: 11.0061 - mean_squared_error: 11.0061\n",
      "Epoch 121/700\n",
      "576/576 [==============================] - 0s 581us/step - loss: 10.7103 - mean_squared_error: 10.7103\n",
      "Epoch 122/700\n",
      "576/576 [==============================] - 0s 604us/step - loss: 10.5128 - mean_squared_error: 10.5128\n",
      "Epoch 123/700\n",
      "576/576 [==============================] - 0s 707us/step - loss: 10.3377 - mean_squared_error: 10.3377\n",
      "Epoch 124/700\n",
      "576/576 [==============================] - 0s 684us/step - loss: 10.1993 - mean_squared_error: 10.1993\n",
      "Epoch 125/700\n",
      "576/576 [==============================] - 0s 688us/step - loss: 10.0571 - mean_squared_error: 10.0571\n",
      "Epoch 126/700\n",
      "576/576 [==============================] - 0s 582us/step - loss: 9.9029 - mean_squared_error: 9.9029\n",
      "Epoch 127/700\n",
      "576/576 [==============================] - 0s 574us/step - loss: 9.7778 - mean_squared_error: 9.7778\n",
      "Epoch 128/700\n",
      "576/576 [==============================] - 0s 574us/step - loss: 9.6834 - mean_squared_error: 9.6834\n",
      "Epoch 129/700\n",
      "576/576 [==============================] - 0s 571us/step - loss: 9.5611 - mean_squared_error: 9.5611\n",
      "Epoch 130/700\n",
      "576/576 [==============================] - 0s 583us/step - loss: 9.4525 - mean_squared_error: 9.4525\n",
      "Epoch 131/700\n",
      "576/576 [==============================] - 0s 585us/step - loss: 9.3731 - mean_squared_error: 9.3731\n",
      "Epoch 132/700\n",
      "576/576 [==============================] - 0s 569us/step - loss: 9.3103 - mean_squared_error: 9.3103\n",
      "Epoch 133/700\n",
      "576/576 [==============================] - 0s 661us/step - loss: 9.1922 - mean_squared_error: 9.1922\n",
      "Epoch 134/700\n",
      "576/576 [==============================] - 0s 568us/step - loss: 9.1665 - mean_squared_error: 9.1665\n",
      "Epoch 135/700\n",
      "576/576 [==============================] - 0s 562us/step - loss: 9.1388 - mean_squared_error: 9.1388\n",
      "Epoch 136/700\n",
      "576/576 [==============================] - 0s 573us/step - loss: 9.0172 - mean_squared_error: 9.0172\n",
      "Epoch 137/700\n",
      "576/576 [==============================] - 0s 557us/step - loss: 8.9923 - mean_squared_error: 8.9923\n",
      "Epoch 138/700\n",
      "576/576 [==============================] - 0s 571us/step - loss: 8.9625 - mean_squared_error: 8.9625\n",
      "Epoch 139/700\n",
      "576/576 [==============================] - 0s 576us/step - loss: 8.8663 - mean_squared_error: 8.8663\n",
      "Epoch 140/700\n",
      "576/576 [==============================] - 0s 567us/step - loss: 8.8671 - mean_squared_error: 8.8671\n",
      "Epoch 141/700\n",
      "576/576 [==============================] - 0s 601us/step - loss: 8.8412 - mean_squared_error: 8.8412\n",
      "Epoch 142/700\n",
      "576/576 [==============================] - 0s 614us/step - loss: 8.8045 - mean_squared_error: 8.8045\n",
      "Epoch 143/700\n",
      "576/576 [==============================] - 0s 620us/step - loss: 8.8210 - mean_squared_error: 8.8210\n",
      "Epoch 144/700\n",
      "576/576 [==============================] - 0s 606us/step - loss: 8.6688 - mean_squared_error: 8.6688\n",
      "Epoch 145/700\n",
      "576/576 [==============================] - 0s 623us/step - loss: 8.8125 - mean_squared_error: 8.8125\n",
      "Epoch 146/700\n",
      "576/576 [==============================] - 0s 613us/step - loss: 8.7071 - mean_squared_error: 8.7071\n",
      "Epoch 147/700\n",
      "576/576 [==============================] - 0s 629us/step - loss: 8.7103 - mean_squared_error: 8.7103\n",
      "Epoch 148/700\n",
      "576/576 [==============================] - 0s 621us/step - loss: 8.6562 - mean_squared_error: 8.6562\n",
      "Epoch 149/700\n",
      "576/576 [==============================] - 0s 609us/step - loss: 8.6414 - mean_squared_error: 8.6414\n",
      "Epoch 150/700\n",
      "576/576 [==============================] - 0s 620us/step - loss: 8.6088 - mean_squared_error: 8.6088\n",
      "Epoch 151/700\n",
      "576/576 [==============================] - 0s 607us/step - loss: 8.5521 - mean_squared_error: 8.5521\n",
      "Epoch 152/700\n",
      "576/576 [==============================] - 0s 603us/step - loss: 8.6041 - mean_squared_error: 8.6041\n",
      "Epoch 153/700\n",
      "576/576 [==============================] - 0s 616us/step - loss: 8.5696 - mean_squared_error: 8.5696\n",
      "Epoch 154/700\n",
      "576/576 [==============================] - 0s 616us/step - loss: 8.5691 - mean_squared_error: 8.5691\n",
      "Epoch 155/700\n",
      "576/576 [==============================] - 0s 602us/step - loss: 8.5074 - mean_squared_error: 8.5074\n",
      "Epoch 156/700\n",
      "576/576 [==============================] - 0s 620us/step - loss: 8.4733 - mean_squared_error: 8.4733\n",
      "Epoch 157/700\n",
      "576/576 [==============================] - 0s 602us/step - loss: 8.5329 - mean_squared_error: 8.5329\n",
      "Epoch 158/700\n",
      "576/576 [==============================] - 0s 602us/step - loss: 8.5034 - mean_squared_error: 8.5034\n",
      "Epoch 159/700\n",
      "576/576 [==============================] - 0s 602us/step - loss: 8.4847 - mean_squared_error: 8.4847\n",
      "Epoch 160/700\n",
      "576/576 [==============================] - 0s 599us/step - loss: 8.4455 - mean_squared_error: 8.4455\n",
      "Epoch 161/700\n",
      "576/576 [==============================] - 0s 604us/step - loss: 8.4433 - mean_squared_error: 8.4433\n",
      "Epoch 162/700\n",
      "576/576 [==============================] - 0s 604us/step - loss: 8.4270 - mean_squared_error: 8.4270\n",
      "Epoch 163/700\n",
      "576/576 [==============================] - 0s 592us/step - loss: 8.4059 - mean_squared_error: 8.4059\n",
      "Epoch 164/700\n",
      "576/576 [==============================] - 0s 590us/step - loss: 8.4274 - mean_squared_error: 8.4274\n",
      "Epoch 165/700\n",
      "576/576 [==============================] - 0s 596us/step - loss: 8.3452 - mean_squared_error: 8.3452\n",
      "Epoch 166/700\n",
      "576/576 [==============================] - 0s 606us/step - loss: 8.4800 - mean_squared_error: 8.4800\n",
      "Epoch 167/700\n",
      "576/576 [==============================] - 0s 592us/step - loss: 8.3987 - mean_squared_error: 8.3987\n",
      "Epoch 168/700\n",
      "576/576 [==============================] - 0s 646us/step - loss: 8.3543 - mean_squared_error: 8.3543\n",
      "Epoch 169/700\n",
      "576/576 [==============================] - 0s 707us/step - loss: 8.3797 - mean_squared_error: 8.3797\n",
      "Epoch 170/700\n",
      "576/576 [==============================] - 0s 705us/step - loss: 8.3399 - mean_squared_error: 8.33990s - loss: 8.3118 - mean_squared_e\n",
      "Epoch 171/700\n",
      "576/576 [==============================] - 0s 674us/step - loss: 8.3264 - mean_squared_error: 8.3264\n",
      "Epoch 172/700\n",
      "576/576 [==============================] - 0s 613us/step - loss: 8.3457 - mean_squared_error: 8.3457\n",
      "Epoch 173/700\n",
      "576/576 [==============================] - 0s 583us/step - loss: 8.3437 - mean_squared_error: 8.3437\n",
      "Epoch 174/700\n",
      "576/576 [==============================] - 0s 601us/step - loss: 8.3290 - mean_squared_error: 8.3290\n",
      "Epoch 175/700\n",
      "576/576 [==============================] - 0s 580us/step - loss: 8.2435 - mean_squared_error: 8.2435\n",
      "Epoch 176/700\n",
      "576/576 [==============================] - 0s 596us/step - loss: 8.3547 - mean_squared_error: 8.3547\n",
      "Epoch 177/700\n",
      "576/576 [==============================] - 0s 585us/step - loss: 8.3296 - mean_squared_error: 8.3296\n",
      "Epoch 178/700\n",
      "576/576 [==============================] - 0s 599us/step - loss: 8.3304 - mean_squared_error: 8.3304\n",
      "Epoch 179/700\n",
      "576/576 [==============================] - 0s 590us/step - loss: 8.3076 - mean_squared_error: 8.3076\n",
      "Epoch 180/700\n",
      "576/576 [==============================] - 0s 587us/step - loss: 8.3752 - mean_squared_error: 8.3752\n",
      "Epoch 181/700\n",
      "576/576 [==============================] - 0s 581us/step - loss: 8.3154 - mean_squared_error: 8.3154\n",
      "Epoch 182/700\n",
      "576/576 [==============================] - 0s 593us/step - loss: 8.2869 - mean_squared_error: 8.2869\n",
      "Epoch 183/700\n",
      "576/576 [==============================] - 0s 584us/step - loss: 8.3014 - mean_squared_error: 8.3014\n",
      "Epoch 184/700\n",
      "576/576 [==============================] - 0s 585us/step - loss: 8.2939 - mean_squared_error: 8.2939\n",
      "Epoch 185/700\n",
      "576/576 [==============================] - 0s 585us/step - loss: 8.3201 - mean_squared_error: 8.3201\n",
      "Epoch 186/700\n",
      "576/576 [==============================] - 0s 567us/step - loss: 8.2965 - mean_squared_error: 8.2965\n",
      "Epoch 187/700\n",
      "576/576 [==============================] - 0s 585us/step - loss: 8.2762 - mean_squared_error: 8.2762\n",
      "Epoch 188/700\n",
      "576/576 [==============================] - 0s 601us/step - loss: 8.3138 - mean_squared_error: 8.3138\n",
      "Epoch 189/700\n",
      "576/576 [==============================] - 0s 611us/step - loss: 8.2811 - mean_squared_error: 8.2811\n",
      "Epoch 190/700\n",
      "576/576 [==============================] - 0s 578us/step - loss: 8.2952 - mean_squared_error: 8.2952\n",
      "Epoch 191/700\n",
      "576/576 [==============================] - 0s 581us/step - loss: 8.2302 - mean_squared_error: 8.2302\n",
      "Epoch 192/700\n",
      "576/576 [==============================] - 0s 573us/step - loss: 8.2089 - mean_squared_error: 8.2089\n",
      "Epoch 193/700\n",
      "576/576 [==============================] - 0s 567us/step - loss: 8.2646 - mean_squared_error: 8.2646\n",
      "Epoch 194/700\n",
      "576/576 [==============================] - 0s 578us/step - loss: 8.2872 - mean_squared_error: 8.2872\n",
      "Epoch 195/700\n",
      "576/576 [==============================] - 0s 578us/step - loss: 8.2847 - mean_squared_error: 8.2847\n",
      "Epoch 196/700\n",
      "576/576 [==============================] - 0s 573us/step - loss: 8.2270 - mean_squared_error: 8.2270\n",
      "Epoch 197/700\n",
      "576/576 [==============================] - 0s 588us/step - loss: 8.2562 - mean_squared_error: 8.2562\n",
      "Epoch 198/700\n",
      "576/576 [==============================] - 0s 566us/step - loss: 8.1941 - mean_squared_error: 8.1941\n",
      "Epoch 199/700\n",
      "576/576 [==============================] - 0s 564us/step - loss: 8.3462 - mean_squared_error: 8.3462\n",
      "Epoch 200/700\n",
      "576/576 [==============================] - 0s 561us/step - loss: 8.2463 - mean_squared_error: 8.2463\n",
      "Epoch 201/700\n",
      "576/576 [==============================] - 0s 566us/step - loss: 8.2071 - mean_squared_error: 8.2071\n",
      "Epoch 202/700\n",
      "576/576 [==============================] - 0s 567us/step - loss: 8.2306 - mean_squared_error: 8.2306\n",
      "Epoch 203/700\n",
      "576/576 [==============================] - 0s 588us/step - loss: 8.2055 - mean_squared_error: 8.2055\n",
      "Epoch 204/700\n",
      "576/576 [==============================] - 0s 573us/step - loss: 8.2994 - mean_squared_error: 8.2994\n",
      "Epoch 205/700\n",
      "576/576 [==============================] - 0s 578us/step - loss: 8.2048 - mean_squared_error: 8.2048\n",
      "Epoch 206/700\n",
      "576/576 [==============================] - 0s 576us/step - loss: 8.2384 - mean_squared_error: 8.2384\n",
      "Epoch 207/700\n",
      "576/576 [==============================] - 0s 562us/step - loss: 8.2185 - mean_squared_error: 8.2185\n",
      "Epoch 208/700\n",
      "576/576 [==============================] - 0s 561us/step - loss: 8.2347 - mean_squared_error: 8.2347\n",
      "Epoch 209/700\n",
      "576/576 [==============================] - 0s 563us/step - loss: 8.2586 - mean_squared_error: 8.2586\n",
      "Epoch 210/700\n",
      "576/576 [==============================] - 0s 561us/step - loss: 8.1860 - mean_squared_error: 8.1860\n",
      "Epoch 211/700\n",
      "576/576 [==============================] - 0s 562us/step - loss: 8.1905 - mean_squared_error: 8.1905\n",
      "Epoch 212/700\n",
      "576/576 [==============================] - 0s 604us/step - loss: 8.2305 - mean_squared_error: 8.2305\n",
      "Epoch 213/700\n",
      "576/576 [==============================] - 0s 618us/step - loss: 8.2661 - mean_squared_error: 8.2661\n",
      "Epoch 214/700\n",
      "576/576 [==============================] - 0s 615us/step - loss: 8.2188 - mean_squared_error: 8.2188\n",
      "Epoch 215/700\n",
      "576/576 [==============================] - 0s 658us/step - loss: 8.1598 - mean_squared_error: 8.1598\n",
      "Epoch 216/700\n",
      "576/576 [==============================] - 0s 756us/step - loss: 8.2289 - mean_squared_error: 8.2289\n",
      "Epoch 217/700\n",
      "576/576 [==============================] - 0s 729us/step - loss: 8.1982 - mean_squared_error: 8.1982\n",
      "Epoch 218/700\n",
      "576/576 [==============================] - 0s 681us/step - loss: 8.2336 - mean_squared_error: 8.2336\n",
      "Epoch 219/700\n",
      "576/576 [==============================] - 0s 618us/step - loss: 8.1799 - mean_squared_error: 8.1799\n",
      "Epoch 220/700\n",
      "576/576 [==============================] - 0s 621us/step - loss: 8.2539 - mean_squared_error: 8.2539\n",
      "Epoch 221/700\n",
      "576/576 [==============================] - 0s 615us/step - loss: 8.1773 - mean_squared_error: 8.1773\n",
      "Epoch 222/700\n",
      "576/576 [==============================] - 0s 606us/step - loss: 8.1923 - mean_squared_error: 8.1923\n",
      "Epoch 223/700\n",
      "576/576 [==============================] - 0s 616us/step - loss: 8.1859 - mean_squared_error: 8.1859\n",
      "Epoch 224/700\n",
      "576/576 [==============================] - 0s 616us/step - loss: 8.1497 - mean_squared_error: 8.1497\n",
      "Epoch 225/700\n",
      "576/576 [==============================] - 0s 607us/step - loss: 8.1918 - mean_squared_error: 8.1918\n",
      "Epoch 226/700\n",
      "576/576 [==============================] - 0s 591us/step - loss: 8.2115 - mean_squared_error: 8.2115\n",
      "Epoch 227/700\n",
      "576/576 [==============================] - 0s 616us/step - loss: 8.2024 - mean_squared_error: 8.2024\n",
      "Epoch 228/700\n",
      "576/576 [==============================] - 0s 614us/step - loss: 8.1775 - mean_squared_error: 8.1775\n",
      "Epoch 229/700\n",
      "576/576 [==============================] - 0s 600us/step - loss: 8.1835 - mean_squared_error: 8.1835\n",
      "Epoch 230/700\n",
      "576/576 [==============================] - 0s 621us/step - loss: 8.2002 - mean_squared_error: 8.2002\n",
      "Epoch 231/700\n",
      "576/576 [==============================] - 0s 616us/step - loss: 8.1805 - mean_squared_error: 8.1805\n",
      "Epoch 232/700\n",
      "576/576 [==============================] - 0s 595us/step - loss: 8.1762 - mean_squared_error: 8.1762\n",
      "Epoch 233/700\n",
      "576/576 [==============================] - 0s 599us/step - loss: 8.1669 - mean_squared_error: 8.1669\n",
      "Epoch 234/700\n",
      "576/576 [==============================] - 0s 606us/step - loss: 8.2139 - mean_squared_error: 8.2139\n",
      "Epoch 235/700\n",
      "576/576 [==============================] - 0s 599us/step - loss: 8.1777 - mean_squared_error: 8.1777\n",
      "Epoch 236/700\n",
      "576/576 [==============================] - 0s 601us/step - loss: 8.2089 - mean_squared_error: 8.2089\n",
      "Epoch 237/700\n",
      "576/576 [==============================] - 0s 595us/step - loss: 8.1487 - mean_squared_error: 8.1487\n",
      "Epoch 238/700\n",
      "576/576 [==============================] - 0s 592us/step - loss: 8.1897 - mean_squared_error: 8.1897\n",
      "Epoch 239/700\n",
      "576/576 [==============================] - 0s 604us/step - loss: 8.1753 - mean_squared_error: 8.1753\n",
      "Epoch 240/700\n",
      "576/576 [==============================] - 0s 594us/step - loss: 8.2299 - mean_squared_error: 8.2299\n",
      "Epoch 241/700\n",
      "576/576 [==============================] - 0s 590us/step - loss: 8.1673 - mean_squared_error: 8.1673\n",
      "Epoch 242/700\n",
      "576/576 [==============================] - 0s 583us/step - loss: 8.1735 - mean_squared_error: 8.1735\n",
      "Epoch 243/700\n",
      "576/576 [==============================] - 0s 616us/step - loss: 8.1935 - mean_squared_error: 8.1935\n",
      "Epoch 244/700\n",
      "576/576 [==============================] - 0s 597us/step - loss: 8.1854 - mean_squared_error: 8.1854\n",
      "Epoch 245/700\n",
      "576/576 [==============================] - 0s 593us/step - loss: 8.2324 - mean_squared_error: 8.2324\n",
      "Epoch 246/700\n",
      "576/576 [==============================] - 0s 593us/step - loss: 8.1795 - mean_squared_error: 8.1795\n",
      "Epoch 247/700\n",
      "576/576 [==============================] - 0s 608us/step - loss: 8.2065 - mean_squared_error: 8.2065\n",
      "Epoch 248/700\n",
      "576/576 [==============================] - 0s 599us/step - loss: 8.1695 - mean_squared_error: 8.1695\n",
      "Epoch 249/700\n",
      "576/576 [==============================] - 0s 604us/step - loss: 8.1879 - mean_squared_error: 8.1879\n",
      "Epoch 250/700\n",
      "576/576 [==============================] - 0s 574us/step - loss: 8.1408 - mean_squared_error: 8.1408\n",
      "Epoch 251/700\n",
      "576/576 [==============================] - 0s 588us/step - loss: 8.1910 - mean_squared_error: 8.1910\n",
      "Epoch 252/700\n",
      "576/576 [==============================] - 0s 590us/step - loss: 8.1214 - mean_squared_error: 8.1214\n",
      "Epoch 253/700\n",
      "576/576 [==============================] - 0s 577us/step - loss: 8.1881 - mean_squared_error: 8.1881\n",
      "Epoch 254/700\n",
      "576/576 [==============================] - 0s 594us/step - loss: 8.2003 - mean_squared_error: 8.2003\n",
      "Epoch 255/700\n",
      "576/576 [==============================] - 0s 578us/step - loss: 8.2276 - mean_squared_error: 8.2276\n",
      "Epoch 256/700\n",
      "576/576 [==============================] - 0s 587us/step - loss: 8.2519 - mean_squared_error: 8.2519\n",
      "Epoch 257/700\n",
      "576/576 [==============================] - 0s 587us/step - loss: 8.1299 - mean_squared_error: 8.1299\n",
      "Epoch 258/700\n",
      "576/576 [==============================] - 0s 581us/step - loss: 8.1247 - mean_squared_error: 8.1247\n",
      "Epoch 259/700\n",
      "576/576 [==============================] - 0s 585us/step - loss: 8.1677 - mean_squared_error: 8.1677\n",
      "Epoch 260/700\n",
      "576/576 [==============================] - 0s 596us/step - loss: 8.1729 - mean_squared_error: 8.1729\n",
      "Epoch 261/700\n",
      "576/576 [==============================] - 0s 688us/step - loss: 8.2358 - mean_squared_error: 8.2358\n",
      "Epoch 262/700\n",
      "576/576 [==============================] - 0s 677us/step - loss: 8.1567 - mean_squared_error: 8.1567\n",
      "Epoch 263/700\n",
      "576/576 [==============================] - 0s 677us/step - loss: 8.1595 - mean_squared_error: 8.1595\n",
      "Epoch 264/700\n",
      "576/576 [==============================] - 0s 613us/step - loss: 8.1333 - mean_squared_error: 8.1333\n",
      "Epoch 265/700\n",
      "576/576 [==============================] - 0s 578us/step - loss: 8.1170 - mean_squared_error: 8.1170\n",
      "Epoch 266/700\n",
      "576/576 [==============================] - 0s 569us/step - loss: 8.1727 - mean_squared_error: 8.1727\n",
      "Epoch 267/700\n",
      "576/576 [==============================] - 0s 571us/step - loss: 8.1461 - mean_squared_error: 8.1461\n",
      "Epoch 268/700\n",
      "576/576 [==============================] - 0s 574us/step - loss: 8.1283 - mean_squared_error: 8.1283\n",
      "Epoch 269/700\n",
      "576/576 [==============================] - 0s 576us/step - loss: 8.1376 - mean_squared_error: 8.1376\n",
      "Epoch 270/700\n",
      "576/576 [==============================] - 0s 567us/step - loss: 8.1586 - mean_squared_error: 8.1586\n",
      "Epoch 271/700\n",
      "576/576 [==============================] - 0s 575us/step - loss: 8.1502 - mean_squared_error: 8.1502\n",
      "Epoch 272/700\n",
      "576/576 [==============================] - 0s 569us/step - loss: 8.1997 - mean_squared_error: 8.1997\n",
      "Epoch 273/700\n",
      "576/576 [==============================] - 0s 552us/step - loss: 8.1713 - mean_squared_error: 8.1713\n",
      "Epoch 274/700\n",
      "576/576 [==============================] - 0s 562us/step - loss: 8.1158 - mean_squared_error: 8.1158\n",
      "Epoch 275/700\n",
      "576/576 [==============================] - 0s 573us/step - loss: 8.1381 - mean_squared_error: 8.1381\n",
      "Epoch 276/700\n",
      "576/576 [==============================] - 0s 571us/step - loss: 8.1455 - mean_squared_error: 8.1455\n",
      "Epoch 277/700\n",
      "576/576 [==============================] - 0s 574us/step - loss: 8.2100 - mean_squared_error: 8.2100\n",
      "Epoch 278/700\n",
      "576/576 [==============================] - 0s 569us/step - loss: 8.1706 - mean_squared_error: 8.1706\n",
      "Epoch 279/700\n",
      "576/576 [==============================] - 0s 562us/step - loss: 8.1153 - mean_squared_error: 8.1153\n",
      "Epoch 280/700\n",
      "576/576 [==============================] - 0s 562us/step - loss: 8.1695 - mean_squared_error: 8.1695\n",
      "Epoch 281/700\n",
      "576/576 [==============================] - 0s 557us/step - loss: 8.1792 - mean_squared_error: 8.1792\n",
      "Epoch 282/700\n",
      "576/576 [==============================] - 0s 557us/step - loss: 8.0941 - mean_squared_error: 8.0941\n",
      "Epoch 283/700\n",
      "576/576 [==============================] - 0s 616us/step - loss: 8.0834 - mean_squared_error: 8.0834\n",
      "Epoch 284/700\n",
      "576/576 [==============================] - 0s 627us/step - loss: 8.1937 - mean_squared_error: 8.1937\n",
      "Epoch 285/700\n",
      "576/576 [==============================] - 0s 620us/step - loss: 8.1641 - mean_squared_error: 8.1641\n",
      "Epoch 286/700\n",
      "576/576 [==============================] - 0s 617us/step - loss: 8.1259 - mean_squared_error: 8.1259\n",
      "Epoch 287/700\n",
      "576/576 [==============================] - 0s 633us/step - loss: 8.1310 - mean_squared_error: 8.1310\n",
      "Epoch 288/700\n",
      "576/576 [==============================] - 0s 597us/step - loss: 8.1496 - mean_squared_error: 8.1496\n",
      "Epoch 289/700\n",
      "576/576 [==============================] - 0s 622us/step - loss: 8.2089 - mean_squared_error: 8.2089\n",
      "Epoch 290/700\n",
      "576/576 [==============================] - 0s 619us/step - loss: 8.1269 - mean_squared_error: 8.1269\n",
      "Epoch 291/700\n",
      "576/576 [==============================] - 0s 613us/step - loss: 8.1254 - mean_squared_error: 8.1254\n",
      "Epoch 292/700\n",
      "576/576 [==============================] - 0s 611us/step - loss: 8.1436 - mean_squared_error: 8.1436\n",
      "Epoch 293/700\n",
      "576/576 [==============================] - 0s 609us/step - loss: 8.1354 - mean_squared_error: 8.1354\n",
      "Epoch 294/700\n",
      "576/576 [==============================] - 0s 601us/step - loss: 8.1348 - mean_squared_error: 8.1348\n",
      "Epoch 295/700\n",
      "576/576 [==============================] - 0s 620us/step - loss: 8.1529 - mean_squared_error: 8.1529\n",
      "Epoch 296/700\n",
      "576/576 [==============================] - 0s 609us/step - loss: 8.1387 - mean_squared_error: 8.1387\n",
      "Epoch 297/700\n",
      "576/576 [==============================] - 0s 604us/step - loss: 8.1631 - mean_squared_error: 8.1631\n",
      "Epoch 298/700\n",
      "576/576 [==============================] - 0s 602us/step - loss: 8.1031 - mean_squared_error: 8.1031\n",
      "Epoch 299/700\n",
      "576/576 [==============================] - 0s 601us/step - loss: 8.1787 - mean_squared_error: 8.1787\n",
      "Epoch 300/700\n",
      "576/576 [==============================] - 0s 614us/step - loss: 8.1047 - mean_squared_error: 8.1047\n",
      "Epoch 301/700\n",
      "576/576 [==============================] - 0s 600us/step - loss: 8.2051 - mean_squared_error: 8.2051\n",
      "Epoch 302/700\n",
      "576/576 [==============================] - 0s 611us/step - loss: 8.1331 - mean_squared_error: 8.1331\n",
      "Epoch 303/700\n",
      "576/576 [==============================] - 0s 595us/step - loss: 8.1023 - mean_squared_error: 8.1023\n",
      "Epoch 304/700\n",
      "576/576 [==============================] - 0s 598us/step - loss: 8.1477 - mean_squared_error: 8.1477\n",
      "Epoch 305/700\n",
      "576/576 [==============================] - 0s 598us/step - loss: 8.1567 - mean_squared_error: 8.1567\n",
      "Epoch 306/700\n",
      "576/576 [==============================] - 0s 602us/step - loss: 8.2153 - mean_squared_error: 8.2153\n",
      "Epoch 307/700\n",
      "576/576 [==============================] - 0s 691us/step - loss: 8.1344 - mean_squared_error: 8.1344\n",
      "Epoch 308/700\n",
      "576/576 [==============================] - 0s 714us/step - loss: 8.1414 - mean_squared_error: 8.1414\n",
      "Epoch 309/700\n",
      "576/576 [==============================] - 0s 694us/step - loss: 8.1531 - mean_squared_error: 8.1531\n",
      "Epoch 310/700\n",
      "576/576 [==============================] - 0s 635us/step - loss: 8.2411 - mean_squared_error: 8.2411\n",
      "Epoch 311/700\n",
      "576/576 [==============================] - 0s 606us/step - loss: 8.1434 - mean_squared_error: 8.1434\n",
      "Epoch 312/700\n",
      "576/576 [==============================] - 0s 599us/step - loss: 8.1423 - mean_squared_error: 8.1423\n",
      "Epoch 313/700\n",
      "576/576 [==============================] - 0s 592us/step - loss: 8.1278 - mean_squared_error: 8.1278\n",
      "Epoch 314/700\n",
      "576/576 [==============================] - 0s 596us/step - loss: 8.1240 - mean_squared_error: 8.1240\n",
      "Epoch 315/700\n",
      "576/576 [==============================] - 0s 591us/step - loss: 8.0981 - mean_squared_error: 8.0981\n",
      "Epoch 316/700\n",
      "576/576 [==============================] - 0s 574us/step - loss: 8.0644 - mean_squared_error: 8.0644\n",
      "Epoch 317/700\n",
      "576/576 [==============================] - 0s 609us/step - loss: 8.1681 - mean_squared_error: 8.1681\n",
      "Epoch 318/700\n",
      "576/576 [==============================] - 0s 588us/step - loss: 8.1527 - mean_squared_error: 8.1527\n",
      "Epoch 319/700\n",
      "576/576 [==============================] - 0s 605us/step - loss: 8.1199 - mean_squared_error: 8.1199\n",
      "Epoch 320/700\n",
      "576/576 [==============================] - 0s 588us/step - loss: 8.1626 - mean_squared_error: 8.1626\n",
      "Epoch 321/700\n",
      "576/576 [==============================] - 0s 591us/step - loss: 8.1155 - mean_squared_error: 8.1155\n",
      "Epoch 322/700\n",
      "576/576 [==============================] - 0s 573us/step - loss: 8.1405 - mean_squared_error: 8.1405\n",
      "Epoch 323/700\n",
      "576/576 [==============================] - 0s 601us/step - loss: 8.1285 - mean_squared_error: 8.1285\n",
      "Epoch 324/700\n",
      "576/576 [==============================] - 0s 575us/step - loss: 8.1641 - mean_squared_error: 8.1641\n",
      "Epoch 325/700\n",
      "576/576 [==============================] - 0s 582us/step - loss: 8.0982 - mean_squared_error: 8.0982\n",
      "Epoch 326/700\n",
      "576/576 [==============================] - 0s 581us/step - loss: 8.0792 - mean_squared_error: 8.0792\n",
      "Epoch 327/700\n",
      "576/576 [==============================] - 0s 588us/step - loss: 8.1462 - mean_squared_error: 8.1462\n",
      "Epoch 328/700\n",
      "576/576 [==============================] - 0s 578us/step - loss: 8.1181 - mean_squared_error: 8.1181\n",
      "Epoch 329/700\n",
      "576/576 [==============================] - 0s 573us/step - loss: 8.1787 - mean_squared_error: 8.1787\n",
      "Epoch 330/700\n",
      "576/576 [==============================] - 0s 590us/step - loss: 8.1002 - mean_squared_error: 8.1002\n",
      "Epoch 331/700\n",
      "576/576 [==============================] - 0s 590us/step - loss: 8.1633 - mean_squared_error: 8.1633\n",
      "Epoch 332/700\n",
      "576/576 [==============================] - 0s 580us/step - loss: 8.1056 - mean_squared_error: 8.1056\n",
      "Epoch 333/700\n",
      "576/576 [==============================] - 0s 580us/step - loss: 8.3120 - mean_squared_error: 8.3120\n",
      "Epoch 334/700\n",
      "576/576 [==============================] - 0s 679us/step - loss: 8.1330 - mean_squared_error: 8.1330\n",
      "Epoch 335/700\n",
      "576/576 [==============================] - 0s 578us/step - loss: 8.1293 - mean_squared_error: 8.1293\n",
      "Epoch 336/700\n",
      "576/576 [==============================] - 0s 597us/step - loss: 8.1469 - mean_squared_error: 8.1469\n",
      "Epoch 337/700\n",
      "576/576 [==============================] - 0s 625us/step - loss: 8.0971 - mean_squared_error: 8.0971\n",
      "Epoch 338/700\n",
      "576/576 [==============================] - 0s 606us/step - loss: 8.1489 - mean_squared_error: 8.1489\n",
      "Epoch 339/700\n",
      "576/576 [==============================] - 0s 647us/step - loss: 8.1135 - mean_squared_error: 8.1135\n",
      "Epoch 340/700\n",
      "576/576 [==============================] - 0s 635us/step - loss: 8.1556 - mean_squared_error: 8.1556\n",
      "Epoch 341/700\n",
      "576/576 [==============================] - 0s 689us/step - loss: 8.1225 - mean_squared_error: 8.1225\n",
      "Epoch 342/700\n",
      "576/576 [==============================] - 0s 684us/step - loss: 8.1378 - mean_squared_error: 8.1378\n",
      "Epoch 343/700\n",
      "576/576 [==============================] - 0s 622us/step - loss: 8.1689 - mean_squared_error: 8.1689\n",
      "Epoch 344/700\n",
      "576/576 [==============================] - 0s 628us/step - loss: 8.1415 - mean_squared_error: 8.1415\n",
      "Epoch 345/700\n",
      "576/576 [==============================] - 0s 608us/step - loss: 8.1155 - mean_squared_error: 8.1155\n",
      "Epoch 346/700\n",
      "576/576 [==============================] - 0s 614us/step - loss: 8.2068 - mean_squared_error: 8.2068\n",
      "Epoch 347/700\n",
      "576/576 [==============================] - 0s 618us/step - loss: 8.1835 - mean_squared_error: 8.1835\n",
      "Epoch 348/700\n",
      "576/576 [==============================] - 0s 619us/step - loss: 8.1584 - mean_squared_error: 8.1584\n",
      "Epoch 349/700\n",
      "576/576 [==============================] - 0s 621us/step - loss: 8.1589 - mean_squared_error: 8.1589\n",
      "Epoch 350/700\n",
      "576/576 [==============================] - 0s 621us/step - loss: 8.1054 - mean_squared_error: 8.1054\n",
      "Epoch 351/700\n",
      "576/576 [==============================] - 0s 604us/step - loss: 8.1154 - mean_squared_error: 8.1154\n",
      "Epoch 352/700\n",
      "576/576 [==============================] - 0s 695us/step - loss: 8.1066 - mean_squared_error: 8.1066\n",
      "Epoch 353/700\n",
      "576/576 [==============================] - 0s 754us/step - loss: 8.1085 - mean_squared_error: 8.1085\n",
      "Epoch 354/700\n",
      "576/576 [==============================] - 0s 816us/step - loss: 8.2337 - mean_squared_error: 8.2337\n",
      "Epoch 355/700\n",
      "576/576 [==============================] - 0s 700us/step - loss: 8.1045 - mean_squared_error: 8.1045\n",
      "Epoch 356/700\n",
      "576/576 [==============================] - 0s 646us/step - loss: 8.0832 - mean_squared_error: 8.0832\n",
      "Epoch 357/700\n",
      "576/576 [==============================] - 0s 651us/step - loss: 8.1214 - mean_squared_error: 8.1214\n",
      "Epoch 358/700\n",
      "576/576 [==============================] - 0s 663us/step - loss: 8.1576 - mean_squared_error: 8.1576\n",
      "Epoch 359/700\n",
      "576/576 [==============================] - 0s 651us/step - loss: 8.1339 - mean_squared_error: 8.1339\n",
      "Epoch 360/700\n",
      "576/576 [==============================] - 0s 646us/step - loss: 8.1295 - mean_squared_error: 8.1295\n",
      "Epoch 361/700\n",
      "576/576 [==============================] - 0s 649us/step - loss: 8.0911 - mean_squared_error: 8.0911\n",
      "Epoch 362/700\n",
      "576/576 [==============================] - 0s 652us/step - loss: 8.1505 - mean_squared_error: 8.1505\n",
      "Epoch 363/700\n",
      "576/576 [==============================] - 0s 710us/step - loss: 8.1534 - mean_squared_error: 8.1534\n",
      "Epoch 364/700\n",
      "576/576 [==============================] - 0s 686us/step - loss: 8.1025 - mean_squared_error: 8.1025\n",
      "Epoch 365/700\n",
      "576/576 [==============================] - 0s 705us/step - loss: 8.1204 - mean_squared_error: 8.1204\n",
      "Epoch 366/700\n",
      "576/576 [==============================] - 0s 672us/step - loss: 8.0564 - mean_squared_error: 8.0564\n",
      "Epoch 367/700\n",
      "576/576 [==============================] - 0s 673us/step - loss: 8.1406 - mean_squared_error: 8.1406\n",
      "Epoch 368/700\n",
      "576/576 [==============================] - 0s 670us/step - loss: 8.0473 - mean_squared_error: 8.0473\n",
      "Epoch 369/700\n",
      "576/576 [==============================] - 0s 653us/step - loss: 8.1201 - mean_squared_error: 8.1201\n",
      "Epoch 370/700\n",
      "576/576 [==============================] - 0s 684us/step - loss: 8.0911 - mean_squared_error: 8.0911\n",
      "Epoch 371/700\n",
      "576/576 [==============================] - 0s 644us/step - loss: 8.1091 - mean_squared_error: 8.1091\n",
      "Epoch 372/700\n",
      "576/576 [==============================] - 0s 654us/step - loss: 8.1551 - mean_squared_error: 8.1551\n",
      "Epoch 373/700\n",
      "576/576 [==============================] - 0s 676us/step - loss: 8.1176 - mean_squared_error: 8.1176\n",
      "Epoch 374/700\n",
      "576/576 [==============================] - 0s 660us/step - loss: 8.1263 - mean_squared_error: 8.1263\n",
      "Epoch 375/700\n",
      "576/576 [==============================] - 0s 634us/step - loss: 8.1072 - mean_squared_error: 8.1072\n",
      "Epoch 376/700\n",
      "576/576 [==============================] - 0s 665us/step - loss: 8.1260 - mean_squared_error: 8.1260\n",
      "Epoch 377/700\n",
      "576/576 [==============================] - 0s 635us/step - loss: 8.3934 - mean_squared_error: 8.3934\n",
      "Epoch 378/700\n",
      "576/576 [==============================] - 0s 656us/step - loss: 8.1396 - mean_squared_error: 8.1396\n",
      "Epoch 379/700\n",
      "576/576 [==============================] - 0s 637us/step - loss: 8.0975 - mean_squared_error: 8.0975\n",
      "Epoch 380/700\n",
      "576/576 [==============================] - 0s 689us/step - loss: 8.1021 - mean_squared_error: 8.1021\n",
      "Epoch 381/700\n",
      "576/576 [==============================] - 0s 637us/step - loss: 8.1255 - mean_squared_error: 8.1255\n",
      "Epoch 382/700\n",
      "576/576 [==============================] - 0s 672us/step - loss: 8.1722 - mean_squared_error: 8.1722\n",
      "Epoch 383/700\n",
      "576/576 [==============================] - 0s 672us/step - loss: 8.1196 - mean_squared_error: 8.1196\n",
      "Epoch 384/700\n",
      "576/576 [==============================] - 0s 668us/step - loss: 8.1057 - mean_squared_error: 8.1057\n",
      "Epoch 385/700\n",
      "576/576 [==============================] - 0s 672us/step - loss: 8.1038 - mean_squared_error: 8.1038\n",
      "Epoch 386/700\n",
      "576/576 [==============================] - 0s 661us/step - loss: 8.1111 - mean_squared_error: 8.1111\n",
      "Epoch 387/700\n",
      "576/576 [==============================] - 0s 643us/step - loss: 8.0937 - mean_squared_error: 8.0937\n",
      "Epoch 388/700\n",
      "576/576 [==============================] - 0s 628us/step - loss: 8.2039 - mean_squared_error: 8.2039\n",
      "Epoch 389/700\n",
      "576/576 [==============================] - 0s 631us/step - loss: 8.1317 - mean_squared_error: 8.1317\n",
      "Epoch 390/700\n",
      "576/576 [==============================] - 0s 631us/step - loss: 8.1411 - mean_squared_error: 8.1411\n",
      "Epoch 391/700\n",
      "576/576 [==============================] - 0s 628us/step - loss: 8.0730 - mean_squared_error: 8.0730\n",
      "Epoch 392/700\n",
      "576/576 [==============================] - 0s 621us/step - loss: 8.1146 - mean_squared_error: 8.1146\n",
      "Epoch 393/700\n",
      "576/576 [==============================] - 0s 625us/step - loss: 8.1329 - mean_squared_error: 8.1329\n",
      "Epoch 394/700\n",
      "576/576 [==============================] - 0s 751us/step - loss: 8.1373 - mean_squared_error: 8.1373\n",
      "Epoch 395/700\n",
      "576/576 [==============================] - 0s 747us/step - loss: 8.1568 - mean_squared_error: 8.1568\n",
      "Epoch 396/700\n",
      "576/576 [==============================] - 0s 762us/step - loss: 8.1641 - mean_squared_error: 8.1641\n",
      "Epoch 397/700\n",
      "576/576 [==============================] - 0s 658us/step - loss: 8.1276 - mean_squared_error: 8.1276\n",
      "Epoch 398/700\n",
      "576/576 [==============================] - 0s 636us/step - loss: 8.1911 - mean_squared_error: 8.1911\n",
      "Epoch 399/700\n",
      "576/576 [==============================] - 0s 620us/step - loss: 8.1555 - mean_squared_error: 8.1555\n",
      "Epoch 400/700\n",
      "576/576 [==============================] - 0s 609us/step - loss: 8.1027 - mean_squared_error: 8.1027\n",
      "Epoch 401/700\n",
      "576/576 [==============================] - 0s 616us/step - loss: 8.1119 - mean_squared_error: 8.1119\n",
      "Epoch 402/700\n",
      "576/576 [==============================] - 0s 621us/step - loss: 8.0966 - mean_squared_error: 8.0966\n",
      "Epoch 403/700\n",
      "576/576 [==============================] - 0s 641us/step - loss: 8.1692 - mean_squared_error: 8.1692\n",
      "Epoch 404/700\n",
      "576/576 [==============================] - 0s 613us/step - loss: 8.1194 - mean_squared_error: 8.1194\n",
      "Epoch 405/700\n",
      "576/576 [==============================] - 0s 613us/step - loss: 8.1322 - mean_squared_error: 8.1322\n",
      "Epoch 406/700\n",
      "576/576 [==============================] - 0s 613us/step - loss: 8.1061 - mean_squared_error: 8.1061\n",
      "Epoch 407/700\n",
      "576/576 [==============================] - 0s 620us/step - loss: 8.1479 - mean_squared_error: 8.1479\n",
      "Epoch 408/700\n",
      "576/576 [==============================] - 0s 635us/step - loss: 8.1252 - mean_squared_error: 8.1252\n",
      "Epoch 409/700\n",
      "576/576 [==============================] - 0s 616us/step - loss: 8.1516 - mean_squared_error: 8.1516\n",
      "Epoch 410/700\n",
      "576/576 [==============================] - 0s 644us/step - loss: 8.1122 - mean_squared_error: 8.1122\n",
      "Epoch 411/700\n",
      "576/576 [==============================] - 0s 620us/step - loss: 8.0948 - mean_squared_error: 8.0948\n",
      "Epoch 412/700\n",
      "576/576 [==============================] - 0s 609us/step - loss: 8.0600 - mean_squared_error: 8.0600\n",
      "Epoch 413/700\n",
      "576/576 [==============================] - 0s 606us/step - loss: 8.1579 - mean_squared_error: 8.1579\n",
      "Epoch 414/700\n",
      "576/576 [==============================] - 0s 613us/step - loss: 8.1244 - mean_squared_error: 8.1244\n",
      "Epoch 415/700\n",
      "576/576 [==============================] - 0s 623us/step - loss: 8.2602 - mean_squared_error: 8.2602: 0s - loss: 9.1032 - mean_squared_err\n",
      "Epoch 416/700\n",
      "576/576 [==============================] - 0s 646us/step - loss: 8.1000 - mean_squared_error: 8.1000\n",
      "Epoch 417/700\n",
      "576/576 [==============================] - 0s 627us/step - loss: 8.1904 - mean_squared_error: 8.1904\n",
      "Epoch 418/700\n",
      "576/576 [==============================] - 0s 639us/step - loss: 8.0862 - mean_squared_error: 8.0862\n",
      "Epoch 419/700\n",
      "576/576 [==============================] - 0s 623us/step - loss: 8.1295 - mean_squared_error: 8.1295\n",
      "Epoch 420/700\n",
      "576/576 [==============================] - 0s 628us/step - loss: 8.0687 - mean_squared_error: 8.0687\n",
      "Epoch 421/700\n",
      "576/576 [==============================] - 0s 639us/step - loss: 8.1032 - mean_squared_error: 8.1032\n",
      "Epoch 422/700\n",
      "576/576 [==============================] - 0s 632us/step - loss: 8.0538 - mean_squared_error: 8.0538\n",
      "Epoch 423/700\n",
      "576/576 [==============================] - 0s 620us/step - loss: 8.1934 - mean_squared_error: 8.1934\n",
      "Epoch 424/700\n",
      "576/576 [==============================] - 0s 630us/step - loss: 8.1534 - mean_squared_error: 8.1534\n",
      "Epoch 425/700\n",
      "576/576 [==============================] - 0s 648us/step - loss: 8.0476 - mean_squared_error: 8.0476\n",
      "Epoch 426/700\n",
      "576/576 [==============================] - 0s 759us/step - loss: 8.1188 - mean_squared_error: 8.1188\n",
      "Epoch 427/700\n",
      "576/576 [==============================] - 0s 642us/step - loss: 8.1386 - mean_squared_error: 8.1386\n",
      "Epoch 428/700\n",
      "576/576 [==============================] - 0s 663us/step - loss: 8.0833 - mean_squared_error: 8.0833\n",
      "Epoch 429/700\n",
      "576/576 [==============================] - 0s 623us/step - loss: 8.1288 - mean_squared_error: 8.1288\n",
      "Epoch 430/700\n",
      "576/576 [==============================] - 0s 651us/step - loss: 8.1065 - mean_squared_error: 8.1065\n",
      "Epoch 431/700\n",
      "576/576 [==============================] - 0s 766us/step - loss: 8.1162 - mean_squared_error: 8.1162\n",
      "Epoch 432/700\n",
      "576/576 [==============================] - 0s 655us/step - loss: 8.0968 - mean_squared_error: 8.0968\n",
      "Epoch 433/700\n",
      "576/576 [==============================] - 0s 628us/step - loss: 8.1524 - mean_squared_error: 8.1524\n",
      "Epoch 434/700\n",
      "576/576 [==============================] - 0s 687us/step - loss: 8.1318 - mean_squared_error: 8.1318\n",
      "Epoch 435/700\n",
      "576/576 [==============================] - 0s 621us/step - loss: 8.1512 - mean_squared_error: 8.1512\n",
      "Epoch 436/700\n",
      "576/576 [==============================] - 0s 714us/step - loss: 8.1260 - mean_squared_error: 8.1260\n",
      "Epoch 437/700\n",
      "576/576 [==============================] - 0s 737us/step - loss: 8.1623 - mean_squared_error: 8.1623\n",
      "Epoch 438/700\n",
      "576/576 [==============================] - 0s 778us/step - loss: 8.0561 - mean_squared_error: 8.0561\n",
      "Epoch 439/700\n",
      "576/576 [==============================] - 0s 722us/step - loss: 8.1297 - mean_squared_error: 8.1297\n",
      "Epoch 440/700\n",
      "576/576 [==============================] - 0s 667us/step - loss: 8.1364 - mean_squared_error: 8.1364\n",
      "Epoch 441/700\n",
      "576/576 [==============================] - 0s 632us/step - loss: 8.0998 - mean_squared_error: 8.0998\n",
      "Epoch 442/700\n",
      "576/576 [==============================] - 0s 677us/step - loss: 8.0808 - mean_squared_error: 8.0808\n",
      "Epoch 443/700\n",
      "576/576 [==============================] - 0s 663us/step - loss: 8.0600 - mean_squared_error: 8.0600\n",
      "Epoch 444/700\n",
      "576/576 [==============================] - 0s 634us/step - loss: 8.1204 - mean_squared_error: 8.1204\n",
      "Epoch 445/700\n",
      "576/576 [==============================] - 0s 628us/step - loss: 8.1373 - mean_squared_error: 8.1373\n",
      "Epoch 446/700\n",
      "576/576 [==============================] - 0s 629us/step - loss: 8.1239 - mean_squared_error: 8.1239\n",
      "Epoch 447/700\n",
      "576/576 [==============================] - 0s 632us/step - loss: 8.1167 - mean_squared_error: 8.1167\n",
      "Epoch 448/700\n",
      "576/576 [==============================] - 0s 611us/step - loss: 8.1075 - mean_squared_error: 8.1075\n",
      "Epoch 449/700\n",
      "576/576 [==============================] - 0s 601us/step - loss: 8.1408 - mean_squared_error: 8.1408\n",
      "Epoch 450/700\n",
      "576/576 [==============================] - 0s 615us/step - loss: 8.1114 - mean_squared_error: 8.1114\n",
      "Epoch 451/700\n",
      "576/576 [==============================] - 0s 617us/step - loss: 8.1071 - mean_squared_error: 8.1071\n",
      "Epoch 452/700\n",
      "576/576 [==============================] - 0s 601us/step - loss: 8.0798 - mean_squared_error: 8.0798\n",
      "Epoch 453/700\n",
      "576/576 [==============================] - 0s 663us/step - loss: 8.1124 - mean_squared_error: 8.1124\n",
      "Epoch 454/700\n",
      "576/576 [==============================] - 0s 634us/step - loss: 8.0944 - mean_squared_error: 8.0944\n",
      "Epoch 455/700\n",
      "576/576 [==============================] - 0s 625us/step - loss: 8.0877 - mean_squared_error: 8.0877\n",
      "Epoch 456/700\n",
      "576/576 [==============================] - 0s 616us/step - loss: 8.1042 - mean_squared_error: 8.1042\n",
      "Epoch 457/700\n",
      "576/576 [==============================] - 0s 656us/step - loss: 8.0690 - mean_squared_error: 8.0690\n",
      "Epoch 458/700\n",
      "576/576 [==============================] - 0s 613us/step - loss: 8.2030 - mean_squared_error: 8.2030\n",
      "Epoch 459/700\n",
      "576/576 [==============================] - 0s 665us/step - loss: 8.1213 - mean_squared_error: 8.1213\n",
      "Epoch 460/700\n",
      "576/576 [==============================] - 0s 644us/step - loss: 8.0810 - mean_squared_error: 8.0810\n",
      "Epoch 461/700\n",
      "576/576 [==============================] - 0s 592us/step - loss: 8.0801 - mean_squared_error: 8.0801\n",
      "Epoch 462/700\n",
      "576/576 [==============================] - 0s 587us/step - loss: 8.1259 - mean_squared_error: 8.1259\n",
      "Epoch 463/700\n",
      "576/576 [==============================] - 0s 587us/step - loss: 8.1028 - mean_squared_error: 8.1028\n",
      "Epoch 464/700\n",
      "576/576 [==============================] - 0s 594us/step - loss: 8.1385 - mean_squared_error: 8.1385\n",
      "Epoch 465/700\n",
      "576/576 [==============================] - 0s 596us/step - loss: 8.0944 - mean_squared_error: 8.0944\n",
      "Epoch 466/700\n",
      "576/576 [==============================] - 0s 571us/step - loss: 8.1288 - mean_squared_error: 8.1288\n",
      "Epoch 467/700\n",
      "576/576 [==============================] - 0s 571us/step - loss: 8.1509 - mean_squared_error: 8.1509\n",
      "Epoch 468/700\n",
      "576/576 [==============================] - 0s 667us/step - loss: 8.1288 - mean_squared_error: 8.1288\n",
      "Epoch 469/700\n",
      "576/576 [==============================] - 0s 587us/step - loss: 8.1223 - mean_squared_error: 8.1223\n",
      "Epoch 470/700\n",
      "576/576 [==============================] - 0s 644us/step - loss: 8.1121 - mean_squared_error: 8.1121\n",
      "Epoch 471/700\n",
      "576/576 [==============================] - 0s 641us/step - loss: 8.0903 - mean_squared_error: 8.0903\n",
      "Epoch 472/700\n",
      "576/576 [==============================] - 0s 648us/step - loss: 8.0975 - mean_squared_error: 8.0975\n",
      "Epoch 473/700\n",
      "576/576 [==============================] - 0s 590us/step - loss: 8.0998 - mean_squared_error: 8.0998\n",
      "Epoch 474/700\n",
      "576/576 [==============================] - 0s 604us/step - loss: 8.0856 - mean_squared_error: 8.0856\n",
      "Epoch 475/700\n",
      "576/576 [==============================] - 0s 672us/step - loss: 8.1270 - mean_squared_error: 8.1270\n",
      "Epoch 476/700\n",
      "576/576 [==============================] - 0s 569us/step - loss: 8.1177 - mean_squared_error: 8.1177\n",
      "Epoch 477/700\n",
      "576/576 [==============================] - 0s 663us/step - loss: 8.1715 - mean_squared_error: 8.1715\n",
      "Epoch 478/700\n",
      "576/576 [==============================] - 0s 608us/step - loss: 8.1086 - mean_squared_error: 8.1086\n",
      "Epoch 479/700\n",
      "576/576 [==============================] - 0s 629us/step - loss: 8.1080 - mean_squared_error: 8.1080\n",
      "Epoch 480/700\n",
      "576/576 [==============================] - 0s 693us/step - loss: 8.1004 - mean_squared_error: 8.1004\n",
      "Epoch 481/700\n",
      "576/576 [==============================] - 0s 743us/step - loss: 8.0869 - mean_squared_error: 8.0869\n",
      "Epoch 482/700\n",
      "576/576 [==============================] - 0s 695us/step - loss: 8.0800 - mean_squared_error: 8.0800\n",
      "Epoch 483/700\n",
      "576/576 [==============================] - 0s 738us/step - loss: 8.0783 - mean_squared_error: 8.0783\n",
      "Epoch 484/700\n",
      "576/576 [==============================] - 0s 574us/step - loss: 8.1217 - mean_squared_error: 8.1217\n",
      "Epoch 485/700\n",
      "576/576 [==============================] - 0s 562us/step - loss: 8.0584 - mean_squared_error: 8.0584\n",
      "Epoch 486/700\n",
      "576/576 [==============================] - 0s 581us/step - loss: 8.0923 - mean_squared_error: 8.0923\n",
      "Epoch 487/700\n",
      "576/576 [==============================] - 0s 561us/step - loss: 8.1341 - mean_squared_error: 8.1341\n",
      "Epoch 488/700\n",
      "576/576 [==============================] - 0s 557us/step - loss: 8.0881 - mean_squared_error: 8.0881\n",
      "Epoch 489/700\n",
      "576/576 [==============================] - 0s 561us/step - loss: 8.1028 - mean_squared_error: 8.1028\n",
      "Epoch 490/700\n",
      "576/576 [==============================] - 0s 559us/step - loss: 8.0974 - mean_squared_error: 8.0974\n",
      "Epoch 491/700\n",
      "576/576 [==============================] - 0s 576us/step - loss: 8.1290 - mean_squared_error: 8.1290\n",
      "Epoch 492/700\n",
      "576/576 [==============================] - 0s 561us/step - loss: 8.0938 - mean_squared_error: 8.0938\n",
      "Epoch 493/700\n",
      "576/576 [==============================] - 0s 547us/step - loss: 8.0873 - mean_squared_error: 8.0873\n",
      "Epoch 494/700\n",
      "576/576 [==============================] - 0s 579us/step - loss: 8.1156 - mean_squared_error: 8.1156\n",
      "Epoch 495/700\n",
      "576/576 [==============================] - 0s 569us/step - loss: 8.1086 - mean_squared_error: 8.1086\n",
      "Epoch 496/700\n",
      "576/576 [==============================] - 0s 601us/step - loss: 8.0798 - mean_squared_error: 8.0798\n",
      "Epoch 497/700\n",
      "576/576 [==============================] - 0s 616us/step - loss: 8.1497 - mean_squared_error: 8.1497\n",
      "Epoch 498/700\n",
      "576/576 [==============================] - 0s 618us/step - loss: 8.1390 - mean_squared_error: 8.1390\n",
      "Epoch 499/700\n",
      "576/576 [==============================] - 0s 626us/step - loss: 8.1533 - mean_squared_error: 8.1533\n",
      "Epoch 500/700\n",
      "576/576 [==============================] - 0s 614us/step - loss: 8.0602 - mean_squared_error: 8.0602\n",
      "Epoch 501/700\n",
      "576/576 [==============================] - 0s 613us/step - loss: 8.0887 - mean_squared_error: 8.0887\n",
      "Epoch 502/700\n",
      "576/576 [==============================] - 0s 613us/step - loss: 8.0999 - mean_squared_error: 8.0999\n",
      "Epoch 503/700\n",
      "576/576 [==============================] - 0s 608us/step - loss: 8.0920 - mean_squared_error: 8.0920\n",
      "Epoch 504/700\n",
      "576/576 [==============================] - 0s 599us/step - loss: 8.1007 - mean_squared_error: 8.1007\n",
      "Epoch 505/700\n",
      "576/576 [==============================] - 0s 619us/step - loss: 8.1230 - mean_squared_error: 8.1230\n",
      "Epoch 506/700\n",
      "576/576 [==============================] - 0s 609us/step - loss: 8.0716 - mean_squared_error: 8.0716\n",
      "Epoch 507/700\n",
      "576/576 [==============================] - 0s 600us/step - loss: 8.1229 - mean_squared_error: 8.1229\n",
      "Epoch 508/700\n",
      "576/576 [==============================] - 0s 598us/step - loss: 8.0638 - mean_squared_error: 8.0638\n",
      "Epoch 509/700\n",
      "576/576 [==============================] - 0s 601us/step - loss: 8.0896 - mean_squared_error: 8.0896\n",
      "Epoch 510/700\n",
      "576/576 [==============================] - 0s 609us/step - loss: 8.1490 - mean_squared_error: 8.1490\n",
      "Epoch 511/700\n",
      "576/576 [==============================] - 0s 609us/step - loss: 8.1196 - mean_squared_error: 8.1196\n",
      "Epoch 512/700\n",
      "576/576 [==============================] - 0s 611us/step - loss: 8.1605 - mean_squared_error: 8.1605\n",
      "Epoch 513/700\n",
      "576/576 [==============================] - 0s 668us/step - loss: 8.0827 - mean_squared_error: 8.0827\n",
      "Epoch 514/700\n",
      "576/576 [==============================] - 0s 585us/step - loss: 8.1389 - mean_squared_error: 8.1389\n",
      "Epoch 515/700\n",
      "576/576 [==============================] - 0s 677us/step - loss: 8.2181 - mean_squared_error: 8.2181\n",
      "Epoch 516/700\n",
      "576/576 [==============================] - 0s 639us/step - loss: 8.1160 - mean_squared_error: 8.1160\n",
      "Epoch 517/700\n",
      "576/576 [==============================] - 0s 601us/step - loss: 8.0943 - mean_squared_error: 8.0943\n",
      "Epoch 518/700\n",
      "576/576 [==============================] - 0s 675us/step - loss: 8.1319 - mean_squared_error: 8.1319\n",
      "Epoch 519/700\n",
      "576/576 [==============================] - 0s 609us/step - loss: 8.1965 - mean_squared_error: 8.1965\n",
      "Epoch 520/700\n",
      "576/576 [==============================] - 0s 623us/step - loss: 8.1256 - mean_squared_error: 8.1256\n",
      "Epoch 521/700\n",
      "576/576 [==============================] - 0s 624us/step - loss: 8.0836 - mean_squared_error: 8.0836\n",
      "Epoch 522/700\n",
      "576/576 [==============================] - 0s 653us/step - loss: 8.1431 - mean_squared_error: 8.1431\n",
      "Epoch 523/700\n",
      "576/576 [==============================] - 0s 651us/step - loss: 8.0952 - mean_squared_error: 8.0952\n",
      "Epoch 524/700\n",
      "576/576 [==============================] - 0s 645us/step - loss: 8.0943 - mean_squared_error: 8.0943\n",
      "Epoch 525/700\n",
      "576/576 [==============================] - 0s 684us/step - loss: 8.0895 - mean_squared_error: 8.0895\n",
      "Epoch 526/700\n",
      "576/576 [==============================] - 0s 717us/step - loss: 8.1374 - mean_squared_error: 8.1374\n",
      "Epoch 527/700\n",
      "576/576 [==============================] - 0s 755us/step - loss: 8.0884 - mean_squared_error: 8.0884\n",
      "Epoch 528/700\n",
      "576/576 [==============================] - 0s 713us/step - loss: 8.0753 - mean_squared_error: 8.0753\n",
      "Epoch 529/700\n",
      "576/576 [==============================] - 0s 633us/step - loss: 8.1100 - mean_squared_error: 8.1100\n",
      "Epoch 530/700\n",
      "576/576 [==============================] - 0s 641us/step - loss: 8.1055 - mean_squared_error: 8.1055\n",
      "Epoch 531/700\n",
      "576/576 [==============================] - 0s 602us/step - loss: 8.0784 - mean_squared_error: 8.0784\n",
      "Epoch 532/700\n",
      "576/576 [==============================] - 0s 614us/step - loss: 8.1418 - mean_squared_error: 8.14180s - loss: 8.3578 - mean_squared_error\n",
      "Epoch 533/700\n",
      "576/576 [==============================] - 0s 594us/step - loss: 8.1877 - mean_squared_error: 8.1877\n",
      "Epoch 534/700\n",
      "576/576 [==============================] - 0s 657us/step - loss: 8.0574 - mean_squared_error: 8.0574\n",
      "Epoch 535/700\n",
      "576/576 [==============================] - 0s 635us/step - loss: 8.1251 - mean_squared_error: 8.1251\n",
      "Epoch 536/700\n",
      "576/576 [==============================] - 0s 599us/step - loss: 8.0622 - mean_squared_error: 8.0622\n",
      "Epoch 537/700\n",
      "576/576 [==============================] - 0s 660us/step - loss: 8.0978 - mean_squared_error: 8.0978\n",
      "Epoch 538/700\n",
      "576/576 [==============================] - 0s 646us/step - loss: 8.1255 - mean_squared_error: 8.1255\n",
      "Epoch 539/700\n",
      "576/576 [==============================] - 0s 618us/step - loss: 8.0739 - mean_squared_error: 8.0739\n",
      "Epoch 540/700\n",
      "576/576 [==============================] - 0s 597us/step - loss: 8.0667 - mean_squared_error: 8.0667\n",
      "Epoch 541/700\n",
      "576/576 [==============================] - 0s 588us/step - loss: 8.1432 - mean_squared_error: 8.1432\n",
      "Epoch 542/700\n",
      "576/576 [==============================] - 0s 578us/step - loss: 8.1136 - mean_squared_error: 8.1136\n",
      "Epoch 543/700\n",
      "576/576 [==============================] - 0s 568us/step - loss: 8.0694 - mean_squared_error: 8.0694\n",
      "Epoch 544/700\n",
      "576/576 [==============================] - 0s 561us/step - loss: 8.0685 - mean_squared_error: 8.0685\n",
      "Epoch 545/700\n",
      "576/576 [==============================] - 0s 580us/step - loss: 8.0935 - mean_squared_error: 8.0935\n",
      "Epoch 546/700\n",
      "576/576 [==============================] - 0s 592us/step - loss: 8.0825 - mean_squared_error: 8.0825\n",
      "Epoch 547/700\n",
      "576/576 [==============================] - 0s 582us/step - loss: 8.1260 - mean_squared_error: 8.1260\n",
      "Epoch 548/700\n",
      "576/576 [==============================] - 0s 592us/step - loss: 8.1497 - mean_squared_error: 8.1497\n",
      "Epoch 549/700\n",
      "576/576 [==============================] - 0s 609us/step - loss: 8.0923 - mean_squared_error: 8.0923\n",
      "Epoch 550/700\n",
      "576/576 [==============================] - 0s 581us/step - loss: 8.1248 - mean_squared_error: 8.12480s - loss: 7.1821 - mean_squared_err\n",
      "Epoch 551/700\n",
      "576/576 [==============================] - 0s 688us/step - loss: 8.1117 - mean_squared_error: 8.1117\n",
      "Epoch 552/700\n",
      "576/576 [==============================] - 0s 626us/step - loss: 8.0527 - mean_squared_error: 8.0527\n",
      "Epoch 553/700\n",
      "576/576 [==============================] - 0s 646us/step - loss: 8.1114 - mean_squared_error: 8.1114\n",
      "Epoch 554/700\n",
      "576/576 [==============================] - 0s 635us/step - loss: 8.0542 - mean_squared_error: 8.0542\n",
      "Epoch 555/700\n",
      "576/576 [==============================] - 0s 594us/step - loss: 8.1686 - mean_squared_error: 8.1686\n",
      "Epoch 556/700\n",
      "576/576 [==============================] - 0s 588us/step - loss: 8.1271 - mean_squared_error: 8.1271\n",
      "Epoch 557/700\n",
      "576/576 [==============================] - 0s 618us/step - loss: 8.0499 - mean_squared_error: 8.0499\n",
      "Epoch 558/700\n",
      "576/576 [==============================] - 0s 595us/step - loss: 8.1452 - mean_squared_error: 8.1452\n",
      "Epoch 559/700\n",
      "576/576 [==============================] - 0s 599us/step - loss: 8.2080 - mean_squared_error: 8.2080\n",
      "Epoch 560/700\n",
      "576/576 [==============================] - 0s 606us/step - loss: 8.0570 - mean_squared_error: 8.0570\n",
      "Epoch 561/700\n",
      "576/576 [==============================] - 0s 705us/step - loss: 8.0859 - mean_squared_error: 8.0859\n",
      "Epoch 562/700\n",
      "576/576 [==============================] - 0s 742us/step - loss: 8.0822 - mean_squared_error: 8.0822\n",
      "Epoch 563/700\n",
      "576/576 [==============================] - 0s 616us/step - loss: 8.0947 - mean_squared_error: 8.0947\n",
      "Epoch 564/700\n",
      "576/576 [==============================] - 0s 620us/step - loss: 8.1372 - mean_squared_error: 8.1372\n",
      "Epoch 565/700\n",
      "576/576 [==============================] - 0s 623us/step - loss: 8.0357 - mean_squared_error: 8.0357\n",
      "Epoch 566/700\n",
      "576/576 [==============================] - 0s 623us/step - loss: 8.1598 - mean_squared_error: 8.1598\n",
      "Epoch 567/700\n",
      "576/576 [==============================] - 0s 581us/step - loss: 8.0541 - mean_squared_error: 8.0541\n",
      "Epoch 568/700\n",
      "576/576 [==============================] - 0s 628us/step - loss: 8.0237 - mean_squared_error: 8.0237\n",
      "Epoch 569/700\n",
      "576/576 [==============================] - 0s 661us/step - loss: 8.1572 - mean_squared_error: 8.1572\n",
      "Epoch 570/700\n",
      "576/576 [==============================] - 0s 792us/step - loss: 8.0725 - mean_squared_error: 8.0725\n",
      "Epoch 571/700\n",
      "576/576 [==============================] - 0s 789us/step - loss: 8.0657 - mean_squared_error: 8.0657\n",
      "Epoch 572/700\n",
      "576/576 [==============================] - 0s 764us/step - loss: 8.1195 - mean_squared_error: 8.1195\n",
      "Epoch 573/700\n",
      "576/576 [==============================] - 0s 620us/step - loss: 8.0337 - mean_squared_error: 8.0337\n",
      "Epoch 574/700\n",
      "576/576 [==============================] - 0s 613us/step - loss: 8.0977 - mean_squared_error: 8.0977\n",
      "Epoch 575/700\n",
      "576/576 [==============================] - 0s 607us/step - loss: 8.1247 - mean_squared_error: 8.1247\n",
      "Epoch 576/700\n",
      "576/576 [==============================] - 0s 635us/step - loss: 8.0301 - mean_squared_error: 8.0301\n",
      "Epoch 577/700\n",
      "576/576 [==============================] - 0s 612us/step - loss: 8.1033 - mean_squared_error: 8.1033\n",
      "Epoch 578/700\n",
      "576/576 [==============================] - 0s 628us/step - loss: 8.1374 - mean_squared_error: 8.1374\n",
      "Epoch 579/700\n",
      "576/576 [==============================] - 0s 625us/step - loss: 8.0700 - mean_squared_error: 8.0700\n",
      "Epoch 580/700\n",
      "576/576 [==============================] - 0s 606us/step - loss: 8.0662 - mean_squared_error: 8.0662\n",
      "Epoch 581/700\n",
      "576/576 [==============================] - 0s 626us/step - loss: 8.0809 - mean_squared_error: 8.0809\n",
      "Epoch 582/700\n",
      "576/576 [==============================] - 0s 605us/step - loss: 8.1026 - mean_squared_error: 8.1026\n",
      "Epoch 583/700\n",
      "576/576 [==============================] - 0s 614us/step - loss: 8.0077 - mean_squared_error: 8.0077\n",
      "Epoch 584/700\n",
      "576/576 [==============================] - 0s 619us/step - loss: 8.1695 - mean_squared_error: 8.1695\n",
      "Epoch 585/700\n",
      "576/576 [==============================] - 0s 624us/step - loss: 8.0632 - mean_squared_error: 8.0632\n",
      "Epoch 586/700\n",
      "576/576 [==============================] - 0s 616us/step - loss: 8.2028 - mean_squared_error: 8.2028\n",
      "Epoch 587/700\n",
      "576/576 [==============================] - 0s 629us/step - loss: 8.0961 - mean_squared_error: 8.0961\n",
      "Epoch 588/700\n",
      "576/576 [==============================] - 0s 608us/step - loss: 8.2199 - mean_squared_error: 8.2199\n",
      "Epoch 589/700\n",
      "576/576 [==============================] - 0s 618us/step - loss: 8.1103 - mean_squared_error: 8.1103\n",
      "Epoch 590/700\n",
      "576/576 [==============================] - 0s 627us/step - loss: 8.0944 - mean_squared_error: 8.0944\n",
      "Epoch 591/700\n",
      "576/576 [==============================] - 0s 607us/step - loss: 8.1127 - mean_squared_error: 8.1127\n",
      "Epoch 592/700\n",
      "576/576 [==============================] - 0s 616us/step - loss: 8.0999 - mean_squared_error: 8.0999\n",
      "Epoch 593/700\n",
      "576/576 [==============================] - 0s 614us/step - loss: 8.1115 - mean_squared_error: 8.1115\n",
      "Epoch 594/700\n",
      "576/576 [==============================] - 0s 611us/step - loss: 8.1075 - mean_squared_error: 8.1075\n",
      "Epoch 595/700\n",
      "576/576 [==============================] - 0s 616us/step - loss: 8.1328 - mean_squared_error: 8.1328\n",
      "Epoch 596/700\n",
      "576/576 [==============================] - 0s 620us/step - loss: 8.0903 - mean_squared_error: 8.0903\n",
      "Epoch 597/700\n",
      "576/576 [==============================] - 0s 618us/step - loss: 8.0692 - mean_squared_error: 8.0692\n",
      "Epoch 598/700\n",
      "576/576 [==============================] - 0s 616us/step - loss: 8.1867 - mean_squared_error: 8.1867\n",
      "Epoch 599/700\n",
      "576/576 [==============================] - 0s 614us/step - loss: 8.0974 - mean_squared_error: 8.0974\n",
      "Epoch 600/700\n",
      "576/576 [==============================] - 0s 620us/step - loss: 8.0929 - mean_squared_error: 8.0929\n",
      "Epoch 601/700\n",
      "576/576 [==============================] - 0s 640us/step - loss: 8.0712 - mean_squared_error: 8.0712\n",
      "Epoch 602/700\n",
      "576/576 [==============================] - 0s 618us/step - loss: 8.1101 - mean_squared_error: 8.1101\n",
      "Epoch 603/700\n",
      "576/576 [==============================] - 0s 616us/step - loss: 8.0733 - mean_squared_error: 8.0733\n",
      "Epoch 604/700\n",
      "576/576 [==============================] - 0s 620us/step - loss: 8.0864 - mean_squared_error: 8.0864\n",
      "Epoch 605/700\n",
      "576/576 [==============================] - 0s 600us/step - loss: 8.1152 - mean_squared_error: 8.1152\n",
      "Epoch 606/700\n",
      "576/576 [==============================] - ETA: 0s - loss: 7.8614 - mean_squared_error: 7.86 - 0s 620us/step - loss: 8.1301 - mean_squared_error: 8.1301\n",
      "Epoch 607/700\n",
      "576/576 [==============================] - 0s 605us/step - loss: 8.0708 - mean_squared_error: 8.0708\n",
      "Epoch 608/700\n",
      "576/576 [==============================] - 0s 605us/step - loss: 8.0980 - mean_squared_error: 8.0980\n",
      "Epoch 609/700\n",
      "576/576 [==============================] - 0s 612us/step - loss: 8.0905 - mean_squared_error: 8.0905\n",
      "Epoch 610/700\n",
      "576/576 [==============================] - 0s 622us/step - loss: 8.0251 - mean_squared_error: 8.0251\n",
      "Epoch 611/700\n",
      "576/576 [==============================] - 0s 603us/step - loss: 8.0911 - mean_squared_error: 8.0911\n",
      "Epoch 612/700\n",
      "576/576 [==============================] - 0s 622us/step - loss: 8.0604 - mean_squared_error: 8.0604\n",
      "Epoch 613/700\n",
      "576/576 [==============================] - 0s 655us/step - loss: 8.0644 - mean_squared_error: 8.0644\n",
      "Epoch 614/700\n",
      "576/576 [==============================] - 0s 834us/step - loss: 8.1099 - mean_squared_error: 8.1099\n",
      "Epoch 615/700\n",
      "576/576 [==============================] - 0s 787us/step - loss: 8.0834 - mean_squared_error: 8.0834\n",
      "Epoch 616/700\n",
      "576/576 [==============================] - 0s 795us/step - loss: 8.0262 - mean_squared_error: 8.0262\n",
      "Epoch 617/700\n",
      "576/576 [==============================] - 0s 625us/step - loss: 8.0668 - mean_squared_error: 8.06680s - loss: 8.3623 - mean_squared_error\n",
      "Epoch 618/700\n",
      "576/576 [==============================] - 0s 619us/step - loss: 8.1051 - mean_squared_error: 8.1051\n",
      "Epoch 619/700\n",
      "576/576 [==============================] - 0s 623us/step - loss: 8.0871 - mean_squared_error: 8.0871\n",
      "Epoch 620/700\n",
      "576/576 [==============================] - 0s 612us/step - loss: 8.0630 - mean_squared_error: 8.0630\n",
      "Epoch 621/700\n",
      "576/576 [==============================] - 0s 608us/step - loss: 8.0894 - mean_squared_error: 8.0894\n",
      "Epoch 622/700\n",
      "576/576 [==============================] - 0s 633us/step - loss: 8.0767 - mean_squared_error: 8.0767\n",
      "Epoch 623/700\n",
      "576/576 [==============================] - 0s 611us/step - loss: 8.0904 - mean_squared_error: 8.0904\n",
      "Epoch 624/700\n",
      "576/576 [==============================] - 0s 620us/step - loss: 8.0565 - mean_squared_error: 8.0565\n",
      "Epoch 625/700\n",
      "576/576 [==============================] - 0s 611us/step - loss: 8.0738 - mean_squared_error: 8.0738\n",
      "Epoch 626/700\n",
      "576/576 [==============================] - 0s 599us/step - loss: 8.0980 - mean_squared_error: 8.0980\n",
      "Epoch 627/700\n",
      "576/576 [==============================] - 0s 607us/step - loss: 8.1248 - mean_squared_error: 8.1248\n",
      "Epoch 628/700\n",
      "576/576 [==============================] - 0s 625us/step - loss: 8.1323 - mean_squared_error: 8.1323\n",
      "Epoch 629/700\n",
      "576/576 [==============================] - 0s 606us/step - loss: 8.1419 - mean_squared_error: 8.1419\n",
      "Epoch 630/700\n",
      "576/576 [==============================] - 0s 613us/step - loss: 8.1649 - mean_squared_error: 8.1649\n",
      "Epoch 631/700\n",
      "576/576 [==============================] - 0s 621us/step - loss: 8.1080 - mean_squared_error: 8.1080\n",
      "Epoch 632/700\n",
      "576/576 [==============================] - 0s 613us/step - loss: 8.0660 - mean_squared_error: 8.0660\n",
      "Epoch 633/700\n",
      "576/576 [==============================] - 0s 634us/step - loss: 8.1248 - mean_squared_error: 8.1248\n",
      "Epoch 634/700\n",
      "576/576 [==============================] - 0s 628us/step - loss: 8.1273 - mean_squared_error: 8.1273\n",
      "Epoch 635/700\n",
      "576/576 [==============================] - 0s 617us/step - loss: 8.0340 - mean_squared_error: 8.0340\n",
      "Epoch 636/700\n",
      "576/576 [==============================] - 0s 622us/step - loss: 8.2411 - mean_squared_error: 8.2411\n",
      "Epoch 637/700\n",
      "576/576 [==============================] - 0s 621us/step - loss: 8.0445 - mean_squared_error: 8.0445\n",
      "Epoch 638/700\n",
      "576/576 [==============================] - 0s 610us/step - loss: 8.1177 - mean_squared_error: 8.1177\n",
      "Epoch 639/700\n",
      "576/576 [==============================] - 0s 636us/step - loss: 8.0640 - mean_squared_error: 8.0640\n",
      "Epoch 640/700\n",
      "576/576 [==============================] - 0s 614us/step - loss: 8.0805 - mean_squared_error: 8.0805\n",
      "Epoch 641/700\n",
      "576/576 [==============================] - 0s 618us/step - loss: 8.0439 - mean_squared_error: 8.0439\n",
      "Epoch 642/700\n",
      "576/576 [==============================] - 0s 613us/step - loss: 8.1259 - mean_squared_error: 8.1259\n",
      "Epoch 643/700\n",
      "576/576 [==============================] - 0s 597us/step - loss: 8.0909 - mean_squared_error: 8.0909\n",
      "Epoch 644/700\n",
      "576/576 [==============================] - 0s 601us/step - loss: 8.0779 - mean_squared_error: 8.0779\n",
      "Epoch 645/700\n",
      "576/576 [==============================] - 0s 623us/step - loss: 8.1050 - mean_squared_error: 8.1050\n",
      "Epoch 646/700\n",
      "576/576 [==============================] - 0s 616us/step - loss: 8.0884 - mean_squared_error: 8.0884\n",
      "Epoch 647/700\n",
      "576/576 [==============================] - 0s 637us/step - loss: 8.1422 - mean_squared_error: 8.1422\n",
      "Epoch 648/700\n",
      "576/576 [==============================] - 0s 629us/step - loss: 8.0959 - mean_squared_error: 8.0959\n",
      "Epoch 649/700\n",
      "576/576 [==============================] - 0s 598us/step - loss: 8.0607 - mean_squared_error: 8.0607\n",
      "Epoch 650/700\n",
      "576/576 [==============================] - 0s 613us/step - loss: 8.1043 - mean_squared_error: 8.1043\n",
      "Epoch 651/700\n",
      "576/576 [==============================] - 0s 619us/step - loss: 8.1602 - mean_squared_error: 8.1602\n",
      "Epoch 652/700\n",
      "576/576 [==============================] - 0s 614us/step - loss: 8.1194 - mean_squared_error: 8.1194\n",
      "Epoch 653/700\n",
      "576/576 [==============================] - 0s 618us/step - loss: 8.0419 - mean_squared_error: 8.0419\n",
      "Epoch 654/700\n",
      "576/576 [==============================] - 0s 604us/step - loss: 8.1494 - mean_squared_error: 8.1494\n",
      "Epoch 655/700\n",
      "576/576 [==============================] - 0s 618us/step - loss: 8.1424 - mean_squared_error: 8.1424\n",
      "Epoch 656/700\n",
      "576/576 [==============================] - 0s 626us/step - loss: 8.0793 - mean_squared_error: 8.0793\n",
      "Epoch 657/700\n",
      "576/576 [==============================] - 0s 675us/step - loss: 8.0943 - mean_squared_error: 8.0943\n",
      "Epoch 658/700\n",
      "576/576 [==============================] - 0s 782us/step - loss: 8.1270 - mean_squared_error: 8.1270\n",
      "Epoch 659/700\n",
      "576/576 [==============================] - 0s 785us/step - loss: 8.1263 - mean_squared_error: 8.1263\n",
      "Epoch 660/700\n",
      "576/576 [==============================] - 0s 760us/step - loss: 8.1553 - mean_squared_error: 8.1553\n",
      "Epoch 661/700\n",
      "576/576 [==============================] - 0s 628us/step - loss: 8.0531 - mean_squared_error: 8.0531\n",
      "Epoch 662/700\n",
      "576/576 [==============================] - 0s 613us/step - loss: 8.0719 - mean_squared_error: 8.0719\n",
      "Epoch 663/700\n",
      "576/576 [==============================] - 0s 621us/step - loss: 8.1007 - mean_squared_error: 8.1007\n",
      "Epoch 664/700\n",
      "576/576 [==============================] - 0s 612us/step - loss: 8.1127 - mean_squared_error: 8.1127\n",
      "Epoch 665/700\n",
      "576/576 [==============================] - 0s 594us/step - loss: 8.0973 - mean_squared_error: 8.0973\n",
      "Epoch 666/700\n",
      "576/576 [==============================] - 0s 629us/step - loss: 8.0794 - mean_squared_error: 8.0794\n",
      "Epoch 667/700\n",
      "576/576 [==============================] - 0s 609us/step - loss: 8.0665 - mean_squared_error: 8.0665\n",
      "Epoch 668/700\n",
      "576/576 [==============================] - 0s 614us/step - loss: 8.1763 - mean_squared_error: 8.1763\n",
      "Epoch 669/700\n",
      "576/576 [==============================] - 0s 616us/step - loss: 8.0676 - mean_squared_error: 8.0676\n",
      "Epoch 670/700\n",
      "576/576 [==============================] - 0s 609us/step - loss: 8.1400 - mean_squared_error: 8.1400\n",
      "Epoch 671/700\n",
      "576/576 [==============================] - 0s 618us/step - loss: 8.0399 - mean_squared_error: 8.0399\n",
      "Epoch 672/700\n",
      "576/576 [==============================] - 0s 617us/step - loss: 8.1185 - mean_squared_error: 8.1185\n",
      "Epoch 673/700\n",
      "576/576 [==============================] - 0s 625us/step - loss: 8.0735 - mean_squared_error: 8.0735\n",
      "Epoch 674/700\n",
      "576/576 [==============================] - 0s 620us/step - loss: 8.0935 - mean_squared_error: 8.0935\n",
      "Epoch 675/700\n",
      "576/576 [==============================] - 0s 618us/step - loss: 8.0684 - mean_squared_error: 8.0684\n",
      "Epoch 676/700\n",
      "576/576 [==============================] - 0s 609us/step - loss: 8.0624 - mean_squared_error: 8.0624\n",
      "Epoch 677/700\n",
      "576/576 [==============================] - 0s 613us/step - loss: 8.0714 - mean_squared_error: 8.0714\n",
      "Epoch 678/700\n",
      "576/576 [==============================] - 0s 618us/step - loss: 8.0975 - mean_squared_error: 8.0975\n",
      "Epoch 679/700\n",
      "576/576 [==============================] - 0s 608us/step - loss: 8.0567 - mean_squared_error: 8.0567\n",
      "Epoch 680/700\n",
      "576/576 [==============================] - 0s 623us/step - loss: 8.1106 - mean_squared_error: 8.1106\n",
      "Epoch 681/700\n",
      "576/576 [==============================] - 0s 606us/step - loss: 8.0552 - mean_squared_error: 8.0552\n",
      "Epoch 682/700\n",
      "576/576 [==============================] - 0s 608us/step - loss: 8.0714 - mean_squared_error: 8.0714\n",
      "Epoch 683/700\n",
      "576/576 [==============================] - 0s 623us/step - loss: 8.1227 - mean_squared_error: 8.1227\n",
      "Epoch 684/700\n",
      "576/576 [==============================] - 0s 612us/step - loss: 8.1235 - mean_squared_error: 8.1235\n",
      "Epoch 685/700\n",
      "576/576 [==============================] - 0s 613us/step - loss: 8.0670 - mean_squared_error: 8.0670\n",
      "Epoch 686/700\n",
      "576/576 [==============================] - 0s 628us/step - loss: 8.1780 - mean_squared_error: 8.1780\n",
      "Epoch 687/700\n",
      "576/576 [==============================] - 0s 615us/step - loss: 8.0789 - mean_squared_error: 8.0789\n",
      "Epoch 688/700\n",
      "576/576 [==============================] - 0s 627us/step - loss: 8.0336 - mean_squared_error: 8.0336\n",
      "Epoch 689/700\n",
      "576/576 [==============================] - 0s 603us/step - loss: 8.0534 - mean_squared_error: 8.0534\n",
      "Epoch 690/700\n",
      "576/576 [==============================] - 0s 607us/step - loss: 8.0316 - mean_squared_error: 8.0316\n",
      "Epoch 691/700\n",
      "576/576 [==============================] - 0s 614us/step - loss: 8.1453 - mean_squared_error: 8.1453\n",
      "Epoch 692/700\n",
      "576/576 [==============================] - 0s 635us/step - loss: 8.0423 - mean_squared_error: 8.0423\n",
      "Epoch 693/700\n",
      "576/576 [==============================] - 0s 602us/step - loss: 8.0570 - mean_squared_error: 8.0570\n",
      "Epoch 694/700\n",
      "576/576 [==============================] - 0s 628us/step - loss: 8.0759 - mean_squared_error: 8.0759\n",
      "Epoch 695/700\n",
      "576/576 [==============================] - 0s 620us/step - loss: 8.0559 - mean_squared_error: 8.0559\n",
      "Epoch 696/700\n",
      "576/576 [==============================] - 0s 627us/step - loss: 8.0734 - mean_squared_error: 8.0734\n",
      "Epoch 697/700\n",
      "576/576 [==============================] - 0s 618us/step - loss: 8.0795 - mean_squared_error: 8.0795\n",
      "Epoch 698/700\n",
      "576/576 [==============================] - 0s 606us/step - loss: 8.0720 - mean_squared_error: 8.0720\n",
      "Epoch 699/700\n",
      "576/576 [==============================] - 0s 616us/step - loss: 8.1174 - mean_squared_error: 8.1174\n",
      "Epoch 700/700\n",
      "576/576 [==============================] - 0s 618us/step - loss: 8.1025 - mean_squared_error: 8.1025\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x247569b4390>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%timeit\n",
    "model.fit(X_train, y_train, epochs = 700, batch_size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train r2: 0.92\n",
      "Test r2: 0.94\n"
     ]
    }
   ],
   "source": [
    "y_train_predict = model.predict(X_train)\n",
    "y_test_predict = model.predict(X_test)\n",
    "\n",
    "print('Train r2: {:.2f}'.format(r2_score(y_train, y_train_predict)))\n",
    "print('Test r2: {:.2f}'.format(r2_score(y_test, y_test_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 16.80813408],\n",
       "       [ 17.85715294],\n",
       "       [ 11.82686901],\n",
       "       [ 28.0340538 ],\n",
       "       [ 17.13858986],\n",
       "       [ 10.10619545],\n",
       "       [ 12.20678043],\n",
       "       [ 29.99055672],\n",
       "       [ 33.22487259],\n",
       "       [ 14.79161644],\n",
       "       [ 29.59576988],\n",
       "       [ 13.36938477],\n",
       "       [ 11.37421894],\n",
       "       [ 30.5976429 ],\n",
       "       [ 33.89910126],\n",
       "       [ 16.1223278 ],\n",
       "       [ 37.79653931],\n",
       "       [ 11.54465866],\n",
       "       [ 31.30694199],\n",
       "       [ 11.96498871],\n",
       "       [ 28.94182205],\n",
       "       [ 13.40360355],\n",
       "       [ 33.00215149],\n",
       "       [ 28.35313988],\n",
       "       [ 14.09403992],\n",
       "       [ 24.3465519 ],\n",
       "       [ 13.23223686],\n",
       "       [ 31.58051491],\n",
       "       [ 29.27227592],\n",
       "       [ 14.56171227],\n",
       "       [ 11.89932442],\n",
       "       [ 12.05669022],\n",
       "       [ 13.40360355],\n",
       "       [ 33.69973373],\n",
       "       [ 32.3759346 ],\n",
       "       [ 28.94182205],\n",
       "       [ 32.5344429 ],\n",
       "       [ 10.26582718],\n",
       "       [ 15.32365608],\n",
       "       [ 24.74011612],\n",
       "       [ 14.46663666],\n",
       "       [ 24.74011612],\n",
       "       [ 31.30694199],\n",
       "       [ 28.21720695],\n",
       "       [ 32.25806808],\n",
       "       [ 37.83054352],\n",
       "       [ 14.09403992],\n",
       "       [ 32.77203369],\n",
       "       [ 10.32178497],\n",
       "       [ 36.17342377],\n",
       "       [ 10.98817635],\n",
       "       [ 34.53871155],\n",
       "       [ 13.3175602 ],\n",
       "       [ 15.52629852],\n",
       "       [ 34.72193527],\n",
       "       [ 15.91239357],\n",
       "       [ 13.28430557],\n",
       "       [ 12.20678043],\n",
       "       [ 11.90945244],\n",
       "       [ 24.29693413],\n",
       "       [ 32.28931427],\n",
       "       [ 15.52629852],\n",
       "       [ 15.4246521 ],\n",
       "       [ 33.89910126],\n",
       "       [ 17.85715294],\n",
       "       [ 13.82143307],\n",
       "       [ 10.10619545],\n",
       "       [ 31.58051491],\n",
       "       [ 10.86243248],\n",
       "       [ 31.33004379],\n",
       "       [ 37.83054352],\n",
       "       [ 15.62859344],\n",
       "       [ 34.55303192],\n",
       "       [ 12.13137817],\n",
       "       [ 33.22487259],\n",
       "       [ 14.18558884],\n",
       "       [ 24.3465519 ],\n",
       "       [ 14.59885979],\n",
       "       [ 15.08506966],\n",
       "       [ 18.08646774],\n",
       "       [ 29.23121643],\n",
       "       [ 14.69489479],\n",
       "       [ 34.27716827],\n",
       "       [ 23.50232887],\n",
       "       [ 13.3175602 ],\n",
       "       [ 34.53871155],\n",
       "       [ 13.57784843],\n",
       "       [ 28.21720695],\n",
       "       [ 24.3465519 ],\n",
       "       [ 13.23223686],\n",
       "       [ 18.29018593],\n",
       "       [ 28.94182205],\n",
       "       [ 15.32365608],\n",
       "       [ 36.37950134],\n",
       "       [ 16.0170517 ],\n",
       "       [ 12.59392643],\n",
       "       [ 23.90060616],\n",
       "       [ 34.884552  ],\n",
       "       [ 34.87129211],\n",
       "       [ 35.04104614],\n",
       "       [ 10.83198166],\n",
       "       [ 36.06364059],\n",
       "       [ 11.33990288],\n",
       "       [ 10.37837601],\n",
       "       [ 34.72193527],\n",
       "       [ 28.38194656],\n",
       "       [ 27.32009888],\n",
       "       [ 14.69489479],\n",
       "       [ 11.75512218],\n",
       "       [ 23.05384445],\n",
       "       [ 35.84857941],\n",
       "       [ 11.90945244],\n",
       "       [ 14.27857208],\n",
       "       [ 37.14427948],\n",
       "       [ 29.23121643],\n",
       "       [ 11.27303505],\n",
       "       [ 13.19926262],\n",
       "       [ 12.86696815],\n",
       "       [ 14.9863205 ],\n",
       "       [ 11.82686901],\n",
       "       [ 27.65015602],\n",
       "       [ 26.95462418],\n",
       "       [ 12.51493645],\n",
       "       [ 31.33004379],\n",
       "       [ 23.10257721],\n",
       "       [ 14.56171227],\n",
       "       [ 10.29424095],\n",
       "       [ 32.10474014],\n",
       "       [ 32.3759346 ],\n",
       "       [ 30.5976429 ],\n",
       "       [ 13.45586205],\n",
       "       [ 12.4366684 ],\n",
       "       [ 31.33004379],\n",
       "       [ 17.7432003 ],\n",
       "       [ 37.38191605],\n",
       "       [ 33.19742584],\n",
       "       [ 37.26852036],\n",
       "       [ 10.73935318],\n",
       "       [ 11.98271561],\n",
       "       [ 32.3759346 ],\n",
       "       [ 15.73152733],\n",
       "       [ 28.0340538 ],\n",
       "       [ 37.14427948],\n",
       "       [ 35.04104614],\n",
       "       [ 37.86297226],\n",
       "       [ 17.24982452],\n",
       "       [ 34.87129211],\n",
       "       [ 15.38530922],\n",
       "       [ 10.92496872],\n",
       "       [ 10.43560028],\n",
       "       [ 15.28456879],\n",
       "       [ 24.3465519 ],\n",
       "       [ 11.40746117],\n",
       "       [ 13.28430557],\n",
       "       [ 10.26582718],\n",
       "       [ 10.86243248],\n",
       "       [ 33.49330902],\n",
       "       [ 23.08737564],\n",
       "       [ 34.09152985],\n",
       "       [ 19.19024277],\n",
       "       [ 32.25806808],\n",
       "       [ 14.00246239],\n",
       "       [ 27.32009888],\n",
       "       [ 12.59392643],\n",
       "       [ 11.54465866],\n",
       "       [ 14.9863205 ],\n",
       "       [ 12.05669022],\n",
       "       [ 31.33004379],\n",
       "       [ 10.29424095],\n",
       "       [ 10.8005619 ],\n",
       "       [ 10.92496872],\n",
       "       [ 26.92382431],\n",
       "       [ 13.11494255],\n",
       "       [ 16.91773605],\n",
       "       [ 16.0170517 ],\n",
       "       [ 17.02789116],\n",
       "       [ 35.04104614],\n",
       "       [ 14.27857208],\n",
       "       [ 23.08737564],\n",
       "       [ 13.40360355],\n",
       "       [ 14.56171227],\n",
       "       [ 28.35313988],\n",
       "       [ 16.80813408],\n",
       "       [ 10.92496872],\n",
       "       [ 31.60296822],\n",
       "       [ 10.37837601],\n",
       "       [ 13.3175602 ],\n",
       "       [ 16.0170517 ],\n",
       "       [ 14.9863205 ],\n",
       "       [ 32.50415802],\n",
       "       [ 12.4366684 ],\n",
       "       [ 32.3759346 ]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict = model.predict(X_test)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cooling Load Y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Keras GridSearch \n",
    "GridSearchCV is used to find the best parameters for Deep learning model( Epoch 5, batch size 500). \n",
    "\n",
    "R-squared and prediction are required outputs from such configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X2,y2, random_state = 10)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "def create_model():\n",
    "    #create model\n",
    "    model = Sequential() \n",
    "    model.add(Dense(6, input_dim=6, kernel_initializer='normal', activation='relu')) # by default, kernel_init has uniform weights\n",
    "    model.add(Dense(5, kernel_initializer='uniform', activation='sigmoid')) # hidden layer\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    #compile model\n",
    "    model.compile(loss='mse', optimizer='adam' , metrics = ['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = {'epochs':[500,700] , 'batch_size':[5,20]}\n",
    "keras_regressor = KerasRegressor(build_fn = create_model , verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 40min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "grid_search = GridSearchCV(keras_regressor, param_grid , cv=5, scoring='r2' )\n",
    "grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'batch_size': 5, 'epochs': 500}\n",
      "Best cross-validation score: 0.884600\n",
      "Best estimator:\n",
      "<keras.wrappers.scikit_learn.KerasRegressor object at 0x00000247817CDDD8>\n",
      "0.888633569253 0.919355634814\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters: {}\".format(grid_search.best_params_)) \n",
    "print(\"Best cross-validation score: {:.6f}\".format(grid_search.best_score_)) \n",
    "print(\"Best estimator:\\n{}\".format(grid_search.best_estimator_))\n",
    "\n",
    "train_score = grid_search.score(X_train,y_train)\n",
    "test_score = grid_search.score(X_test,y_test)\n",
    "print(train_score, test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prediction of Cooling Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = Sequential() \n",
    "model.add(Dense(6, input_dim=6, kernel_initializer='normal', activation='relu')) # by default, kernel_init has uniform weights\n",
    "model.add(Dense(5, kernel_initializer='uniform', activation='sigmoid')) # hidden layer\n",
    "model.add(Dense(1, kernel_initializer='normal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile model \n",
    "model.compile(loss='mse', optimizer='adam' , metrics = ['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "576/576 [==============================] - 4s 7ms/step - loss: 702.1587 - mean_squared_error: 702.1587\n",
      "Epoch 2/500\n",
      "576/576 [==============================] - 0s 592us/step - loss: 672.4704 - mean_squared_error: 672.4704\n",
      "Epoch 3/500\n",
      "576/576 [==============================] - 0s 581us/step - loss: 628.5146 - mean_squared_error: 628.5146\n",
      "Epoch 4/500\n",
      "576/576 [==============================] - 0s 609us/step - loss: 587.7823 - mean_squared_error: 587.7823\n",
      "Epoch 5/500\n",
      "576/576 [==============================] - 0s 592us/step - loss: 553.2295 - mean_squared_error: 553.2295\n",
      "Epoch 6/500\n",
      "576/576 [==============================] - 0s 649us/step - loss: 522.3173 - mean_squared_error: 522.3173\n",
      "Epoch 7/500\n",
      "576/576 [==============================] - 0s 635us/step - loss: 493.8168 - mean_squared_error: 493.8168\n",
      "Epoch 8/500\n",
      "576/576 [==============================] - 0s 644us/step - loss: 467.3133 - mean_squared_error: 467.3133\n",
      "Epoch 9/500\n",
      "576/576 [==============================] - 0s 649us/step - loss: 442.4713 - mean_squared_error: 442.4713\n",
      "Epoch 10/500\n",
      "576/576 [==============================] - 0s 632us/step - loss: 419.1125 - mean_squared_error: 419.1125\n",
      "Epoch 11/500\n",
      "576/576 [==============================] - 0s 606us/step - loss: 397.0071 - mean_squared_error: 397.0071\n",
      "Epoch 12/500\n",
      "576/576 [==============================] - 0s 608us/step - loss: 376.0022 - mean_squared_error: 376.0022\n",
      "Epoch 13/500\n",
      "576/576 [==============================] - 0s 627us/step - loss: 356.1563 - mean_squared_error: 356.1563\n",
      "Epoch 14/500\n",
      "576/576 [==============================] - 0s 616us/step - loss: 337.5001 - mean_squared_error: 337.5001\n",
      "Epoch 15/500\n",
      "576/576 [==============================] - 0s 623us/step - loss: 319.8412 - mean_squared_error: 319.8412\n",
      "Epoch 16/500\n",
      "576/576 [==============================] - ETA: 0s - loss: 299.4153 - mean_squared_error: 299.41 - 0s 620us/step - loss: 303.0005 - mean_squared_error: 303.0005\n",
      "Epoch 17/500\n",
      "576/576 [==============================] - 0s 649us/step - loss: 287.1771 - mean_squared_error: 287.1771\n",
      "Epoch 18/500\n",
      "576/576 [==============================] - 0s 621us/step - loss: 272.2254 - mean_squared_error: 272.2254\n",
      "Epoch 19/500\n",
      "576/576 [==============================] - 0s 641us/step - loss: 258.1495 - mean_squared_error: 258.1495\n",
      "Epoch 20/500\n",
      "576/576 [==============================] - 0s 644us/step - loss: 244.7916 - mean_squared_error: 244.7916\n",
      "Epoch 21/500\n",
      "576/576 [==============================] - 0s 611us/step - loss: 232.2678 - mean_squared_error: 232.2678\n",
      "Epoch 22/500\n",
      "576/576 [==============================] - 0s 641us/step - loss: 220.4856 - mean_squared_error: 220.4856\n",
      "Epoch 23/500\n",
      "576/576 [==============================] - 0s 609us/step - loss: 209.3913 - mean_squared_error: 209.3913\n",
      "Epoch 24/500\n",
      "576/576 [==============================] - 0s 614us/step - loss: 198.9941 - mean_squared_error: 198.9941\n",
      "Epoch 25/500\n",
      "576/576 [==============================] - 0s 642us/step - loss: 189.1611 - mean_squared_error: 189.1611\n",
      "Epoch 26/500\n",
      "576/576 [==============================] - 0s 714us/step - loss: 179.9690 - mean_squared_error: 179.9690\n",
      "Epoch 27/500\n",
      "576/576 [==============================] - 0s 773us/step - loss: 171.5574 - mean_squared_error: 171.5574\n",
      "Epoch 28/500\n",
      "576/576 [==============================] - 0s 766us/step - loss: 163.6791 - mean_squared_error: 163.6791\n",
      "Epoch 29/500\n",
      "576/576 [==============================] - 0s 702us/step - loss: 156.4083 - mean_squared_error: 156.4083\n",
      "Epoch 30/500\n",
      "576/576 [==============================] - 0s 635us/step - loss: 149.5732 - mean_squared_error: 149.5732\n",
      "Epoch 31/500\n",
      "576/576 [==============================] - 0s 649us/step - loss: 143.3233 - mean_squared_error: 143.3233\n",
      "Epoch 32/500\n",
      "576/576 [==============================] - 0s 618us/step - loss: 137.5755 - mean_squared_error: 137.5755\n",
      "Epoch 33/500\n",
      "576/576 [==============================] - 0s 635us/step - loss: 132.3190 - mean_squared_error: 132.3190\n",
      "Epoch 34/500\n",
      "576/576 [==============================] - 0s 646us/step - loss: 127.5500 - mean_squared_error: 127.5500\n",
      "Epoch 35/500\n",
      "576/576 [==============================] - 0s 653us/step - loss: 123.1988 - mean_squared_error: 123.1988\n",
      "Epoch 36/500\n",
      "576/576 [==============================] - 0s 663us/step - loss: 119.2527 - mean_squared_error: 119.2527\n",
      "Epoch 37/500\n",
      "576/576 [==============================] - 0s 681us/step - loss: 115.5937 - mean_squared_error: 115.5937\n",
      "Epoch 38/500\n",
      "576/576 [==============================] - 0s 625us/step - loss: 112.4042 - mean_squared_error: 112.4042\n",
      "Epoch 39/500\n",
      "576/576 [==============================] - 0s 679us/step - loss: 109.5175 - mean_squared_error: 109.5175\n",
      "Epoch 40/500\n",
      "576/576 [==============================] - 0s 691us/step - loss: 106.9790 - mean_squared_error: 106.9790\n",
      "Epoch 41/500\n",
      "576/576 [==============================] - 0s 597us/step - loss: 104.6747 - mean_squared_error: 104.6747\n",
      "Epoch 42/500\n",
      "576/576 [==============================] - 0s 655us/step - loss: 102.6014 - mean_squared_error: 102.6014\n",
      "Epoch 43/500\n",
      "576/576 [==============================] - 0s 661us/step - loss: 100.7990 - mean_squared_error: 100.7990\n",
      "Epoch 44/500\n",
      "576/576 [==============================] - 0s 642us/step - loss: 98.7555 - mean_squared_error: 98.7555\n",
      "Epoch 45/500\n",
      "576/576 [==============================] - 0s 646us/step - loss: 85.6501 - mean_squared_error: 85.6501\n",
      "Epoch 46/500\n",
      "576/576 [==============================] - 0s 644us/step - loss: 74.3701 - mean_squared_error: 74.3701\n",
      "Epoch 47/500\n",
      "576/576 [==============================] - 0s 637us/step - loss: 68.7147 - mean_squared_error: 68.7147\n",
      "Epoch 48/500\n",
      "576/576 [==============================] - 0s 641us/step - loss: 63.8133 - mean_squared_error: 63.8133\n",
      "Epoch 49/500\n",
      "576/576 [==============================] - 0s 665us/step - loss: 59.3124 - mean_squared_error: 59.3124\n",
      "Epoch 50/500\n",
      "576/576 [==============================] - 0s 616us/step - loss: 55.3010 - mean_squared_error: 55.3010\n",
      "Epoch 51/500\n",
      "576/576 [==============================] - 0s 632us/step - loss: 51.5764 - mean_squared_error: 51.5764\n",
      "Epoch 52/500\n",
      "576/576 [==============================] - 0s 663us/step - loss: 48.0946 - mean_squared_error: 48.0946\n",
      "Epoch 53/500\n",
      "576/576 [==============================] - 0s 602us/step - loss: 44.9480 - mean_squared_error: 44.9480\n",
      "Epoch 54/500\n",
      "576/576 [==============================] - 0s 562us/step - loss: 42.0706 - mean_squared_error: 42.0706\n",
      "Epoch 55/500\n",
      "576/576 [==============================] - 0s 618us/step - loss: 39.3744 - mean_squared_error: 39.3744\n",
      "Epoch 56/500\n",
      "576/576 [==============================] - 0s 639us/step - loss: 36.9755 - mean_squared_error: 36.9755\n",
      "Epoch 57/500\n",
      "576/576 [==============================] - 0s 618us/step - loss: 34.7854 - mean_squared_error: 34.7854\n",
      "Epoch 58/500\n",
      "576/576 [==============================] - 0s 602us/step - loss: 32.7836 - mean_squared_error: 32.7836\n",
      "Epoch 59/500\n",
      "576/576 [==============================] - 0s 644us/step - loss: 30.9649 - mean_squared_error: 30.9649\n",
      "Epoch 60/500\n",
      "576/576 [==============================] - 0s 663us/step - loss: 29.3500 - mean_squared_error: 29.3500\n",
      "Epoch 61/500\n",
      "576/576 [==============================] - 0s 628us/step - loss: 27.8448 - mean_squared_error: 27.8448\n",
      "Epoch 62/500\n",
      "576/576 [==============================] - 0s 639us/step - loss: 26.4949 - mean_squared_error: 26.4949\n",
      "Epoch 63/500\n",
      "576/576 [==============================] - 0s 625us/step - loss: 25.3014 - mean_squared_error: 25.3014\n",
      "Epoch 64/500\n",
      "576/576 [==============================] - 0s 620us/step - loss: 24.2232 - mean_squared_error: 24.2232\n",
      "Epoch 65/500\n",
      "576/576 [==============================] - 0s 637us/step - loss: 23.3156 - mean_squared_error: 23.3156\n",
      "Epoch 66/500\n",
      "576/576 [==============================] - 0s 623us/step - loss: 22.4742 - mean_squared_error: 22.4742\n",
      "Epoch 67/500\n",
      "576/576 [==============================] - 0s 627us/step - loss: 21.7361 - mean_squared_error: 21.7361\n",
      "Epoch 68/500\n",
      "576/576 [==============================] - 0s 639us/step - loss: 21.1323 - mean_squared_error: 21.1323\n",
      "Epoch 69/500\n",
      "576/576 [==============================] - 0s 731us/step - loss: 20.5737 - mean_squared_error: 20.5737\n",
      "Epoch 70/500\n",
      "576/576 [==============================] - 0s 742us/step - loss: 20.1100 - mean_squared_error: 20.1100\n",
      "Epoch 71/500\n",
      "576/576 [==============================] - 0s 649us/step - loss: 19.7207 - mean_squared_error: 19.7207\n",
      "Epoch 72/500\n",
      "576/576 [==============================] - 0s 519us/step - loss: 19.3952 - mean_squared_error: 19.3952\n",
      "Epoch 73/500\n",
      "576/576 [==============================] - 0s 661us/step - loss: 19.0772 - mean_squared_error: 19.0772\n",
      "Epoch 74/500\n",
      "576/576 [==============================] - 0s 599us/step - loss: 18.8390 - mean_squared_error: 18.8390\n",
      "Epoch 75/500\n",
      "576/576 [==============================] - 0s 590us/step - loss: 18.6303 - mean_squared_error: 18.6303\n",
      "Epoch 76/500\n",
      "576/576 [==============================] - 0s 606us/step - loss: 18.5455 - mean_squared_error: 18.5455\n",
      "Epoch 77/500\n",
      "576/576 [==============================] - 0s 562us/step - loss: 18.3460 - mean_squared_error: 18.3460\n",
      "Epoch 78/500\n",
      "576/576 [==============================] - 0s 604us/step - loss: 18.2477 - mean_squared_error: 18.2477\n",
      "Epoch 79/500\n",
      "576/576 [==============================] - 0s 548us/step - loss: 18.1435 - mean_squared_error: 18.1435\n",
      "Epoch 80/500\n",
      "576/576 [==============================] - 0s 616us/step - loss: 18.0910 - mean_squared_error: 18.0910\n",
      "Epoch 81/500\n",
      "576/576 [==============================] - 0s 602us/step - loss: 17.9954 - mean_squared_error: 17.9954\n",
      "Epoch 82/500\n",
      "576/576 [==============================] - 0s 571us/step - loss: 17.9604 - mean_squared_error: 17.9604\n",
      "Epoch 83/500\n",
      "576/576 [==============================] - 0s 562us/step - loss: 17.9064 - mean_squared_error: 17.9064\n",
      "Epoch 84/500\n",
      "576/576 [==============================] - 0s 621us/step - loss: 17.8885 - mean_squared_error: 17.8885\n",
      "Epoch 85/500\n",
      "576/576 [==============================] - 0s 608us/step - loss: 17.8413 - mean_squared_error: 17.8413\n",
      "Epoch 86/500\n",
      "576/576 [==============================] - 0s 562us/step - loss: 17.8605 - mean_squared_error: 17.8605\n",
      "Epoch 87/500\n",
      "576/576 [==============================] - 0s 595us/step - loss: 17.8378 - mean_squared_error: 17.8378\n",
      "Epoch 88/500\n",
      "576/576 [==============================] - 0s 564us/step - loss: 17.8374 - mean_squared_error: 17.8374\n",
      "Epoch 89/500\n",
      "576/576 [==============================] - 0s 578us/step - loss: 17.8119 - mean_squared_error: 17.8119\n",
      "Epoch 90/500\n",
      "576/576 [==============================] - 0s 599us/step - loss: 17.8019 - mean_squared_error: 17.8019\n",
      "Epoch 91/500\n",
      "576/576 [==============================] - 0s 597us/step - loss: 17.7964 - mean_squared_error: 17.7964\n",
      "Epoch 92/500\n",
      "576/576 [==============================] - 0s 608us/step - loss: 17.7764 - mean_squared_error: 17.7764\n",
      "Epoch 93/500\n",
      "576/576 [==============================] - 0s 571us/step - loss: 17.7848 - mean_squared_error: 17.7848\n",
      "Epoch 94/500\n",
      "576/576 [==============================] - 0s 550us/step - loss: 17.7800 - mean_squared_error: 17.7800\n",
      "Epoch 95/500\n",
      "576/576 [==============================] - 0s 602us/step - loss: 17.7783 - mean_squared_error: 17.7783\n",
      "Epoch 96/500\n",
      "576/576 [==============================] - 0s 581us/step - loss: 17.7729 - mean_squared_error: 17.7729\n",
      "Epoch 97/500\n",
      "576/576 [==============================] - 0s 594us/step - loss: 17.7831 - mean_squared_error: 17.7831\n",
      "Epoch 98/500\n",
      "576/576 [==============================] - 0s 581us/step - loss: 17.7566 - mean_squared_error: 17.7566\n",
      "Epoch 99/500\n",
      "576/576 [==============================] - 0s 576us/step - loss: 17.7705 - mean_squared_error: 17.7705\n",
      "Epoch 100/500\n",
      "576/576 [==============================] - 0s 590us/step - loss: 17.7504 - mean_squared_error: 17.7504\n",
      "Epoch 101/500\n",
      "576/576 [==============================] - 0s 581us/step - loss: 17.7763 - mean_squared_error: 17.7763\n",
      "Epoch 102/500\n",
      "576/576 [==============================] - 0s 606us/step - loss: 17.7477 - mean_squared_error: 17.7477\n",
      "Epoch 103/500\n",
      "576/576 [==============================] - 0s 621us/step - loss: 17.7596 - mean_squared_error: 17.7596\n",
      "Epoch 104/500\n",
      "576/576 [==============================] - 0s 614us/step - loss: 17.7342 - mean_squared_error: 17.7342\n",
      "Epoch 105/500\n",
      "576/576 [==============================] - 0s 599us/step - loss: 17.7357 - mean_squared_error: 17.7357\n",
      "Epoch 106/500\n",
      "576/576 [==============================] - 0s 581us/step - loss: 17.7441 - mean_squared_error: 17.7441\n",
      "Epoch 107/500\n",
      "576/576 [==============================] - 0s 567us/step - loss: 17.7119 - mean_squared_error: 17.7119\n",
      "Epoch 108/500\n",
      "576/576 [==============================] - 0s 595us/step - loss: 17.7230 - mean_squared_error: 17.7230\n",
      "Epoch 109/500\n",
      "576/576 [==============================] - 0s 599us/step - loss: 17.7098 - mean_squared_error: 17.7098\n",
      "Epoch 110/500\n",
      "576/576 [==============================] - 0s 604us/step - loss: 17.7202 - mean_squared_error: 17.7202\n",
      "Epoch 111/500\n",
      "576/576 [==============================] - 0s 590us/step - loss: 17.7042 - mean_squared_error: 17.7042\n",
      "Epoch 112/500\n",
      "576/576 [==============================] - 0s 571us/step - loss: 17.7138 - mean_squared_error: 17.7138\n",
      "Epoch 113/500\n",
      "576/576 [==============================] - 0s 578us/step - loss: 17.6951 - mean_squared_error: 17.6951\n",
      "Epoch 114/500\n",
      "576/576 [==============================] - 0s 595us/step - loss: 17.6737 - mean_squared_error: 17.6737\n",
      "Epoch 115/500\n",
      "576/576 [==============================] - 0s 614us/step - loss: 17.6528 - mean_squared_error: 17.6528\n",
      "Epoch 116/500\n",
      "576/576 [==============================] - 0s 726us/step - loss: 17.6648 - mean_squared_error: 17.6648\n",
      "Epoch 117/500\n",
      "576/576 [==============================] - 0s 729us/step - loss: 17.6397 - mean_squared_error: 17.6397\n",
      "Epoch 118/500\n",
      "576/576 [==============================] - 0s 642us/step - loss: 17.6183 - mean_squared_error: 17.6183\n",
      "Epoch 119/500\n",
      "576/576 [==============================] - 0s 597us/step - loss: 17.6080 - mean_squared_error: 17.6080\n",
      "Epoch 120/500\n",
      "576/576 [==============================] - 0s 623us/step - loss: 17.5638 - mean_squared_error: 17.5638\n",
      "Epoch 121/500\n",
      "576/576 [==============================] - 0s 656us/step - loss: 17.5692 - mean_squared_error: 17.5692\n",
      "Epoch 122/500\n",
      "576/576 [==============================] - 0s 576us/step - loss: 17.4956 - mean_squared_error: 17.4956\n",
      "Epoch 123/500\n",
      "576/576 [==============================] - 0s 621us/step - loss: 17.4662 - mean_squared_error: 17.4662\n",
      "Epoch 124/500\n",
      "576/576 [==============================] - 0s 686us/step - loss: 17.4014 - mean_squared_error: 17.4014\n",
      "Epoch 125/500\n",
      "576/576 [==============================] - 0s 606us/step - loss: 17.3638 - mean_squared_error: 17.3638\n",
      "Epoch 126/500\n",
      "576/576 [==============================] - 0s 592us/step - loss: 17.2857 - mean_squared_error: 17.2857\n",
      "Epoch 127/500\n",
      "576/576 [==============================] - 0s 660us/step - loss: 17.2231 - mean_squared_error: 17.2231\n",
      "Epoch 128/500\n",
      "576/576 [==============================] - 0s 571us/step - loss: 17.1432 - mean_squared_error: 17.1432\n",
      "Epoch 129/500\n",
      "576/576 [==============================] - 0s 653us/step - loss: 17.0960 - mean_squared_error: 17.0960\n",
      "Epoch 130/500\n",
      "576/576 [==============================] - 0s 580us/step - loss: 17.0007 - mean_squared_error: 17.0007\n",
      "Epoch 131/500\n",
      "576/576 [==============================] - 0s 637us/step - loss: 16.9147 - mean_squared_error: 16.9147\n",
      "Epoch 132/500\n",
      "576/576 [==============================] - 0s 538us/step - loss: 16.7959 - mean_squared_error: 16.7959\n",
      "Epoch 133/500\n",
      "576/576 [==============================] - 0s 569us/step - loss: 16.6994 - mean_squared_error: 16.6994\n",
      "Epoch 134/500\n",
      "576/576 [==============================] - 0s 566us/step - loss: 16.6300 - mean_squared_error: 16.6300\n",
      "Epoch 135/500\n",
      "576/576 [==============================] - 0s 601us/step - loss: 16.5122 - mean_squared_error: 16.5122\n",
      "Epoch 136/500\n",
      "576/576 [==============================] - 0s 613us/step - loss: 16.3732 - mean_squared_error: 16.3732\n",
      "Epoch 137/500\n",
      "576/576 [==============================] - 0s 566us/step - loss: 16.3421 - mean_squared_error: 16.3421\n",
      "Epoch 138/500\n",
      "576/576 [==============================] - 0s 606us/step - loss: 16.1202 - mean_squared_error: 16.1202\n",
      "Epoch 139/500\n",
      "576/576 [==============================] - 0s 625us/step - loss: 16.0585 - mean_squared_error: 16.0585\n",
      "Epoch 140/500\n",
      "576/576 [==============================] - 0s 602us/step - loss: 15.8673 - mean_squared_error: 15.8673\n",
      "Epoch 141/500\n",
      "576/576 [==============================] - 0s 520us/step - loss: 15.7357 - mean_squared_error: 15.7357\n",
      "Epoch 142/500\n",
      "576/576 [==============================] - 0s 554us/step - loss: 15.6113 - mean_squared_error: 15.61130s - loss: 14.0182 - mean_squared_er\n",
      "Epoch 143/500\n",
      "576/576 [==============================] - 0s 667us/step - loss: 15.4786 - mean_squared_error: 15.4786\n",
      "Epoch 144/500\n",
      "576/576 [==============================] - 0s 561us/step - loss: 15.3929 - mean_squared_error: 15.3929\n",
      "Epoch 145/500\n",
      "576/576 [==============================] - 0s 547us/step - loss: 15.2527 - mean_squared_error: 15.2527\n",
      "Epoch 146/500\n",
      "576/576 [==============================] - 0s 550us/step - loss: 15.1036 - mean_squared_error: 15.1036\n",
      "Epoch 147/500\n",
      "576/576 [==============================] - 0s 562us/step - loss: 14.9809 - mean_squared_error: 14.9809\n",
      "Epoch 148/500\n",
      "576/576 [==============================] - 0s 580us/step - loss: 14.8975 - mean_squared_error: 14.8975\n",
      "Epoch 149/500\n",
      "576/576 [==============================] - 0s 550us/step - loss: 14.7157 - mean_squared_error: 14.7157\n",
      "Epoch 150/500\n",
      "576/576 [==============================] - 0s 679us/step - loss: 14.6130 - mean_squared_error: 14.6130\n",
      "Epoch 151/500\n",
      "576/576 [==============================] - 0s 557us/step - loss: 14.5044 - mean_squared_error: 14.5044\n",
      "Epoch 152/500\n",
      "576/576 [==============================] - 0s 540us/step - loss: 14.3630 - mean_squared_error: 14.3630\n",
      "Epoch 153/500\n",
      "576/576 [==============================] - 0s 555us/step - loss: 14.2471 - mean_squared_error: 14.2471\n",
      "Epoch 154/500\n",
      "576/576 [==============================] - 0s 564us/step - loss: 14.1714 - mean_squared_error: 14.1714\n",
      "Epoch 155/500\n",
      "576/576 [==============================] - 0s 547us/step - loss: 13.9854 - mean_squared_error: 13.9854\n",
      "Epoch 156/500\n",
      "576/576 [==============================] - 0s 557us/step - loss: 13.9156 - mean_squared_error: 13.9156\n",
      "Epoch 157/500\n",
      "576/576 [==============================] - 0s 681us/step - loss: 13.7775 - mean_squared_error: 13.7775\n",
      "Epoch 158/500\n",
      "576/576 [==============================] - 0s 559us/step - loss: 13.6422 - mean_squared_error: 13.6422\n",
      "Epoch 159/500\n",
      "576/576 [==============================] - 0s 564us/step - loss: 13.3931 - mean_squared_error: 13.3931\n",
      "Epoch 160/500\n",
      "576/576 [==============================] - 0s 554us/step - loss: 13.3031 - mean_squared_error: 13.3031\n",
      "Epoch 161/500\n",
      "576/576 [==============================] - 0s 561us/step - loss: 13.0719 - mean_squared_error: 13.0719\n",
      "Epoch 162/500\n",
      "576/576 [==============================] - 0s 677us/step - loss: 12.8793 - mean_squared_error: 12.8793\n",
      "Epoch 163/500\n",
      "576/576 [==============================] - 0s 686us/step - loss: 12.7668 - mean_squared_error: 12.7668\n",
      "Epoch 164/500\n",
      "576/576 [==============================] - 0s 641us/step - loss: 12.6000 - mean_squared_error: 12.6000\n",
      "Epoch 165/500\n",
      "576/576 [==============================] - 0s 550us/step - loss: 12.4756 - mean_squared_error: 12.4756\n",
      "Epoch 166/500\n",
      "576/576 [==============================] - 0s 526us/step - loss: 12.3845 - mean_squared_error: 12.3845\n",
      "Epoch 167/500\n",
      "576/576 [==============================] - 0s 555us/step - loss: 12.2038 - mean_squared_error: 12.2038\n",
      "Epoch 168/500\n",
      "576/576 [==============================] - 0s 536us/step - loss: 12.1052 - mean_squared_error: 12.1052\n",
      "Epoch 169/500\n",
      "576/576 [==============================] - 0s 538us/step - loss: 12.0202 - mean_squared_error: 12.0202\n",
      "Epoch 170/500\n",
      "576/576 [==============================] - 0s 573us/step - loss: 11.9261 - mean_squared_error: 11.9261\n",
      "Epoch 171/500\n",
      "576/576 [==============================] - 0s 587us/step - loss: 11.8065 - mean_squared_error: 11.8065\n",
      "Epoch 172/500\n",
      "576/576 [==============================] - 0s 616us/step - loss: 11.7197 - mean_squared_error: 11.7197\n",
      "Epoch 173/500\n",
      "576/576 [==============================] - 0s 618us/step - loss: 11.6623 - mean_squared_error: 11.6623\n",
      "Epoch 174/500\n",
      "576/576 [==============================] - 0s 559us/step - loss: 11.4884 - mean_squared_error: 11.4884\n",
      "Epoch 175/500\n",
      "576/576 [==============================] - 0s 609us/step - loss: 11.4954 - mean_squared_error: 11.4954\n",
      "Epoch 176/500\n",
      "576/576 [==============================] - 0s 599us/step - loss: 11.4243 - mean_squared_error: 11.4243\n",
      "Epoch 177/500\n",
      "576/576 [==============================] - 0s 545us/step - loss: 11.3401 - mean_squared_error: 11.3401\n",
      "Epoch 178/500\n",
      "576/576 [==============================] - 0s 562us/step - loss: 11.2955 - mean_squared_error: 11.2955\n",
      "Epoch 179/500\n",
      "576/576 [==============================] - 0s 571us/step - loss: 11.2234 - mean_squared_error: 11.2234\n",
      "Epoch 180/500\n",
      "576/576 [==============================] - 0s 571us/step - loss: 11.1837 - mean_squared_error: 11.1837\n",
      "Epoch 181/500\n",
      "576/576 [==============================] - 0s 710us/step - loss: 11.1137 - mean_squared_error: 11.1137\n",
      "Epoch 182/500\n",
      "576/576 [==============================] - 0s 649us/step - loss: 11.0949 - mean_squared_error: 11.0949\n",
      "Epoch 183/500\n",
      "576/576 [==============================] - 0s 648us/step - loss: 11.0018 - mean_squared_error: 11.0018\n",
      "Epoch 184/500\n",
      "576/576 [==============================] - 0s 646us/step - loss: 11.0124 - mean_squared_error: 11.0124\n",
      "Epoch 185/500\n",
      "576/576 [==============================] - 0s 621us/step - loss: 10.9674 - mean_squared_error: 10.9674\n",
      "Epoch 186/500\n",
      "576/576 [==============================] - 0s 561us/step - loss: 10.9334 - mean_squared_error: 10.9334\n",
      "Epoch 187/500\n",
      "576/576 [==============================] - 0s 541us/step - loss: 10.9010 - mean_squared_error: 10.9010\n",
      "Epoch 188/500\n",
      "576/576 [==============================] - 0s 561us/step - loss: 10.9397 - mean_squared_error: 10.9397\n",
      "Epoch 189/500\n",
      "576/576 [==============================] - 0s 534us/step - loss: 10.8607 - mean_squared_error: 10.8607\n",
      "Epoch 190/500\n",
      "576/576 [==============================] - 0s 595us/step - loss: 10.8656 - mean_squared_error: 10.8656\n",
      "Epoch 191/500\n",
      "576/576 [==============================] - 0s 618us/step - loss: 10.8131 - mean_squared_error: 10.8131\n",
      "Epoch 192/500\n",
      "576/576 [==============================] - 0s 562us/step - loss: 10.7154 - mean_squared_error: 10.7154\n",
      "Epoch 193/500\n",
      "576/576 [==============================] - 0s 548us/step - loss: 10.7701 - mean_squared_error: 10.7701\n",
      "Epoch 194/500\n",
      "576/576 [==============================] - 0s 576us/step - loss: 10.7154 - mean_squared_error: 10.7154\n",
      "Epoch 195/500\n",
      "576/576 [==============================] - 0s 656us/step - loss: 10.7157 - mean_squared_error: 10.7157\n",
      "Epoch 196/500\n",
      "576/576 [==============================] - 0s 618us/step - loss: 10.6677 - mean_squared_error: 10.6677\n",
      "Epoch 197/500\n",
      "576/576 [==============================] - 0s 609us/step - loss: 10.6623 - mean_squared_error: 10.6623\n",
      "Epoch 198/500\n",
      "576/576 [==============================] - 0s 552us/step - loss: 10.6373 - mean_squared_error: 10.6373\n",
      "Epoch 199/500\n",
      "576/576 [==============================] - 0s 554us/step - loss: 10.6535 - mean_squared_error: 10.6535\n",
      "Epoch 200/500\n",
      "576/576 [==============================] - 0s 536us/step - loss: 10.5940 - mean_squared_error: 10.5940\n",
      "Epoch 201/500\n",
      "576/576 [==============================] - 0s 552us/step - loss: 10.5999 - mean_squared_error: 10.5999\n",
      "Epoch 202/500\n",
      "576/576 [==============================] - 0s 566us/step - loss: 10.5898 - mean_squared_error: 10.5898\n",
      "Epoch 203/500\n",
      "576/576 [==============================] - 0s 561us/step - loss: 10.5730 - mean_squared_error: 10.5730\n",
      "Epoch 204/500\n",
      "576/576 [==============================] - 0s 569us/step - loss: 10.5869 - mean_squared_error: 10.5869\n",
      "Epoch 205/500\n",
      "576/576 [==============================] - 0s 571us/step - loss: 10.5832 - mean_squared_error: 10.5832\n",
      "Epoch 206/500\n",
      "576/576 [==============================] - 0s 641us/step - loss: 10.5554 - mean_squared_error: 10.5554\n",
      "Epoch 207/500\n",
      "576/576 [==============================] - 0s 567us/step - loss: 10.5362 - mean_squared_error: 10.5362\n",
      "Epoch 208/500\n",
      "576/576 [==============================] - 0s 625us/step - loss: 10.5128 - mean_squared_error: 10.5128\n",
      "Epoch 209/500\n",
      "576/576 [==============================] - 0s 808us/step - loss: 10.5231 - mean_squared_error: 10.5231\n",
      "Epoch 210/500\n",
      "576/576 [==============================] - 0s 815us/step - loss: 10.5262 - mean_squared_error: 10.5262\n",
      "Epoch 211/500\n",
      "576/576 [==============================] - 0s 688us/step - loss: 10.4894 - mean_squared_error: 10.4894\n",
      "Epoch 212/500\n",
      "576/576 [==============================] - 0s 668us/step - loss: 10.5058 - mean_squared_error: 10.5058\n",
      "Epoch 213/500\n",
      "576/576 [==============================] - 0s 567us/step - loss: 10.4801 - mean_squared_error: 10.4801\n",
      "Epoch 214/500\n",
      "576/576 [==============================] - 0s 646us/step - loss: 10.4795 - mean_squared_error: 10.4795\n",
      "Epoch 215/500\n",
      "576/576 [==============================] - 0s 580us/step - loss: 10.5599 - mean_squared_error: 10.5599\n",
      "Epoch 216/500\n",
      "576/576 [==============================] - 0s 578us/step - loss: 10.4684 - mean_squared_error: 10.4684\n",
      "Epoch 217/500\n",
      "576/576 [==============================] - 0s 679us/step - loss: 10.4822 - mean_squared_error: 10.4822\n",
      "Epoch 218/500\n",
      "576/576 [==============================] - 0s 519us/step - loss: 10.4378 - mean_squared_error: 10.4378\n",
      "Epoch 219/500\n",
      "576/576 [==============================] - 0s 648us/step - loss: 10.4516 - mean_squared_error: 10.4516\n",
      "Epoch 220/500\n",
      "576/576 [==============================] - 0s 538us/step - loss: 10.4729 - mean_squared_error: 10.4729\n",
      "Epoch 221/500\n",
      "576/576 [==============================] - 0s 534us/step - loss: 10.4841 - mean_squared_error: 10.4841\n",
      "Epoch 222/500\n",
      "576/576 [==============================] - 0s 618us/step - loss: 10.4626 - mean_squared_error: 10.4626\n",
      "Epoch 223/500\n",
      "576/576 [==============================] - 0s 520us/step - loss: 10.4541 - mean_squared_error: 10.4541\n",
      "Epoch 224/500\n",
      "576/576 [==============================] - 0s 590us/step - loss: 10.4335 - mean_squared_error: 10.4335\n",
      "Epoch 225/500\n",
      "576/576 [==============================] - 0s 661us/step - loss: 10.3829 - mean_squared_error: 10.3829\n",
      "Epoch 226/500\n",
      "576/576 [==============================] - 0s 526us/step - loss: 10.4252 - mean_squared_error: 10.4252\n",
      "Epoch 227/500\n",
      "576/576 [==============================] - 0s 574us/step - loss: 10.4282 - mean_squared_error: 10.4282\n",
      "Epoch 228/500\n",
      "576/576 [==============================] - 0s 667us/step - loss: 10.4561 - mean_squared_error: 10.4561\n",
      "Epoch 229/500\n",
      "576/576 [==============================] - 0s 540us/step - loss: 10.4653 - mean_squared_error: 10.4653\n",
      "Epoch 230/500\n",
      "576/576 [==============================] - 0s 547us/step - loss: 10.4141 - mean_squared_error: 10.4141\n",
      "Epoch 231/500\n",
      "576/576 [==============================] - 0s 627us/step - loss: 10.3808 - mean_squared_error: 10.3808\n",
      "Epoch 232/500\n",
      "576/576 [==============================] - 0s 567us/step - loss: 10.3358 - mean_squared_error: 10.3358\n",
      "Epoch 233/500\n",
      "576/576 [==============================] - 0s 550us/step - loss: 10.4248 - mean_squared_error: 10.4248\n",
      "Epoch 234/500\n",
      "576/576 [==============================] - 0s 550us/step - loss: 10.3865 - mean_squared_error: 10.3865\n",
      "Epoch 235/500\n",
      "576/576 [==============================] - 0s 541us/step - loss: 10.4453 - mean_squared_error: 10.4453\n",
      "Epoch 236/500\n",
      "576/576 [==============================] - 0s 536us/step - loss: 10.3955 - mean_squared_error: 10.3955\n",
      "Epoch 237/500\n",
      "576/576 [==============================] - 0s 552us/step - loss: 10.3690 - mean_squared_error: 10.3690\n",
      "Epoch 238/500\n",
      "576/576 [==============================] - 0s 529us/step - loss: 10.3944 - mean_squared_error: 10.3944\n",
      "Epoch 239/500\n",
      "576/576 [==============================] - 0s 548us/step - loss: 10.3974 - mean_squared_error: 10.3974\n",
      "Epoch 240/500\n",
      "576/576 [==============================] - 0s 559us/step - loss: 10.3331 - mean_squared_error: 10.3331\n",
      "Epoch 241/500\n",
      "576/576 [==============================] - 0s 548us/step - loss: 10.3146 - mean_squared_error: 10.3146\n",
      "Epoch 242/500\n",
      "576/576 [==============================] - 0s 536us/step - loss: 10.4131 - mean_squared_error: 10.4131\n",
      "Epoch 243/500\n",
      "576/576 [==============================] - 0s 552us/step - loss: 10.3598 - mean_squared_error: 10.3598\n",
      "Epoch 244/500\n",
      "576/576 [==============================] - 0s 524us/step - loss: 10.3659 - mean_squared_error: 10.3659\n",
      "Epoch 245/500\n",
      "576/576 [==============================] - 0s 550us/step - loss: 10.3439 - mean_squared_error: 10.3439\n",
      "Epoch 246/500\n",
      "576/576 [==============================] - 0s 533us/step - loss: 10.3670 - mean_squared_error: 10.3670\n",
      "Epoch 247/500\n",
      "576/576 [==============================] - 0s 562us/step - loss: 10.3490 - mean_squared_error: 10.3490\n",
      "Epoch 248/500\n",
      "576/576 [==============================] - 0s 552us/step - loss: 10.3941 - mean_squared_error: 10.3941\n",
      "Epoch 249/500\n",
      "576/576 [==============================] - 0s 543us/step - loss: 10.3624 - mean_squared_error: 10.3624\n",
      "Epoch 250/500\n",
      "576/576 [==============================] - 0s 541us/step - loss: 10.3354 - mean_squared_error: 10.3354\n",
      "Epoch 251/500\n",
      "576/576 [==============================] - 0s 540us/step - loss: 10.3594 - mean_squared_error: 10.3594\n",
      "Epoch 252/500\n",
      "576/576 [==============================] - 0s 559us/step - loss: 10.2566 - mean_squared_error: 10.2566\n",
      "Epoch 253/500\n",
      "576/576 [==============================] - 0s 526us/step - loss: 10.3139 - mean_squared_error: 10.3139\n",
      "Epoch 254/500\n",
      "576/576 [==============================] - 0s 545us/step - loss: 10.3577 - mean_squared_error: 10.3577\n",
      "Epoch 255/500\n",
      "576/576 [==============================] - 0s 540us/step - loss: 10.3275 - mean_squared_error: 10.3275\n",
      "Epoch 256/500\n",
      "576/576 [==============================] - 0s 529us/step - loss: 10.3266 - mean_squared_error: 10.3266\n",
      "Epoch 257/500\n",
      "576/576 [==============================] - 0s 667us/step - loss: 10.3155 - mean_squared_error: 10.3155\n",
      "Epoch 258/500\n",
      "576/576 [==============================] - 0s 668us/step - loss: 10.3308 - mean_squared_error: 10.3308\n",
      "Epoch 259/500\n",
      "576/576 [==============================] - 0s 649us/step - loss: 10.3484 - mean_squared_error: 10.3484\n",
      "Epoch 260/500\n",
      "576/576 [==============================] - 0s 534us/step - loss: 10.2924 - mean_squared_error: 10.2924\n",
      "Epoch 261/500\n",
      "576/576 [==============================] - 0s 564us/step - loss: 10.3296 - mean_squared_error: 10.3296\n",
      "Epoch 262/500\n",
      "576/576 [==============================] - 0s 540us/step - loss: 10.2880 - mean_squared_error: 10.2880\n",
      "Epoch 263/500\n",
      "576/576 [==============================] - 0s 531us/step - loss: 10.3078 - mean_squared_error: 10.3078\n",
      "Epoch 264/500\n",
      "576/576 [==============================] - 0s 552us/step - loss: 10.3209 - mean_squared_error: 10.3209\n",
      "Epoch 265/500\n",
      "576/576 [==============================] - 0s 561us/step - loss: 10.2948 - mean_squared_error: 10.2948\n",
      "Epoch 266/500\n",
      "576/576 [==============================] - 0s 541us/step - loss: 10.3677 - mean_squared_error: 10.3677\n",
      "Epoch 267/500\n",
      "576/576 [==============================] - 0s 534us/step - loss: 10.3124 - mean_squared_error: 10.3124\n",
      "Epoch 268/500\n",
      "576/576 [==============================] - 0s 569us/step - loss: 10.3064 - mean_squared_error: 10.3064\n",
      "Epoch 269/500\n",
      "576/576 [==============================] - 0s 564us/step - loss: 10.3219 - mean_squared_error: 10.3219\n",
      "Epoch 270/500\n",
      "576/576 [==============================] - 0s 548us/step - loss: 10.2906 - mean_squared_error: 10.2906\n",
      "Epoch 271/500\n",
      "576/576 [==============================] - 0s 536us/step - loss: 10.2739 - mean_squared_error: 10.2739\n",
      "Epoch 272/500\n",
      "576/576 [==============================] - 0s 533us/step - loss: 10.4058 - mean_squared_error: 10.4058\n",
      "Epoch 273/500\n",
      "576/576 [==============================] - 0s 540us/step - loss: 10.2876 - mean_squared_error: 10.2876\n",
      "Epoch 274/500\n",
      "576/576 [==============================] - 0s 554us/step - loss: 10.2697 - mean_squared_error: 10.2697\n",
      "Epoch 275/500\n",
      "576/576 [==============================] - 0s 559us/step - loss: 10.3409 - mean_squared_error: 10.3409\n",
      "Epoch 276/500\n",
      "576/576 [==============================] - 0s 543us/step - loss: 10.3183 - mean_squared_error: 10.3183\n",
      "Epoch 277/500\n",
      "576/576 [==============================] - 0s 550us/step - loss: 10.3304 - mean_squared_error: 10.3304\n",
      "Epoch 278/500\n",
      "576/576 [==============================] - 0s 566us/step - loss: 10.3863 - mean_squared_error: 10.3863\n",
      "Epoch 279/500\n",
      "576/576 [==============================] - 0s 576us/step - loss: 10.3607 - mean_squared_error: 10.3607\n",
      "Epoch 280/500\n",
      "576/576 [==============================] - 0s 515us/step - loss: 10.2916 - mean_squared_error: 10.2916\n",
      "Epoch 281/500\n",
      "576/576 [==============================] - 0s 524us/step - loss: 10.3269 - mean_squared_error: 10.3269\n",
      "Epoch 282/500\n",
      "576/576 [==============================] - 0s 515us/step - loss: 10.2759 - mean_squared_error: 10.2759\n",
      "Epoch 283/500\n",
      "576/576 [==============================] - 0s 554us/step - loss: 10.2824 - mean_squared_error: 10.2824\n",
      "Epoch 284/500\n",
      "576/576 [==============================] - 0s 531us/step - loss: 10.2987 - mean_squared_error: 10.2987\n",
      "Epoch 285/500\n",
      "576/576 [==============================] - 0s 547us/step - loss: 10.3114 - mean_squared_error: 10.3114\n",
      "Epoch 286/500\n",
      "576/576 [==============================] - 0s 545us/step - loss: 10.2811 - mean_squared_error: 10.2811\n",
      "Epoch 287/500\n",
      "576/576 [==============================] - 0s 524us/step - loss: 10.3261 - mean_squared_error: 10.3261\n",
      "Epoch 288/500\n",
      "576/576 [==============================] - 0s 524us/step - loss: 10.3176 - mean_squared_error: 10.3176 - loss: 9.6739 - mean_squared_error\n",
      "Epoch 289/500\n",
      "576/576 [==============================] - 0s 559us/step - loss: 10.2947 - mean_squared_error: 10.2947\n",
      "Epoch 290/500\n",
      "576/576 [==============================] - 0s 531us/step - loss: 10.2775 - mean_squared_error: 10.2775\n",
      "Epoch 291/500\n",
      "576/576 [==============================] - 0s 522us/step - loss: 10.2569 - mean_squared_error: 10.2569\n",
      "Epoch 292/500\n",
      "576/576 [==============================] - 0s 559us/step - loss: 10.3224 - mean_squared_error: 10.3224\n",
      "Epoch 293/500\n",
      "576/576 [==============================] - 0s 522us/step - loss: 10.2634 - mean_squared_error: 10.2634\n",
      "Epoch 294/500\n",
      "576/576 [==============================] - 0s 524us/step - loss: 10.2926 - mean_squared_error: 10.2926\n",
      "Epoch 295/500\n",
      "576/576 [==============================] - 0s 552us/step - loss: 10.1858 - mean_squared_error: 10.1858\n",
      "Epoch 296/500\n",
      "576/576 [==============================] - 0s 520us/step - loss: 10.2995 - mean_squared_error: 10.2995\n",
      "Epoch 297/500\n",
      "576/576 [==============================] - 0s 536us/step - loss: 10.3232 - mean_squared_error: 10.3232\n",
      "Epoch 298/500\n",
      "576/576 [==============================] - 0s 529us/step - loss: 10.3112 - mean_squared_error: 10.3112\n",
      "Epoch 299/500\n",
      "576/576 [==============================] - 0s 547us/step - loss: 10.2663 - mean_squared_error: 10.2663\n",
      "Epoch 300/500\n",
      "576/576 [==============================] - 0s 524us/step - loss: 10.2859 - mean_squared_error: 10.2859\n",
      "Epoch 301/500\n",
      "576/576 [==============================] - 0s 550us/step - loss: 10.2647 - mean_squared_error: 10.2647\n",
      "Epoch 302/500\n",
      "576/576 [==============================] - 0s 548us/step - loss: 10.3150 - mean_squared_error: 10.3150\n",
      "Epoch 303/500\n",
      "576/576 [==============================] - 0s 526us/step - loss: 10.3647 - mean_squared_error: 10.3647\n",
      "Epoch 304/500\n",
      "576/576 [==============================] - 0s 557us/step - loss: 10.2688 - mean_squared_error: 10.2688\n",
      "Epoch 305/500\n",
      "576/576 [==============================] - 0s 527us/step - loss: 10.2618 - mean_squared_error: 10.2618\n",
      "Epoch 306/500\n",
      "576/576 [==============================] - 0s 552us/step - loss: 10.2877 - mean_squared_error: 10.2877\n",
      "Epoch 307/500\n",
      "576/576 [==============================] - 0s 587us/step - loss: 10.3011 - mean_squared_error: 10.3011\n",
      "Epoch 308/500\n",
      "576/576 [==============================] - 0s 679us/step - loss: 10.2553 - mean_squared_error: 10.2553\n",
      "Epoch 309/500\n",
      "576/576 [==============================] - 0s 700us/step - loss: 10.2406 - mean_squared_error: 10.2406\n",
      "Epoch 310/500\n",
      "576/576 [==============================] - 0s 564us/step - loss: 10.3390 - mean_squared_error: 10.3390\n",
      "Epoch 311/500\n",
      "576/576 [==============================] - 0s 552us/step - loss: 10.2781 - mean_squared_error: 10.2781\n",
      "Epoch 312/500\n",
      "576/576 [==============================] - 0s 531us/step - loss: 10.2565 - mean_squared_error: 10.2565\n",
      "Epoch 313/500\n",
      "576/576 [==============================] - 0s 541us/step - loss: 10.2538 - mean_squared_error: 10.2538\n",
      "Epoch 314/500\n",
      "576/576 [==============================] - 0s 550us/step - loss: 10.3128 - mean_squared_error: 10.3128\n",
      "Epoch 315/500\n",
      "576/576 [==============================] - 0s 526us/step - loss: 10.3038 - mean_squared_error: 10.3038\n",
      "Epoch 316/500\n",
      "576/576 [==============================] - 0s 547us/step - loss: 10.2600 - mean_squared_error: 10.2600\n",
      "Epoch 317/500\n",
      "576/576 [==============================] - 0s 557us/step - loss: 10.2627 - mean_squared_error: 10.2627\n",
      "Epoch 318/500\n",
      "576/576 [==============================] - 0s 543us/step - loss: 10.2694 - mean_squared_error: 10.2694\n",
      "Epoch 319/500\n",
      "576/576 [==============================] - 0s 531us/step - loss: 10.2807 - mean_squared_error: 10.2807\n",
      "Epoch 320/500\n",
      "576/576 [==============================] - 0s 566us/step - loss: 10.2933 - mean_squared_error: 10.2933\n",
      "Epoch 321/500\n",
      "576/576 [==============================] - 0s 540us/step - loss: 10.2773 - mean_squared_error: 10.2773\n",
      "Epoch 322/500\n",
      "576/576 [==============================] - 0s 534us/step - loss: 10.2955 - mean_squared_error: 10.2955\n",
      "Epoch 323/500\n",
      "576/576 [==============================] - 0s 534us/step - loss: 10.2549 - mean_squared_error: 10.2549\n",
      "Epoch 324/500\n",
      "576/576 [==============================] - 0s 567us/step - loss: 10.2996 - mean_squared_error: 10.2996\n",
      "Epoch 325/500\n",
      "576/576 [==============================] - 0s 543us/step - loss: 10.2630 - mean_squared_error: 10.2630\n",
      "Epoch 326/500\n",
      "576/576 [==============================] - 0s 554us/step - loss: 10.1636 - mean_squared_error: 10.1636\n",
      "Epoch 327/500\n",
      "576/576 [==============================] - 0s 543us/step - loss: 10.3924 - mean_squared_error: 10.3924\n",
      "Epoch 328/500\n",
      "576/576 [==============================] - 0s 534us/step - loss: 10.2920 - mean_squared_error: 10.2920\n",
      "Epoch 329/500\n",
      "576/576 [==============================] - 0s 540us/step - loss: 10.2411 - mean_squared_error: 10.2411\n",
      "Epoch 330/500\n",
      "576/576 [==============================] - 0s 554us/step - loss: 10.2659 - mean_squared_error: 10.2659\n",
      "Epoch 331/500\n",
      "576/576 [==============================] - 0s 566us/step - loss: 10.2967 - mean_squared_error: 10.2967\n",
      "Epoch 332/500\n",
      "576/576 [==============================] - 0s 561us/step - loss: 10.2954 - mean_squared_error: 10.2954\n",
      "Epoch 333/500\n",
      "576/576 [==============================] - 0s 555us/step - loss: 10.2663 - mean_squared_error: 10.2663\n",
      "Epoch 334/500\n",
      "576/576 [==============================] - 0s 545us/step - loss: 10.2789 - mean_squared_error: 10.2789\n",
      "Epoch 335/500\n",
      "576/576 [==============================] - 0s 545us/step - loss: 10.2695 - mean_squared_error: 10.2695\n",
      "Epoch 336/500\n",
      "576/576 [==============================] - 0s 557us/step - loss: 10.2582 - mean_squared_error: 10.2582\n",
      "Epoch 337/500\n",
      "576/576 [==============================] - 0s 555us/step - loss: 10.3107 - mean_squared_error: 10.3107\n",
      "Epoch 338/500\n",
      "576/576 [==============================] - 0s 534us/step - loss: 10.2705 - mean_squared_error: 10.2705\n",
      "Epoch 339/500\n",
      "576/576 [==============================] - 0s 559us/step - loss: 10.2286 - mean_squared_error: 10.2286\n",
      "Epoch 340/500\n",
      "576/576 [==============================] - ETA: 0s - loss: 10.0508 - mean_squared_error: 10.05 - 0s 527us/step - loss: 10.3062 - mean_squared_error: 10.3062\n",
      "Epoch 341/500\n",
      "576/576 [==============================] - 0s 569us/step - loss: 10.3160 - mean_squared_error: 10.3160\n",
      "Epoch 342/500\n",
      "576/576 [==============================] - 0s 564us/step - loss: 10.2244 - mean_squared_error: 10.2244\n",
      "Epoch 343/500\n",
      "576/576 [==============================] - 0s 534us/step - loss: 10.1992 - mean_squared_error: 10.1992\n",
      "Epoch 344/500\n",
      "576/576 [==============================] - 0s 554us/step - loss: 10.3015 - mean_squared_error: 10.3015\n",
      "Epoch 345/500\n",
      "576/576 [==============================] - 0s 555us/step - loss: 10.2670 - mean_squared_error: 10.2670\n",
      "Epoch 346/500\n",
      "576/576 [==============================] - 0s 541us/step - loss: 10.2639 - mean_squared_error: 10.2639\n",
      "Epoch 347/500\n",
      "576/576 [==============================] - 0s 534us/step - loss: 10.2236 - mean_squared_error: 10.2236\n",
      "Epoch 348/500\n",
      "576/576 [==============================] - 0s 548us/step - loss: 10.2958 - mean_squared_error: 10.2958\n",
      "Epoch 349/500\n",
      "576/576 [==============================] - 0s 536us/step - loss: 10.2455 - mean_squared_error: 10.2455\n",
      "Epoch 350/500\n",
      "576/576 [==============================] - 0s 529us/step - loss: 10.2795 - mean_squared_error: 10.2795\n",
      "Epoch 351/500\n",
      "576/576 [==============================] - 0s 514us/step - loss: 10.2364 - mean_squared_error: 10.2364\n",
      "Epoch 352/500\n",
      "576/576 [==============================] - 0s 527us/step - loss: 10.2184 - mean_squared_error: 10.2184\n",
      "Epoch 353/500\n",
      "576/576 [==============================] - 0s 510us/step - loss: 10.2911 - mean_squared_error: 10.2911\n",
      "Epoch 354/500\n",
      "576/576 [==============================] - 0s 547us/step - loss: 10.3415 - mean_squared_error: 10.3415\n",
      "Epoch 355/500\n",
      "576/576 [==============================] - 0s 514us/step - loss: 10.2926 - mean_squared_error: 10.2926\n",
      "Epoch 356/500\n",
      "576/576 [==============================] - 0s 522us/step - loss: 10.2651 - mean_squared_error: 10.2651\n",
      "Epoch 357/500\n",
      "576/576 [==============================] - 0s 576us/step - loss: 10.2721 - mean_squared_error: 10.2721\n",
      "Epoch 358/500\n",
      "576/576 [==============================] - 0s 675us/step - loss: 10.2559 - mean_squared_error: 10.2559\n",
      "Epoch 359/500\n",
      "576/576 [==============================] - 0s 686us/step - loss: 10.2841 - mean_squared_error: 10.2841\n",
      "Epoch 360/500\n",
      "576/576 [==============================] - 0s 588us/step - loss: 10.2588 - mean_squared_error: 10.2588\n",
      "Epoch 361/500\n",
      "576/576 [==============================] - 0s 526us/step - loss: 10.3738 - mean_squared_error: 10.3738\n",
      "Epoch 362/500\n",
      "576/576 [==============================] - 0s 557us/step - loss: 10.2561 - mean_squared_error: 10.2561\n",
      "Epoch 363/500\n",
      "576/576 [==============================] - 0s 543us/step - loss: 10.2986 - mean_squared_error: 10.2986\n",
      "Epoch 364/500\n",
      "576/576 [==============================] - 0s 524us/step - loss: 10.2395 - mean_squared_error: 10.2395\n",
      "Epoch 365/500\n",
      "576/576 [==============================] - 0s 548us/step - loss: 10.3510 - mean_squared_error: 10.3510\n",
      "Epoch 366/500\n",
      "576/576 [==============================] - 0s 533us/step - loss: 10.2460 - mean_squared_error: 10.2460\n",
      "Epoch 367/500\n",
      "576/576 [==============================] - 0s 566us/step - loss: 10.2518 - mean_squared_error: 10.2518\n",
      "Epoch 368/500\n",
      "576/576 [==============================] - ETA: 0s - loss: 10.7850 - mean_squared_error: 10.78 - 0s 527us/step - loss: 10.2939 - mean_squared_error: 10.2939\n",
      "Epoch 369/500\n",
      "576/576 [==============================] - 0s 519us/step - loss: 10.3122 - mean_squared_error: 10.3122\n",
      "Epoch 370/500\n",
      "576/576 [==============================] - 0s 517us/step - loss: 10.2578 - mean_squared_error: 10.2578\n",
      "Epoch 371/500\n",
      "576/576 [==============================] - 0s 564us/step - loss: 10.2847 - mean_squared_error: 10.2847\n",
      "Epoch 372/500\n",
      "576/576 [==============================] - 0s 517us/step - loss: 10.2511 - mean_squared_error: 10.2511\n",
      "Epoch 373/500\n",
      "576/576 [==============================] - 0s 548us/step - loss: 10.2714 - mean_squared_error: 10.2714\n",
      "Epoch 374/500\n",
      "576/576 [==============================] - 0s 524us/step - loss: 10.3475 - mean_squared_error: 10.3475\n",
      "Epoch 375/500\n",
      "576/576 [==============================] - 0s 531us/step - loss: 10.2607 - mean_squared_error: 10.2607\n",
      "Epoch 376/500\n",
      "576/576 [==============================] - 0s 561us/step - loss: 10.2717 - mean_squared_error: 10.2717\n",
      "Epoch 377/500\n",
      "576/576 [==============================] - 0s 534us/step - loss: 10.2358 - mean_squared_error: 10.2358\n",
      "Epoch 378/500\n",
      "576/576 [==============================] - 0s 531us/step - loss: 10.2637 - mean_squared_error: 10.2637\n",
      "Epoch 379/500\n",
      "576/576 [==============================] - 0s 534us/step - loss: 10.2103 - mean_squared_error: 10.2103\n",
      "Epoch 380/500\n",
      "576/576 [==============================] - 0s 545us/step - loss: 10.2515 - mean_squared_error: 10.2515\n",
      "Epoch 381/500\n",
      "576/576 [==============================] - 0s 536us/step - loss: 10.2568 - mean_squared_error: 10.2568\n",
      "Epoch 382/500\n",
      "576/576 [==============================] - 0s 555us/step - loss: 10.2910 - mean_squared_error: 10.2910\n",
      "Epoch 383/500\n",
      "576/576 [==============================] - 0s 554us/step - loss: 10.2854 - mean_squared_error: 10.2854\n",
      "Epoch 384/500\n",
      "576/576 [==============================] - 0s 543us/step - loss: 10.3159 - mean_squared_error: 10.3159\n",
      "Epoch 385/500\n",
      "576/576 [==============================] - 0s 550us/step - loss: 10.2494 - mean_squared_error: 10.2494\n",
      "Epoch 386/500\n",
      "576/576 [==============================] - 0s 536us/step - loss: 10.3165 - mean_squared_error: 10.3165\n",
      "Epoch 387/500\n",
      "576/576 [==============================] - 0s 552us/step - loss: 10.2753 - mean_squared_error: 10.2753\n",
      "Epoch 388/500\n",
      "576/576 [==============================] - 0s 534us/step - loss: 10.2574 - mean_squared_error: 10.2574\n",
      "Epoch 389/500\n",
      "576/576 [==============================] - 0s 540us/step - loss: 10.3390 - mean_squared_error: 10.3390\n",
      "Epoch 390/500\n",
      "576/576 [==============================] - 0s 534us/step - loss: 10.2861 - mean_squared_error: 10.2861\n",
      "Epoch 391/500\n",
      "576/576 [==============================] - 0s 548us/step - loss: 10.4346 - mean_squared_error: 10.4346\n",
      "Epoch 392/500\n",
      "576/576 [==============================] - 0s 547us/step - loss: 10.2232 - mean_squared_error: 10.2232\n",
      "Epoch 393/500\n",
      "576/576 [==============================] - 0s 533us/step - loss: 10.2821 - mean_squared_error: 10.2821\n",
      "Epoch 394/500\n",
      "576/576 [==============================] - 0s 538us/step - loss: 10.2644 - mean_squared_error: 10.2644\n",
      "Epoch 395/500\n",
      "576/576 [==============================] - 0s 541us/step - loss: 10.2720 - mean_squared_error: 10.2720\n",
      "Epoch 396/500\n",
      "576/576 [==============================] - 0s 548us/step - loss: 10.2704 - mean_squared_error: 10.2704\n",
      "Epoch 397/500\n",
      "576/576 [==============================] - 0s 529us/step - loss: 10.2799 - mean_squared_error: 10.2799\n",
      "Epoch 398/500\n",
      "576/576 [==============================] - 0s 533us/step - loss: 10.2324 - mean_squared_error: 10.2324\n",
      "Epoch 399/500\n",
      "576/576 [==============================] - 0s 545us/step - loss: 10.2838 - mean_squared_error: 10.2838\n",
      "Epoch 400/500\n",
      "576/576 [==============================] - 0s 555us/step - loss: 10.2872 - mean_squared_error: 10.2872\n",
      "Epoch 401/500\n",
      "576/576 [==============================] - 0s 552us/step - loss: 10.1939 - mean_squared_error: 10.1939\n",
      "Epoch 402/500\n",
      "576/576 [==============================] - 0s 552us/step - loss: 10.2815 - mean_squared_error: 10.2815\n",
      "Epoch 403/500\n",
      "576/576 [==============================] - 0s 552us/step - loss: 10.2868 - mean_squared_error: 10.2868\n",
      "Epoch 404/500\n",
      "576/576 [==============================] - 0s 550us/step - loss: 10.2643 - mean_squared_error: 10.2643\n",
      "Epoch 405/500\n",
      "576/576 [==============================] - 0s 531us/step - loss: 10.2520 - mean_squared_error: 10.2520\n",
      "Epoch 406/500\n",
      "576/576 [==============================] - 0s 552us/step - loss: 10.2352 - mean_squared_error: 10.2352\n",
      "Epoch 407/500\n",
      "576/576 [==============================] - 0s 564us/step - loss: 10.2550 - mean_squared_error: 10.2550\n",
      "Epoch 408/500\n",
      "576/576 [==============================] - 0s 675us/step - loss: 10.2669 - mean_squared_error: 10.2669\n",
      "Epoch 409/500\n",
      "576/576 [==============================] - 0s 682us/step - loss: 10.2623 - mean_squared_error: 10.2623\n",
      "Epoch 410/500\n",
      "576/576 [==============================] - 0s 668us/step - loss: 10.2590 - mean_squared_error: 10.2590\n",
      "Epoch 411/500\n",
      "576/576 [==============================] - 0s 552us/step - loss: 10.2662 - mean_squared_error: 10.2662\n",
      "Epoch 412/500\n",
      "576/576 [==============================] - 0s 541us/step - loss: 10.2650 - mean_squared_error: 10.2650\n",
      "Epoch 413/500\n",
      "576/576 [==============================] - 0s 550us/step - loss: 10.2499 - mean_squared_error: 10.2499\n",
      "Epoch 414/500\n",
      "576/576 [==============================] - 0s 540us/step - loss: 10.2291 - mean_squared_error: 10.2291\n",
      "Epoch 415/500\n",
      "576/576 [==============================] - 0s 543us/step - loss: 10.2738 - mean_squared_error: 10.2738\n",
      "Epoch 416/500\n",
      "576/576 [==============================] - 0s 545us/step - loss: 10.3102 - mean_squared_error: 10.3102\n",
      "Epoch 417/500\n",
      "576/576 [==============================] - 0s 548us/step - loss: 10.2458 - mean_squared_error: 10.2458\n",
      "Epoch 418/500\n",
      "576/576 [==============================] - 0s 522us/step - loss: 10.2378 - mean_squared_error: 10.2378\n",
      "Epoch 419/500\n",
      "576/576 [==============================] - 0s 557us/step - loss: 10.2288 - mean_squared_error: 10.2288\n",
      "Epoch 420/500\n",
      "576/576 [==============================] - 0s 527us/step - loss: 10.2205 - mean_squared_error: 10.2205\n",
      "Epoch 421/500\n",
      "576/576 [==============================] - 0s 507us/step - loss: 10.2459 - mean_squared_error: 10.2459\n",
      "Epoch 422/500\n",
      "576/576 [==============================] - 0s 522us/step - loss: 10.2769 - mean_squared_error: 10.2769\n",
      "Epoch 423/500\n",
      "576/576 [==============================] - 0s 548us/step - loss: 10.2479 - mean_squared_error: 10.2479\n",
      "Epoch 424/500\n",
      "576/576 [==============================] - 0s 533us/step - loss: 10.2922 - mean_squared_error: 10.2922\n",
      "Epoch 425/500\n",
      "576/576 [==============================] - 0s 534us/step - loss: 10.2774 - mean_squared_error: 10.2774\n",
      "Epoch 426/500\n",
      "576/576 [==============================] - 0s 526us/step - loss: 10.2549 - mean_squared_error: 10.2549\n",
      "Epoch 427/500\n",
      "576/576 [==============================] - 0s 545us/step - loss: 10.2505 - mean_squared_error: 10.2505\n",
      "Epoch 428/500\n",
      "576/576 [==============================] - 0s 514us/step - loss: 10.2483 - mean_squared_error: 10.2483\n",
      "Epoch 429/500\n",
      "576/576 [==============================] - 0s 526us/step - loss: 10.1979 - mean_squared_error: 10.1979\n",
      "Epoch 430/500\n",
      "576/576 [==============================] - 0s 533us/step - loss: 10.2497 - mean_squared_error: 10.2497\n",
      "Epoch 431/500\n",
      "576/576 [==============================] - 0s 538us/step - loss: 10.2524 - mean_squared_error: 10.2524\n",
      "Epoch 432/500\n",
      "576/576 [==============================] - 0s 541us/step - loss: 10.2494 - mean_squared_error: 10.2494\n",
      "Epoch 433/500\n",
      "576/576 [==============================] - 0s 555us/step - loss: 10.2725 - mean_squared_error: 10.2725\n",
      "Epoch 434/500\n",
      "576/576 [==============================] - 0s 534us/step - loss: 10.2496 - mean_squared_error: 10.2496\n",
      "Epoch 435/500\n",
      "576/576 [==============================] - 0s 531us/step - loss: 10.3126 - mean_squared_error: 10.3126\n",
      "Epoch 436/500\n",
      "576/576 [==============================] - 0s 561us/step - loss: 10.2218 - mean_squared_error: 10.2218\n",
      "Epoch 437/500\n",
      "576/576 [==============================] - 0s 527us/step - loss: 10.2804 - mean_squared_error: 10.2804\n",
      "Epoch 438/500\n",
      "576/576 [==============================] - 0s 548us/step - loss: 10.2647 - mean_squared_error: 10.2647\n",
      "Epoch 439/500\n",
      "576/576 [==============================] - 0s 536us/step - loss: 10.2261 - mean_squared_error: 10.2261\n",
      "Epoch 440/500\n",
      "576/576 [==============================] - 0s 527us/step - loss: 10.2273 - mean_squared_error: 10.2273\n",
      "Epoch 441/500\n",
      "576/576 [==============================] - 0s 543us/step - loss: 10.2668 - mean_squared_error: 10.2668\n",
      "Epoch 442/500\n",
      "576/576 [==============================] - 0s 569us/step - loss: 10.2494 - mean_squared_error: 10.2494\n",
      "Epoch 443/500\n",
      "576/576 [==============================] - 0s 567us/step - loss: 10.2482 - mean_squared_error: 10.2482\n",
      "Epoch 444/500\n",
      "576/576 [==============================] - 0s 547us/step - loss: 10.2577 - mean_squared_error: 10.2577\n",
      "Epoch 445/500\n",
      "576/576 [==============================] - 0s 533us/step - loss: 10.2513 - mean_squared_error: 10.2513\n",
      "Epoch 446/500\n",
      "576/576 [==============================] - 0s 566us/step - loss: 10.2383 - mean_squared_error: 10.2383\n",
      "Epoch 447/500\n",
      "576/576 [==============================] - 0s 522us/step - loss: 10.2826 - mean_squared_error: 10.2826\n",
      "Epoch 448/500\n",
      "576/576 [==============================] - 0s 529us/step - loss: 10.2241 - mean_squared_error: 10.2241\n",
      "Epoch 449/500\n",
      "576/576 [==============================] - 0s 557us/step - loss: 10.2271 - mean_squared_error: 10.2271\n",
      "Epoch 450/500\n",
      "576/576 [==============================] - 0s 550us/step - loss: 10.3336 - mean_squared_error: 10.3336\n",
      "Epoch 451/500\n",
      "576/576 [==============================] - 0s 536us/step - loss: 10.2910 - mean_squared_error: 10.2910\n",
      "Epoch 452/500\n",
      "576/576 [==============================] - 0s 559us/step - loss: 10.2773 - mean_squared_error: 10.2773\n",
      "Epoch 453/500\n",
      "576/576 [==============================] - 0s 547us/step - loss: 10.2032 - mean_squared_error: 10.2032\n",
      "Epoch 454/500\n",
      "576/576 [==============================] - 0s 519us/step - loss: 10.2560 - mean_squared_error: 10.2560\n",
      "Epoch 455/500\n",
      "576/576 [==============================] - 0s 550us/step - loss: 10.2539 - mean_squared_error: 10.2539\n",
      "Epoch 456/500\n",
      "576/576 [==============================] - 0s 533us/step - loss: 10.2308 - mean_squared_error: 10.2308\n",
      "Epoch 457/500\n",
      "576/576 [==============================] - 0s 571us/step - loss: 10.2494 - mean_squared_error: 10.2494\n",
      "Epoch 458/500\n",
      "576/576 [==============================] - 0s 581us/step - loss: 10.3090 - mean_squared_error: 10.3090\n",
      "Epoch 459/500\n",
      "576/576 [==============================] - 0s 667us/step - loss: 10.2194 - mean_squared_error: 10.2194\n",
      "Epoch 460/500\n",
      "576/576 [==============================] - 0s 663us/step - loss: 10.2841 - mean_squared_error: 10.2841\n",
      "Epoch 461/500\n",
      "576/576 [==============================] - 0s 604us/step - loss: 10.2443 - mean_squared_error: 10.2443\n",
      "Epoch 462/500\n",
      "576/576 [==============================] - 0s 559us/step - loss: 10.2133 - mean_squared_error: 10.2133\n",
      "Epoch 463/500\n",
      "576/576 [==============================] - 0s 541us/step - loss: 10.2665 - mean_squared_error: 10.2665\n",
      "Epoch 464/500\n",
      "576/576 [==============================] - 0s 536us/step - loss: 10.2604 - mean_squared_error: 10.2604\n",
      "Epoch 465/500\n",
      "576/576 [==============================] - 0s 559us/step - loss: 10.2480 - mean_squared_error: 10.2480\n",
      "Epoch 466/500\n",
      "576/576 [==============================] - 0s 559us/step - loss: 10.2436 - mean_squared_error: 10.2436\n",
      "Epoch 467/500\n",
      "576/576 [==============================] - 0s 561us/step - loss: 10.2832 - mean_squared_error: 10.2832\n",
      "Epoch 468/500\n",
      "576/576 [==============================] - 0s 561us/step - loss: 10.2928 - mean_squared_error: 10.2928\n",
      "Epoch 469/500\n",
      "576/576 [==============================] - 0s 545us/step - loss: 10.2560 - mean_squared_error: 10.2560\n",
      "Epoch 470/500\n",
      "576/576 [==============================] - 0s 578us/step - loss: 10.2620 - mean_squared_error: 10.2620\n",
      "Epoch 471/500\n",
      "576/576 [==============================] - 0s 548us/step - loss: 10.2948 - mean_squared_error: 10.2948\n",
      "Epoch 472/500\n",
      "576/576 [==============================] - 0s 554us/step - loss: 10.2756 - mean_squared_error: 10.2756\n",
      "Epoch 473/500\n",
      "576/576 [==============================] - 0s 564us/step - loss: 10.3070 - mean_squared_error: 10.3070\n",
      "Epoch 474/500\n",
      "576/576 [==============================] - 0s 562us/step - loss: 10.2617 - mean_squared_error: 10.2617\n",
      "Epoch 475/500\n",
      "576/576 [==============================] - 0s 557us/step - loss: 10.2423 - mean_squared_error: 10.2423\n",
      "Epoch 476/500\n",
      "576/576 [==============================] - 0s 552us/step - loss: 10.2424 - mean_squared_error: 10.2424\n",
      "Epoch 477/500\n",
      "576/576 [==============================] - 0s 527us/step - loss: 10.2613 - mean_squared_error: 10.2613\n",
      "Epoch 478/500\n",
      "576/576 [==============================] - 0s 555us/step - loss: 10.2354 - mean_squared_error: 10.2354\n",
      "Epoch 479/500\n",
      "576/576 [==============================] - 0s 559us/step - loss: 10.2010 - mean_squared_error: 10.2010\n",
      "Epoch 480/500\n",
      "576/576 [==============================] - 0s 529us/step - loss: 10.2642 - mean_squared_error: 10.2642\n",
      "Epoch 481/500\n",
      "576/576 [==============================] - 0s 571us/step - loss: 10.2523 - mean_squared_error: 10.2523\n",
      "Epoch 482/500\n",
      "576/576 [==============================] - 0s 561us/step - loss: 10.2081 - mean_squared_error: 10.2081\n",
      "Epoch 483/500\n",
      "576/576 [==============================] - 0s 550us/step - loss: 10.2349 - mean_squared_error: 10.2349\n",
      "Epoch 484/500\n",
      "576/576 [==============================] - 0s 557us/step - loss: 10.2675 - mean_squared_error: 10.2675\n",
      "Epoch 485/500\n",
      "576/576 [==============================] - 0s 557us/step - loss: 10.2419 - mean_squared_error: 10.2419\n",
      "Epoch 486/500\n",
      "576/576 [==============================] - 0s 547us/step - loss: 10.2441 - mean_squared_error: 10.2441\n",
      "Epoch 487/500\n",
      "576/576 [==============================] - 0s 550us/step - loss: 10.2913 - mean_squared_error: 10.2913\n",
      "Epoch 488/500\n",
      "576/576 [==============================] - 0s 527us/step - loss: 10.2474 - mean_squared_error: 10.2474\n",
      "Epoch 489/500\n",
      "576/576 [==============================] - 0s 531us/step - loss: 10.2844 - mean_squared_error: 10.2844\n",
      "Epoch 490/500\n",
      "576/576 [==============================] - 0s 529us/step - loss: 10.2361 - mean_squared_error: 10.2361\n",
      "Epoch 491/500\n",
      "576/576 [==============================] - 0s 529us/step - loss: 10.2282 - mean_squared_error: 10.2282\n",
      "Epoch 492/500\n",
      "576/576 [==============================] - 0s 519us/step - loss: 10.2193 - mean_squared_error: 10.2193\n",
      "Epoch 493/500\n",
      "576/576 [==============================] - 0s 529us/step - loss: 10.2734 - mean_squared_error: 10.2734\n",
      "Epoch 494/500\n",
      "576/576 [==============================] - 0s 538us/step - loss: 10.2566 - mean_squared_error: 10.2566\n",
      "Epoch 495/500\n",
      "576/576 [==============================] - 0s 557us/step - loss: 10.2497 - mean_squared_error: 10.2497\n",
      "Epoch 496/500\n",
      "576/576 [==============================] - 0s 527us/step - loss: 10.2377 - mean_squared_error: 10.2377\n",
      "Epoch 497/500\n",
      "576/576 [==============================] - 0s 550us/step - loss: 10.2513 - mean_squared_error: 10.2513\n",
      "Epoch 498/500\n",
      "576/576 [==============================] - 0s 517us/step - loss: 10.2306 - mean_squared_error: 10.2306\n",
      "Epoch 499/500\n",
      "576/576 [==============================] - 0s 536us/step - loss: 10.2463 - mean_squared_error: 10.2463\n",
      "Epoch 500/500\n",
      "576/576 [==============================] - 0s 527us/step - loss: 10.2307 - mean_squared_error: 10.2307\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2476f717128>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%timeit\n",
    "model.fit(X_train, y_train, epochs = 500, batch_size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train r2: 0.89\n",
      "Test r2: 0.92\n"
     ]
    }
   ],
   "source": [
    "y_train_predict = model.predict(X_train)\n",
    "y_test_predict = model.predict(X_test)\n",
    "\n",
    "print('Train r2: {:.2f}'.format(r2_score(y_train, y_train_predict)))\n",
    "print('Test r2: {:.2f}'.format(r2_score(y_test, y_test_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 18.9114933 ],\n",
       "       [ 19.53837013],\n",
       "       [ 14.94638348],\n",
       "       [ 30.05441666],\n",
       "       [ 18.9114933 ],\n",
       "       [ 12.88966751],\n",
       "       [ 15.2800169 ],\n",
       "       [ 33.20976257],\n",
       "       [ 33.42266846],\n",
       "       [ 17.30390167],\n",
       "       [ 30.25447845],\n",
       "       [ 16.36349297],\n",
       "       [ 14.88229561],\n",
       "       [ 33.20976257],\n",
       "       [ 35.62066269],\n",
       "       [ 18.29689407],\n",
       "       [ 39.16533661],\n",
       "       [ 14.69166946],\n",
       "       [ 33.95184326],\n",
       "       [ 15.39590454],\n",
       "       [ 30.25447845],\n",
       "       [ 16.35479736],\n",
       "       [ 33.42266846],\n",
       "       [ 30.94549942],\n",
       "       [ 16.72874641],\n",
       "       [ 26.36516762],\n",
       "       [ 16.35479736],\n",
       "       [ 33.95184326],\n",
       "       [ 30.25447845],\n",
       "       [ 17.01878166],\n",
       "       [ 14.94638348],\n",
       "       [ 15.2800169 ],\n",
       "       [ 16.35479736],\n",
       "       [ 35.62066269],\n",
       "       [ 33.26756287],\n",
       "       [ 30.25447845],\n",
       "       [ 33.42266846],\n",
       "       [ 13.66536522],\n",
       "       [ 17.89488029],\n",
       "       [ 26.36516762],\n",
       "       [ 17.01878166],\n",
       "       [ 26.36516762],\n",
       "       [ 33.95184326],\n",
       "       [ 31.55599213],\n",
       "       [ 34.41160202],\n",
       "       [ 39.16533661],\n",
       "       [ 16.72874641],\n",
       "       [ 33.42266846],\n",
       "       [ 13.66536522],\n",
       "       [ 37.43637848],\n",
       "       [ 14.20477676],\n",
       "       [ 36.14707947],\n",
       "       [ 16.35479736],\n",
       "       [ 17.89488029],\n",
       "       [ 35.66204071],\n",
       "       [ 18.29689407],\n",
       "       [ 16.08357048],\n",
       "       [ 15.2800169 ],\n",
       "       [ 15.2800169 ],\n",
       "       [ 27.9134407 ],\n",
       "       [ 33.42266846],\n",
       "       [ 17.89488029],\n",
       "       [ 17.89488029],\n",
       "       [ 35.62066269],\n",
       "       [ 19.53837013],\n",
       "       [ 16.72874641],\n",
       "       [ 13.38809299],\n",
       "       [ 33.95184326],\n",
       "       [ 14.20477676],\n",
       "       [ 33.26756287],\n",
       "       [ 39.16533661],\n",
       "       [ 17.89488029],\n",
       "       [ 35.66204071],\n",
       "       [ 15.2800169 ],\n",
       "       [ 33.42266846],\n",
       "       [ 17.01878166],\n",
       "       [ 26.36516762],\n",
       "       [ 17.30390167],\n",
       "       [ 17.60211372],\n",
       "       [ 19.53837013],\n",
       "       [ 31.55599213],\n",
       "       [ 17.30390167],\n",
       "       [ 35.62066269],\n",
       "       [ 27.9134407 ],\n",
       "       [ 16.35479736],\n",
       "       [ 36.14707947],\n",
       "       [ 16.35479736],\n",
       "       [ 31.55599213],\n",
       "       [ 26.36516762],\n",
       "       [ 16.35479736],\n",
       "       [ 19.53837013],\n",
       "       [ 30.25447845],\n",
       "       [ 17.89488029],\n",
       "       [ 37.43637848],\n",
       "       [ 18.29689407],\n",
       "       [ 15.54594803],\n",
       "       [ 27.9134407 ],\n",
       "       [ 35.66204071],\n",
       "       [ 36.14707947],\n",
       "       [ 35.66204071],\n",
       "       [ 14.38796043],\n",
       "       [ 37.43637848],\n",
       "       [ 14.69166946],\n",
       "       [ 13.66536522],\n",
       "       [ 35.66204071],\n",
       "       [ 30.05441666],\n",
       "       [ 30.05441666],\n",
       "       [ 17.30390167],\n",
       "       [ 14.94638348],\n",
       "       [ 28.50571632],\n",
       "       [ 37.29547501],\n",
       "       [ 15.2800169 ],\n",
       "       [ 17.01878166],\n",
       "       [ 38.49252701],\n",
       "       [ 31.55599213],\n",
       "       [ 14.69166946],\n",
       "       [ 16.08357048],\n",
       "       [ 15.80827141],\n",
       "       [ 17.60211372],\n",
       "       [ 14.94638348],\n",
       "       [ 30.94549942],\n",
       "       [ 30.05441666],\n",
       "       [ 15.54594803],\n",
       "       [ 33.26756287],\n",
       "       [ 27.9134407 ],\n",
       "       [ 17.01878166],\n",
       "       [ 13.83980751],\n",
       "       [ 33.95184326],\n",
       "       [ 33.26756287],\n",
       "       [ 33.20976257],\n",
       "       [ 16.36349297],\n",
       "       [ 15.54594803],\n",
       "       [ 33.26756287],\n",
       "       [ 19.53837013],\n",
       "       [ 38.49252701],\n",
       "       [ 34.41160202],\n",
       "       [ 38.49252701],\n",
       "       [ 14.20477676],\n",
       "       [ 15.2800169 ],\n",
       "       [ 33.26756287],\n",
       "       [ 17.89488029],\n",
       "       [ 30.05441666],\n",
       "       [ 38.49252701],\n",
       "       [ 35.66204071],\n",
       "       [ 39.16533661],\n",
       "       [ 18.9114933 ],\n",
       "       [ 36.14707947],\n",
       "       [ 17.60211372],\n",
       "       [ 14.20477676],\n",
       "       [ 13.66536522],\n",
       "       [ 17.60211372],\n",
       "       [ 26.36516762],\n",
       "       [ 14.69166946],\n",
       "       [ 16.08357048],\n",
       "       [ 13.66536522],\n",
       "       [ 14.20477676],\n",
       "       [ 35.62066269],\n",
       "       [ 27.50131607],\n",
       "       [ 35.62066269],\n",
       "       [ 23.53553581],\n",
       "       [ 34.41160202],\n",
       "       [ 16.72874641],\n",
       "       [ 30.05441666],\n",
       "       [ 15.54594803],\n",
       "       [ 14.69166946],\n",
       "       [ 17.60211372],\n",
       "       [ 15.2800169 ],\n",
       "       [ 33.26756287],\n",
       "       [ 13.83980751],\n",
       "       [ 14.20477676],\n",
       "       [ 14.20477676],\n",
       "       [ 30.94549942],\n",
       "       [ 16.08357048],\n",
       "       [ 18.9114933 ],\n",
       "       [ 18.29689407],\n",
       "       [ 18.9114933 ],\n",
       "       [ 35.66204071],\n",
       "       [ 17.01878166],\n",
       "       [ 27.50131607],\n",
       "       [ 16.35479736],\n",
       "       [ 17.01878166],\n",
       "       [ 30.94549942],\n",
       "       [ 18.9114933 ],\n",
       "       [ 14.20477676],\n",
       "       [ 33.26756287],\n",
       "       [ 13.66536522],\n",
       "       [ 16.35479736],\n",
       "       [ 18.29689407],\n",
       "       [ 17.60211372],\n",
       "       [ 34.41160202],\n",
       "       [ 15.54594803],\n",
       "       [ 33.26756287]], dtype=float32)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict = model.predict(X_test)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Base Regression & Ensemble Learning - Heating & Cooling\n",
    "\n",
    "- In this part, there are eleven base regressors used to find parameters, train, cross-validate, and test in R-squared:\n",
    "\n",
    "  KNN, Linear/Ridge/lasso, Polynomial/Ridge/Lasso, Linear SVR, Kernelized RBF, Decision Tree Regressor, Random Forest.\n",
    "  \n",
    "  \n",
    "- Bagging Ensemble is used to improve KNN, Linear/Ridge/lasso, and Decision Tree.\n",
    "- Adaboost Ensemble is used for the same models in Bagging. Gradient Boosting is exclusively run for Decision Tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heating Load Y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1,y1, random_state = 10)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_neighbors': 1}\n",
      "Best cross-validation score: 1.00\n",
      "Best estimator:\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
      "          weights='uniform')\n",
      "R-squared score (training): 0.997628\n",
      "R-squared score (test): 0.996464\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "param = {'n_neighbors':[1, 3, 7, 15, 55]} #create dict of parameters\n",
    "\n",
    "grid_search_knn = GridSearchCV(KNeighborsRegressor(),param, cv=10, scoring='r2') # each parameter has a cv of 10 folds.\n",
    "grid_search_knn.fit(X_train, y_train)\n",
    "print(\"Best parameters: {}\".format(grid_search_knn.best_params_)) \n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search_knn.best_score_)) \n",
    "print(\"Best estimator:\\n{}\".format(grid_search_knn.best_estimator_))\n",
    "\n",
    "train_score = grid_search_knn.score(X_train,y_train)\n",
    "test_score = grid_search_knn.score(X_test,y_test)\n",
    "print('R-squared score (training): {:.6f}'\n",
    "     .format(train_score))\n",
    "print('R-squared score (test): {:.6f}'\n",
    "     .format(test_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear Regression, Ridge, Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear model intercept: 28.68336874266516\n",
      "linear model coeff:\n",
      "[-21.68634717  -7.86138932   1.03715324 -11.28852716  15.23988284\n",
      "   8.0844496    0.95503272]\n",
      "R-squared score (training): 0.911857\n",
      "R-squared score (test): 0.928479\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression \n",
    "linreg= LinearRegression()\n",
    "linreg= linreg.fit(X_train,y_train) \n",
    "print('linear model intercept: {}'\n",
    "     .format(linreg.intercept_))\n",
    "print('linear model coeff:\\n{}'\n",
    "     .format(linreg.coef_))\n",
    "print('R-squared score (training): {:.6f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.6f}'\n",
    "     .format(linreg.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'alpha': 0}\n",
      "Best cross-validation score: 0.908532\n",
      "Best estimator:\n",
      "Ridge(alpha=0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "R-squared score (training): 0.911857\n",
      "R-squared score (test): 0.928479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo E560\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 8.88981597094299e-18\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "C:\\Users\\Lenovo E560\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 3.314054420757686e-17\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "C:\\Users\\Lenovo E560\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 6.860734002307293e-18\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "C:\\Users\\Lenovo E560\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 4.8732100259317657e-17\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "C:\\Users\\Lenovo E560\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 8.700183968890924e-17\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "C:\\Users\\Lenovo E560\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 6.94257050283992e-17\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "C:\\Users\\Lenovo E560\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 3.8324424317879757e-17\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "C:\\Users\\Lenovo E560\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 2.3699804032364814e-17\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "C:\\Users\\Lenovo E560\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 8.070736273948884e-17\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "alpha = {'alpha':[0, 1, 10, 20, 50, 100, 1000]} #create dict of parameters\n",
    "\n",
    "grid_search_ridge = GridSearchCV(Ridge(),alpha, cv=10) # each parameter has a cv of 10 folds. Try max_iter if possible.\n",
    "grid_search_ridge.fit(X_train, y_train)\n",
    "print(\"Best parameters: {}\".format(grid_search_ridge.best_params_))\n",
    "print(\"Best cross-validation score: {:.6f}\".format(grid_search_ridge.best_score_)) \n",
    "print(\"Best estimator:\\n{}\".format(grid_search_ridge.best_estimator_))\n",
    "\n",
    "train_score = grid_search_ridge.score(X_train,y_train)\n",
    "test_score = grid_search_ridge.score(X_test,y_test)\n",
    "print('R-squared score (training): {:.6f}'\n",
    "     .format(train_score))\n",
    "print('R-squared score (test): {:.6f}'\n",
    "     .format(test_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'alpha': 0.5}\n",
      "Best cross-validation score: 0.841679\n",
      "Best estimator:\n",
      "Lasso(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "R-squared score (training): 0.844847\n",
      "R-squared score (test): 0.868881\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "alpha = {'alpha':[0.5, 1, 2, 3, 5, 10, 20, 50]} #create dict of parameters\n",
    "\n",
    "grid_search_Lasso = GridSearchCV(Lasso(),alpha, cv=10) # each parameter has a cv of 10 folds. Try max_iter if possible.\n",
    "grid_search_Lasso.fit(X_train, y_train)\n",
    "print(\"Best parameters: {}\".format(grid_search_Lasso.best_params_)) # The pairt hta has highest score\n",
    "print(\"Best cross-validation score: {:.6f}\".format(grid_search_Lasso.best_score_)) \n",
    "print(\"Best estimator:\\n{}\".format(grid_search_Lasso.best_estimator_))\n",
    "\n",
    "train_score = grid_search_Lasso.score(X_train,y_train)\n",
    "test_score = grid_search_Lasso.score(X_test,y_test)\n",
    "print('R-squared score (training): {:.6f}'\n",
    "     .format(train_score))\n",
    "print('R-squared score (test): {:.6f}'\n",
    "     .format(test_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Polynomial, Ridge, Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(poly deg 2) linear model coeff (w):\n",
      "[  6.48668366e-07  -3.53979817e+11   4.09673723e+10  -2.64800603e+11\n",
      "   3.91516586e+11   5.37707007e+11   3.30866119e+01   8.28208099e+00\n",
      "  -1.00843737e+05   2.03459651e+10  -4.31960538e+11  -2.13263416e+11\n",
      "   3.29398062e+11   2.16674805e-01  -1.72256470e-01   6.88886755e+10\n",
      "   2.63928066e+11  -1.04579310e+12   1.68753175e+12   8.10016630e+09\n",
      "   1.64523225e+11  -2.25369935e+11   4.40220197e+11  -1.35123328e+12\n",
      "  -4.17281294e+09  -8.47543888e+10   1.04964854e+12  -8.18401039e+11\n",
      "  -4.41827255e+09  -8.97399411e+10   5.04614550e+11  -6.79693604e+00\n",
      "   7.36694336e-02  -2.85269165e+00  -3.50024414e+00  -3.28320312e+00]\n",
      "(poly deg 2) linear model intercept (b): -632784811294.386\n",
      "(poly deg 2) R-squared score (training): 0.994948\n",
      "(poly deg 2) R-squared score (test): 0.994589\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X1)\n",
    "\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_poly, y1, random_state = 10)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_2 = scaler.fit_transform(X_train_2)\n",
    "X_test_2 = scaler.transform(X_test_2)\n",
    "\n",
    "linreg_2 = LinearRegression().fit(X_train_2, y_train_2)\n",
    "\n",
    "print('(poly deg 2) linear model coeff (w):\\n{}'\n",
    "     .format(linreg_2.coef_))\n",
    "print('(poly deg 2) linear model intercept (b): {:.3f}'\n",
    "     .format(linreg_2.intercept_))\n",
    "print('(poly deg 2) R-squared score (training): {:.6f}'\n",
    "     .format(linreg_2.score(X_train_2, y_train_2)))\n",
    "print('(poly deg 2) R-squared score (test): {:.6f}\\n'\n",
    "     .format(linreg_2.score(X_test_2, y_test_2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(poly deg 2 + ridge) linear model coeff (w):\n",
      "[  0.00000000e+00  -1.09899082e+00   7.75722169e-01   2.19676347e+00\n",
      "  -6.74297582e-01   1.43515317e+00   3.00699362e+00   1.31929517e+00\n",
      "  -6.52488071e-01   3.11443189e+00   3.89565645e+00  -3.61787960e+00\n",
      "  -1.29238188e+00   4.56289308e+00   1.35163159e+00  -7.00069711e-02\n",
      "  -1.94570665e-01   4.73124502e-03   6.01378437e+00   3.83209944e-01\n",
      "   7.52426440e-01  -8.86104827e-01   7.41932968e-01   5.94429138e+00\n",
      "   2.90295114e+00   1.28913392e+00  -4.86736570e-01   2.28256676e+00\n",
      "  -2.03912451e+00   1.61933108e-01   1.43515317e+00   5.30728352e+00\n",
      "   1.46671989e+00  -1.54922489e+00  -3.71190738e+00  -2.31693639e+00]\n",
      "(poly deg 2 + ridge) linear model intercept (b): 7.941\n",
      "(poly deg 2 + ridge) R-squared score (training): 0.932626\n",
      "(poly deg 2 + ridge) R-squared score (test): 0.946167\n"
     ]
    }
   ],
   "source": [
    "linreg_ridge = Ridge().fit(X_train_2, y_train_2)\n",
    "\n",
    "print('(poly deg 2 + ridge) linear model coeff (w):\\n{}'\n",
    "     .format(linreg_ridge.coef_))\n",
    "print('(poly deg 2 + ridge) linear model intercept (b): {:.3f}'\n",
    "     .format(linreg_ridge.intercept_))\n",
    "print('(poly deg 2 + ridge) R-squared score (training): {:.6f}'\n",
    "     .format(linreg_ridge.score(X_train_2, y_train_2)))\n",
    "print('(poly deg 2 + ridge) R-squared score (test): {:.6f}'\n",
    "     .format(linreg_ridge.score(X_test_2, y_test_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(poly deg 2 + Lasso) linear model coeff (w):\n",
      "[  0.           0.          -0.           0.          -0.          13.07386433\n",
      "   0.           0.           0.          -0.           0.          -0.           0.\n",
      "   0.           0.          -0.           0.          -0.           0.           0.\n",
      "   0.           0.          -0.           0.           0.           0.          -0.\n",
      "   0.           0.           0.           0.66154105   0.73639568   0.           0.\n",
      "   0.           0.        ]\n",
      "(poly deg 2 + Lasso) linear model intercept (b): 15.271\n",
      "(poly deg 2 + Lasso) R-squared score (training): 0.755815\n",
      "(poly deg 2 + Lasso) R-squared score (test): 0.779235\n"
     ]
    }
   ],
   "source": [
    "linreg_lasso = Lasso().fit(X_train_2, y_train_2)\n",
    "\n",
    "print('(poly deg 2 + Lasso) linear model coeff (w):\\n{}'\n",
    "     .format(linreg_lasso.coef_))\n",
    "print('(poly deg 2 + Lasso) linear model intercept (b): {:.3f}'\n",
    "     .format(linreg_lasso.intercept_))\n",
    "print('(poly deg 2 + Lasso) R-squared score (training): {:.6f}'\n",
    "     .format(linreg_lasso.score(X_train_2, y_train_2)))\n",
    "print('(poly deg 2 + Lasso) R-squared score (test): {:.6f}'\n",
    "     .format(linreg_lasso.score(X_test_2, y_test_2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 50, 'epsilon': 0.5}\n",
      "Best cross-validation score: 0.905543\n",
      "Best estimator:\n",
      "SVR(C=50, cache_size=200, coef0=0.0, degree=3, epsilon=0.5, gamma='auto',\n",
      "  kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
      "0.908086976061 0.927375518885\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "param_svr = {'C':[1, 10, 50, 100],\n",
    "            'epsilon':[0.5,1,5,10]} \n",
    "grid_search_svr = GridSearchCV(SVR(kernel='linear'),param_svr, cv=5) # each parameter has a cv of 5 folds. Try max_iter if possible.\n",
    "grid_search_svr.fit(X_train, y_train)\n",
    "print(\"Best parameters: {}\".format(grid_search_svr.best_params_)) \n",
    "print(\"Best cross-validation score: {:.6f}\".format(grid_search_svr.best_score_)) \n",
    "print(\"Best estimator:\\n{}\".format(grid_search_svr.best_estimator_))\n",
    "\n",
    "train_score = grid_search_svr.score(X_train,y_train)\n",
    "test_score = grid_search_svr.score(X_test,y_test)\n",
    "print(train_score, test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Kernelized RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 50, 'epsilon': 1, 'gamma': 10}\n",
      "Best cross-validation score: 0.991814\n",
      "Best estimator:\n",
      "SVR(C=50, cache_size=200, coef0=0.0, degree=3, epsilon=1, gamma=10,\n",
      "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
      "0.994737691898 0.992007295318\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_rbf = {'C':[ 10, 50, 100],\n",
    "             'gamma':[0.1,1,10],\n",
    "            'epsilon':[1,5,10]} \n",
    "\n",
    "grid_search_rbf = GridSearchCV(SVR(kernel='rbf'),param_rbf, cv=5) # each parameter has a cv of 10 folds. Try max_iter if possible.\n",
    "grid_search_rbf.fit(X_train, y_train)\n",
    "print(\"Best parameters: {}\".format(grid_search_rbf.best_params_)) # The pairt hta has highest score\n",
    "print(\"Best cross-validation score: {:.6f}\".format(grid_search_rbf.best_score_)) \n",
    "print(\"Best estimator:\\n{}\".format(grid_search_rbf.best_estimator_))\n",
    "\n",
    "train_score = grid_search_rbf.score(X_train,y_train)\n",
    "test_score = grid_search_rbf.score(X_test,y_test)\n",
    "print(train_score, test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': 8, 'max_features': 5}\n",
      "Best cross-validation score: 0.997546\n",
      "Best estimator:\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=8, max_features=5,\n",
      "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "           min_impurity_split=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           presort=False, random_state=None, splitter='best')\n",
      "0.998788551058 0.997120464507\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor \n",
    "dtr= DecisionTreeRegressor()\n",
    "param_dtr = {'max_features':[1,3,5,7],'max_depth':[2,4,6,8,10]}\n",
    "\n",
    "grid_search_dtr = GridSearchCV(dtr,param_dtr, cv=5) # each parameter has a cv of 10 folds. Try max_iter if possible.\n",
    "grid_search_dtr.fit(X_train, y_train)\n",
    "print(\"Best parameters: {}\".format(grid_search_dtr.best_params_)) # The pairt hta has highest score\n",
    "print(\"Best cross-validation score: {:.6f}\".format(grid_search_dtr.best_score_)) \n",
    "print(\"Best estimator:\\n{}\".format(grid_search_dtr.best_estimator_))\n",
    "\n",
    "train_score = grid_search_dtr.score(X_train,y_train)\n",
    "test_score = grid_search_dtr.score(X_test,y_test)\n",
    "print(train_score, test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': 8, 'max_features': 7}\n",
      "Best cross-validation score: 0.997432\n",
      "Best estimator:\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=8,\n",
      "           max_features=7, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "           min_impurity_split=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "           verbose=0, warm_start=False)\n",
      "0.998816736358 0.997364327381\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor \n",
    "param_RF = {'max_features':[1,3,5,7],'max_depth':[2,4,6,8,10]}\n",
    "\n",
    "grid_search_RF = GridSearchCV(RandomForestRegressor(),param_RF, cv=5) # each parameter has a cv of 10 folds. Try max_iter if possible.\n",
    "grid_search_RF.fit(X_train, y_train)\n",
    "print(\"Best parameters: {}\".format(grid_search_RF.best_params_)) # The pairt hta has highest score\n",
    "print(\"Best cross-validation score: {:.6f}\".format(grid_search_RF.best_score_)) \n",
    "print(\"Best estimator:\\n{}\".format(grid_search_RF.best_estimator_))\n",
    "\n",
    "train_score = grid_search_RF.score(X_train,y_train)\n",
    "test_score = grid_search_RF.score(X_test,y_test)\n",
    "print(train_score, test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bagging Ensemble\n",
    " - By order: KNN, Linear Regression, Ridge, Lasso, Decision Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.966505\n",
      "Out of bag score: 0.956672\n"
     ]
    }
   ],
   "source": [
    "bag_knn= BaggingRegressor(KNeighborsRegressor(n_neighbors=1), \n",
    "                           n_estimators=500, max_samples=100, bootstrap=True, n_jobs=-1, random_state=10,oob_score=True)\n",
    "bag_knn.fit(X1, y1)\n",
    "print('Train score: {0:0.6f}'.format(bag_knn.score(X1,y1)))\n",
    "print('Out of bag score: {0:0.6f}'.format(bag_knn.oob_score_)) # Best one so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.916145\n",
      "Out of bag score: 0.914662\n"
     ]
    }
   ],
   "source": [
    "bag_linreg= BaggingRegressor(LinearRegression(), \n",
    "                           n_estimators=500, max_samples=100, bootstrap=True, n_jobs=-1, random_state=10,oob_score=True)\n",
    "bag_linreg.fit(X1, y1)\n",
    "print('Train score: {0:0.6f}'.format(bag_linreg.score(X1,y1)))\n",
    "print('Out of bag score: {0:0.6f}'.format(bag_linreg.oob_score_)) # Best one so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.916157\n",
      "Out of bag score: 0.914678\n"
     ]
    }
   ],
   "source": [
    "bag_ridge= BaggingRegressor(Ridge(alpha=0, random_state=10), \n",
    "                           n_estimators=500, max_samples=100, bootstrap=True, n_jobs=-1, random_state=10,oob_score=True)\n",
    "bag_ridge.fit(X1, y1)\n",
    "print('Train score: {0:0.6f}'.format(bag_ridge.score(X1,y1)))\n",
    "print('Out of bag score: {0:0.6f}'.format(bag_ridge.oob_score_)) # Best one so far\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.828037\n",
      "Out of bag score: 0.825610\n"
     ]
    }
   ],
   "source": [
    "bag_lasso= BaggingRegressor(Lasso(alpha=0.5, random_state=10), \n",
    "                           n_estimators=500, max_samples=100, bootstrap=True, n_jobs=-1, random_state=10,oob_score=True)\n",
    "bag_lasso.fit(X1, y1)\n",
    "print('Train score: {0:0.6f}'.format(bag_lasso.score(X1,y1)))\n",
    "print('Out of bag score: {0:0.6f}'.format(bag_lasso.oob_score_)) # Best one so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.995505\n",
      "Out of bag score: 0.994302\n"
     ]
    }
   ],
   "source": [
    "bag_dtr = BaggingRegressor(DecisionTreeRegressor(max_depth=8,max_features=5), \n",
    "                            n_estimators=500, max_samples=100, bootstrap=True, n_jobs=-1, random_state=10,oob_score=True)\n",
    "bag_dtr.fit(X1, y1)\n",
    "print('Train score: {0:0.6f}'.format(bag_dtr.score(X1,y1)))\n",
    "print('Out of bag score: {0:0.6f}'.format(bag_dtr.oob_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosting Ensemble( Adaboost)\n",
    " - By order: KNN, Linear Regression, Ridge, Lasso, Decision Tree, Gradient Boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.998060\n",
      "Test score: 0.995951\n"
     ]
    }
   ],
   "source": [
    "ada_knn= AdaBoostRegressor(KNeighborsRegressor(n_neighbors=1),n_estimators=500,\n",
    "                             learning_rate=0.5, random_state=10)\n",
    "ada_knn.fit(X_train, y_train)\n",
    "print('Train score: {0:0.6f}'.format(ada_knn.score(X_train,y_train)))\n",
    "print('Test score: {0:0.6f}'.format(ada_knn.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.910117\n",
      "Test score: 0.926317\n"
     ]
    }
   ],
   "source": [
    "ada_linreg= AdaBoostRegressor(LinearRegression(),n_estimators=500,\n",
    "                            learning_rate=0.5, random_state=10)\n",
    "ada_linreg.fit(X_train, y_train)\n",
    "print('Train score: {0:0.6f}'.format(ada_linreg.score(X_train,y_train)))\n",
    "print('Test score: {0:0.6f}'.format(ada_linreg.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.910119\n",
      "Test score: 0.925539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo E560\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 2.6303009397471052e-17\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "C:\\Users\\Lenovo E560\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 1.0904488494697053e-16\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "C:\\Users\\Lenovo E560\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 4.80705512813155e-17\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "C:\\Users\\Lenovo E560\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 4.1635782745159276e-17\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "C:\\Users\\Lenovo E560\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 6.858974896171097e-17\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "C:\\Users\\Lenovo E560\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 7.53043795369843e-17\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "C:\\Users\\Lenovo E560\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 8.322192046295244e-17\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "C:\\Users\\Lenovo E560\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 9.264132433885823e-18\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "C:\\Users\\Lenovo E560\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 2.817853424524017e-17\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "C:\\Users\\Lenovo E560\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 5.645820891514877e-17\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "C:\\Users\\Lenovo E560\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 3.057040332384364e-17\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "C:\\Users\\Lenovo E560\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 6.680513853329436e-17\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "C:\\Users\\Lenovo E560\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 6.234445799752796e-17\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "ada_ridge= AdaBoostRegressor(Ridge(alpha=0, random_state=10), \n",
    "                            n_estimators=500, learning_rate=0.5, random_state=10)\n",
    "ada_ridge.fit(X_train, y_train)\n",
    "print('Train score: {0:0.6f}'.format(ada_ridge.score(X_train,y_train)))\n",
    "print('Test score: {0:0.6f}'.format(ada_ridge.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.879642\n",
      "Test score: 0.897249\n"
     ]
    }
   ],
   "source": [
    "ada_lasso= AdaBoostRegressor(Lasso(alpha=0.5, random_state=10), \n",
    "                             n_estimators=500, learning_rate=0.5, random_state=10)\n",
    "ada_lasso.fit(X_train, y_train)\n",
    "print('Train score: {0:0.6f}'.format(ada_lasso.score(X_train,y_train)))\n",
    "print('Test score: {0:0.6f}'.format(ada_lasso.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.998704\n",
      "Test score: 0.997325\n"
     ]
    }
   ],
   "source": [
    "ada_dtr = AdaBoostRegressor(DecisionTreeRegressor(max_depth=8,max_features=5, random_state=10), \n",
    "                            n_estimators=500, learning_rate=0.5, random_state=10)\n",
    "ada_dtr.fit(X_train, y_train)\n",
    "print('Train score: {0:0.6f}'.format(ada_dtr.score(X_train,y_train)))\n",
    "print('Test score: {0:0.6f}'.format(ada_dtr.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.995473\n",
      "Test score: 0.993922\n"
     ]
    }
   ],
   "source": [
    "gbr = GradientBoostingRegressor(learning_rate = 0.05)\n",
    "gbr.fit(X_train,y_train)\n",
    "print('Train score: {0:0.6f}'.format(gbr.score(X_train,y_train)))\n",
    "print('Test score: {0:0.6f}'.format(gbr.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cooling Load Y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "X_train, X_test, y_train, y_test = train_test_split(X2,y2, random_state = 10)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KNN Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_neighbors': 7}\n",
      "Best cross-validation score: 0.96\n",
      "Best estimator:\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=7, p=2,\n",
      "          weights='uniform')\n",
      "R-squared score (training): 0.963401\n",
      "R-squared score (test): 0.965582\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "param = {'n_neighbors':[1, 3, 7, 15, 55]} #create dict of parameters\n",
    "\n",
    "grid_search_knn = GridSearchCV(KNeighborsRegressor(),param, cv=10, scoring='r2') # each parameter has a cv of 10 folds.\n",
    "grid_search_knn.fit(X_train, y_train)\n",
    "print(\"Best parameters: {}\".format(grid_search_knn.best_params_)) \n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search_knn.best_score_)) \n",
    "print(\"Best estimator:\\n{}\".format(grid_search_knn.best_estimator_))\n",
    "\n",
    "train_score = grid_search_knn.score(X_train,y_train)\n",
    "test_score = grid_search_knn.score(X_test,y_test)\n",
    "print('R-squared score (training): {:.6f}'\n",
    "     .format(train_score))\n",
    "print('R-squared score (test): {:.6f}'\n",
    "     .format(test_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear Regression, Ridge, Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear model intercept: 33.802185786725076\n",
      "linear model coeff:\n",
      "[-23.47431709  -8.40545573  -1.13889429 -10.32146763  15.95025307\n",
      "   5.89706486]\n",
      "R-squared score (training): 0.879912\n",
      "R-squared score (test): 0.909064\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression \n",
    "linreg= LinearRegression()\n",
    "linreg= linreg.fit(X_train,y_train) \n",
    "print('linear model intercept: {}'\n",
    "     .format(linreg.intercept_))\n",
    "print('linear model coeff:\\n{}'\n",
    "     .format(linreg.coef_))\n",
    "print('R-squared score (training): {:.6f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.6f}'\n",
    "     .format(linreg.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'alpha': 0}\n",
      "Best cross-validation score: 0.876942\n",
      "Best estimator:\n",
      "Ridge(alpha=0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "R-squared score (training): 0.879912\n",
      "R-squared score (test): 0.909064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo E560\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 1.2490755223216238e-18\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "C:\\Users\\Lenovo E560\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 5.5171733691086574e-17\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "C:\\Users\\Lenovo E560\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 1.9595517245386946e-17\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "C:\\Users\\Lenovo E560\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 5.1437026519676165e-17\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "C:\\Users\\Lenovo E560\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 9.212889899930885e-17\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "C:\\Users\\Lenovo E560\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 6.48578701191177e-17\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "C:\\Users\\Lenovo E560\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 3.5312252547388997e-17\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "C:\\Users\\Lenovo E560\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 3.0952754514292325e-17\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "C:\\Users\\Lenovo E560\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 8.228489097158585e-17\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "alpha = {'alpha':[0, 1, 10, 20, 50, 100, 1000]} #create dict of parameters\n",
    "\n",
    "grid_search_ridge = GridSearchCV(Ridge(),alpha, cv=10) # each parameter has a cv of 10 folds. Try max_iter if possible.\n",
    "grid_search_ridge.fit(X_train, y_train)\n",
    "print(\"Best parameters: {}\".format(grid_search_ridge.best_params_))\n",
    "print(\"Best cross-validation score: {:.6f}\".format(grid_search_ridge.best_score_)) \n",
    "print(\"Best estimator:\\n{}\".format(grid_search_ridge.best_estimator_))\n",
    "\n",
    "train_score = grid_search_ridge.score(X_train,y_train)\n",
    "test_score = grid_search_ridge.score(X_test,y_test)\n",
    "print('R-squared score (training): {:.6f}'\n",
    "     .format(train_score))\n",
    "print('R-squared score (test): {:.6f}'\n",
    "     .format(test_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'alpha': 0.5}\n",
      "Best cross-validation score: 0.798141\n",
      "Best estimator:\n",
      "Lasso(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "R-squared score (training): 0.802544\n",
      "R-squared score (test): 0.837901\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "alpha = {'alpha':[0.5, 1, 2, 3, 5, 10, 20, 50]} #create dict of parameters\n",
    "\n",
    "grid_search_Lasso = GridSearchCV(Lasso(),alpha, cv=10) # each parameter has a cv of 10 folds. Try max_iter if possible.\n",
    "grid_search_Lasso.fit(X_train, y_train)\n",
    "print(\"Best parameters: {}\".format(grid_search_Lasso.best_params_)) # The pairt hta has highest score\n",
    "print(\"Best cross-validation score: {:.6f}\".format(grid_search_Lasso.best_score_)) \n",
    "print(\"Best estimator:\\n{}\".format(grid_search_Lasso.best_estimator_))\n",
    "\n",
    "train_score = grid_search_Lasso.score(X_train,y_train)\n",
    "test_score = grid_search_Lasso.score(X_test,y_test)\n",
    "print('R-squared score (training): {:.6f}'\n",
    "     .format(train_score))\n",
    "print('R-squared score (test): {:.6f}'\n",
    "     .format(test_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Polynomial, Ridge, Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(poly deg 2) linear model coeff (w):\n",
      "[ -2.66762813e+12   1.19980657e+13  -7.38382851e+12   1.24725835e+13\n",
      "  -3.77179875e+13   6.49294132e+13   3.75029539e+01  -1.40279468e+05\n",
      "   3.07562227e+11  -6.52978274e+12  -1.47095328e+13  -1.11648665e+13\n",
      "  -1.25585938e+00  -2.40880004e+13  -3.10353351e+13  -3.95275710e+12\n",
      "  -4.80445119e+13   2.20716065e+12   3.11642607e+13   2.64415959e+13\n",
      "   3.73499087e+13  -1.13702215e+12  -3.78039726e+12   3.52750729e+13\n",
      "  -1.20390581e+12  -1.29202717e+14  -9.44335938e+00  -3.39648438e+00]\n",
      "(poly deg 2) linear model intercept (b): 64856990589111.375\n",
      "(poly deg 2) R-squared score (training): 0.969687\n",
      "(poly deg 2) R-squared score (test): 0.972233\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X2)\n",
    "\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_poly, y2, random_state = 10)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_2 = scaler.fit_transform(X_train_2)\n",
    "X_test_2 = scaler.transform(X_test_2)\n",
    "\n",
    "linreg_2 = LinearRegression().fit(X_train_2, y_train_2)\n",
    "\n",
    "print('(poly deg 2) linear model coeff (w):\\n{}'\n",
    "     .format(linreg_2.coef_))\n",
    "print('(poly deg 2) linear model intercept (b): {:.3f}'\n",
    "     .format(linreg_2.intercept_))\n",
    "print('(poly deg 2) R-squared score (training): {:.6f}'\n",
    "     .format(linreg_2.score(X_train_2, y_train_2)))\n",
    "print('(poly deg 2) R-squared score (test): {:.6f}\\n'\n",
    "     .format(linreg_2.score(X_test_2, y_test_2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(poly deg 2 + ridge) linear model coeff (w):\n",
      "[ 0.         -2.01719174  0.85113554  2.30315915 -0.65649862  1.54939651\n",
      "  2.43120657 -1.95277644  2.7429515   4.76500384 -4.4737157  -1.32755956\n",
      "  4.06065375 -0.47155677 -1.3901029   0.07957092  5.79086713 -0.25753033\n",
      " -2.80388615  1.18604615  5.60376597  1.43131236 -0.65807924  2.67869369\n",
      " -1.82393395  1.54939651  4.03812522 -1.95987631]\n",
      "(poly deg 2 + ridge) linear model intercept (b): 15.377\n",
      "(poly deg 2 + ridge) R-squared score (training): 0.893691\n",
      "(poly deg 2 + ridge) R-squared score (test): 0.919163\n"
     ]
    }
   ],
   "source": [
    "linreg_ridge = Ridge().fit(X_train_2, y_train_2)\n",
    "\n",
    "print('(poly deg 2 + ridge) linear model coeff (w):\\n{}'\n",
    "     .format(linreg_ridge.coef_))\n",
    "print('(poly deg 2 + ridge) linear model intercept (b): {:.3f}'\n",
    "     .format(linreg_ridge.intercept_))\n",
    "print('(poly deg 2 + ridge) R-squared score (training): {:.6f}'\n",
    "     .format(linreg_ridge.score(X_train_2, y_train_2)))\n",
    "print('(poly deg 2 + ridge) R-squared score (test): {:.6f}'\n",
    "     .format(linreg_ridge.score(X_test_2, y_test_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(poly deg 2 + Lasso) linear model coeff (w):\n",
      "[  0.           0.          -0.           0.          -0.          12.25163365\n",
      "   0.           0.          -0.           0.          -0.           0.           0.\n",
      "  -0.           0.          -0.           0.           0.           0.          -0.\n",
      "   0.           0.          -0.           0.           0.           0.75812713\n",
      "   0.           0.        ]\n",
      "(poly deg 2 + Lasso) linear model intercept (b): 18.259\n",
      "(poly deg 2 + Lasso) R-squared score (training): 0.749031\n",
      "(poly deg 2 + Lasso) R-squared score (test): 0.779780\n"
     ]
    }
   ],
   "source": [
    "linreg_lasso = Lasso().fit(X_train_2, y_train_2)\n",
    "\n",
    "print('(poly deg 2 + Lasso) linear model coeff (w):\\n{}'\n",
    "     .format(linreg_lasso.coef_))\n",
    "print('(poly deg 2 + Lasso) linear model intercept (b): {:.3f}'\n",
    "     .format(linreg_lasso.intercept_))\n",
    "print('(poly deg 2 + Lasso) R-squared score (training): {:.6f}'\n",
    "     .format(linreg_lasso.score(X_train_2, y_train_2)))\n",
    "print('(poly deg 2 + Lasso) R-squared score (test): {:.6f}'\n",
    "     .format(linreg_lasso.score(X_test_2, y_test_2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 50, 'epsilon': 1}\n",
      "Best cross-validation score: 0.872321\n",
      "Best estimator:\n",
      "SVR(C=50, cache_size=200, coef0=0.0, degree=3, epsilon=1, gamma='auto',\n",
      "  kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
      "0.878572841207 0.910426432572\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "param_svr = {'C':[1, 10, 50, 100],\n",
    "            'epsilon':[0.5,1,5,10]} \n",
    "grid_search_svr = GridSearchCV(SVR(kernel='linear'),param_svr, cv=5) # each parameter has a cv of 5 folds. Try max_iter if possible.\n",
    "grid_search_svr.fit(X_train, y_train)\n",
    "print(\"Best parameters: {}\".format(grid_search_svr.best_params_)) \n",
    "print(\"Best cross-validation score: {:.6f}\".format(grid_search_svr.best_score_)) \n",
    "print(\"Best estimator:\\n{}\".format(grid_search_svr.best_estimator_))\n",
    "\n",
    "train_score = grid_search_svr.score(X_train,y_train)\n",
    "test_score = grid_search_svr.score(X_test,y_test)\n",
    "print(train_score, test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Kernelized RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 50, 'epsilon': 1, 'gamma': 10}\n",
      "Best cross-validation score: 0.957084\n",
      "Best estimator:\n",
      "SVR(C=50, cache_size=200, coef0=0.0, degree=3, epsilon=1, gamma=10,\n",
      "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
      "0.96697053497 0.966127449204\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_rbf = {'C':[ 10, 50, 100],\n",
    "             'gamma':[0.1,1,10],\n",
    "            'epsilon':[1,5,10]} \n",
    "\n",
    "grid_search_rbf = GridSearchCV(SVR(kernel='rbf'),param_rbf, cv=5) # each parameter has a cv of 10 folds. Try max_iter if possible.\n",
    "grid_search_rbf.fit(X_train, y_train)\n",
    "print(\"Best parameters: {}\".format(grid_search_rbf.best_params_)) # The pairt hta has highest score\n",
    "print(\"Best cross-validation score: {:.6f}\".format(grid_search_rbf.best_score_)) \n",
    "print(\"Best estimator:\\n{}\".format(grid_search_rbf.best_estimator_))\n",
    "\n",
    "train_score = grid_search_rbf.score(X_train,y_train)\n",
    "test_score = grid_search_rbf.score(X_test,y_test)\n",
    "print(train_score, test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': 8, 'max_features': 3}\n",
      "Best cross-validation score: 0.967102\n",
      "Best estimator:\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=8, max_features=3,\n",
      "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "           min_impurity_split=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           presort=False, random_state=None, splitter='best')\n",
      "0.971585036829 0.972305352988\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor \n",
    "dtr= DecisionTreeRegressor()\n",
    "param_dtr = {'max_features':[1,3,5,6],'max_depth':[2,4,6,8,10]}\n",
    "\n",
    "grid_search_dtr = GridSearchCV(dtr,param_dtr, cv=5) # each parameter has a cv of 10 folds. Try max_iter if possible.\n",
    "grid_search_dtr.fit(X_train, y_train)\n",
    "print(\"Best parameters: {}\".format(grid_search_dtr.best_params_)) # The pairt hta has highest score\n",
    "print(\"Best cross-validation score: {:.6f}\".format(grid_search_dtr.best_score_)) \n",
    "print(\"Best estimator:\\n{}\".format(grid_search_dtr.best_estimator_))\n",
    "\n",
    "train_score = grid_search_dtr.score(X_train,y_train)\n",
    "test_score = grid_search_dtr.score(X_test,y_test)\n",
    "print(train_score, test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': 8, 'max_features': 3}\n",
      "Best cross-validation score: 0.967283\n",
      "Best estimator:\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=8,\n",
      "           max_features=3, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "           min_impurity_split=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "           verbose=0, warm_start=False)\n",
      "0.971344326368 0.972854915983\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor \n",
    "param_RF = {'max_features':[1,3,5,6],'max_depth':[2,4,6,8,10]}\n",
    "\n",
    "grid_search_RF = GridSearchCV(RandomForestRegressor(),param_RF, cv=5) # each parameter has a cv of 10 folds. Try max_iter if possible.\n",
    "grid_search_RF.fit(X_train, y_train)\n",
    "print(\"Best parameters: {}\".format(grid_search_RF.best_params_)) # The pairt hta has highest score\n",
    "print(\"Best cross-validation score: {:.6f}\".format(grid_search_RF.best_score_)) \n",
    "print(\"Best estimator:\\n{}\".format(grid_search_RF.best_estimator_))\n",
    "\n",
    "train_score = grid_search_RF.score(X_train,y_train)\n",
    "test_score = grid_search_RF.score(X_test,y_test)\n",
    "print(train_score, test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bagging Ensemble - Cooling Load\n",
    " - By order: KNN, Linear Regression, Ridge, Lasso, Decision Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.939986\n",
      "Out of bag score: 0.937084\n"
     ]
    }
   ],
   "source": [
    "bag_knn= BaggingRegressor(KNeighborsRegressor(n_neighbors=7), \n",
    "                           n_estimators=500, max_samples=100, bootstrap=True, n_jobs=-1, random_state=10,oob_score=True)\n",
    "bag_knn.fit(X2, y2)\n",
    "print('Train score: {0:0.6f}'.format(bag_knn.score(X2, y2)))\n",
    "print('Out of bag score: {0:0.6f}'.format(bag_knn.oob_score_)) # Best one so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.887526\n",
      "Out of bag score: 0.885923\n"
     ]
    }
   ],
   "source": [
    "bag_linreg= BaggingRegressor(LinearRegression(), \n",
    "                           n_estimators=500, max_samples=100, bootstrap=True, n_jobs=-1, random_state=10,oob_score=True)\n",
    "bag_linreg.fit(X2, y2)\n",
    "print('Train score: {0:0.6f}'.format(bag_linreg.score(X2, y2)))\n",
    "print('Out of bag score: {0:0.6f}'.format(bag_linreg.oob_score_)) # Best one so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.887512\n",
      "Out of bag score: 0.885913\n"
     ]
    }
   ],
   "source": [
    "bag_ridge= BaggingRegressor(Ridge(alpha=0, random_state=10), \n",
    "                           n_estimators=500, max_samples=100, bootstrap=True, n_jobs=-1, random_state=10,oob_score=True)\n",
    "bag_ridge.fit(X2, y2)\n",
    "print('Train score: {0:0.6f}'.format(bag_ridge.score(X2, y2)))\n",
    "print('Out of bag score: {0:0.6f}'.format(bag_ridge.oob_score_)) # Best one so far\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.820167\n",
      "Out of bag score: 0.818165\n"
     ]
    }
   ],
   "source": [
    "bag_lasso= BaggingRegressor(Lasso(alpha=0.5, random_state=10), \n",
    "                           n_estimators=500, max_samples=100, bootstrap=True, n_jobs=-1, random_state=10,oob_score=True)\n",
    "bag_lasso.fit(X2, y2)\n",
    "print('Train score: {0:0.6f}'.format(bag_lasso.score(X2, y2)))\n",
    "print('Out of bag score: {0:0.6f}'.format(bag_lasso.oob_score_)) # Best one so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.970182\n",
      "Out of bag score: 0.966673\n"
     ]
    }
   ],
   "source": [
    "bag_dtr = BaggingRegressor(DecisionTreeRegressor(max_depth=8,max_features=3), \n",
    "                            n_estimators=500, max_samples=100, bootstrap=True, n_jobs=-1, random_state=10,oob_score=True)\n",
    "bag_dtr.fit(X2, y2)\n",
    "print('Train score: {0:0.6f}'.format(bag_dtr.score(X2, y2)))\n",
    "print('Out of bag score: {0:0.6f}'.format(bag_dtr.oob_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Boosting Ensemble( Adaboost) - Cooling Load\n",
    " - By order: KNN, Linear Regression, Ridge, Lasso, Decision Tree, Gradient Boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.969415\n",
      "Test score: 0.970514\n"
     ]
    }
   ],
   "source": [
    "ada_knn= AdaBoostRegressor(KNeighborsRegressor(n_neighbors=7),n_estimators=500,\n",
    "                             learning_rate=0.5, random_state=10)\n",
    "ada_knn.fit(X_train, y_train)\n",
    "print('Train score: {0:0.6f}'.format(ada_knn.score(X_train,y_train)))\n",
    "print('Test score: {0:0.6f}'.format(ada_knn.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.876652\n",
      "Test score: 0.902982\n"
     ]
    }
   ],
   "source": [
    "ada_linreg= AdaBoostRegressor(LinearRegression(),n_estimators=500,\n",
    "                            learning_rate=0.5, random_state=10)\n",
    "ada_linreg.fit(X_train, y_train)\n",
    "print('Train score: {0:0.6f}'.format(ada_linreg.score(X_train,y_train)))\n",
    "print('Test score: {0:0.6f}'.format(ada_linreg.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.910119\n",
      "Test score: 0.925539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo E560\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 2.6303009397471052e-17\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "C:\\Users\\Lenovo E560\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 1.0904488494697053e-16\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "C:\\Users\\Lenovo E560\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 4.80705512813155e-17\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "C:\\Users\\Lenovo E560\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 4.1635782745159276e-17\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "C:\\Users\\Lenovo E560\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 6.858974896171097e-17\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "C:\\Users\\Lenovo E560\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 7.53043795369843e-17\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "C:\\Users\\Lenovo E560\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 8.322192046295244e-17\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "C:\\Users\\Lenovo E560\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 9.264132433885823e-18\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "C:\\Users\\Lenovo E560\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 2.817853424524017e-17\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "C:\\Users\\Lenovo E560\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 5.645820891514877e-17\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "C:\\Users\\Lenovo E560\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 3.057040332384364e-17\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "C:\\Users\\Lenovo E560\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 6.680513853329436e-17\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "C:\\Users\\Lenovo E560\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 6.234445799752796e-17\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "ada_ridge= AdaBoostRegressor(Ridge(alpha=0, random_state=10), \n",
    "                            n_estimators=500, learning_rate=0.5, random_state=10)\n",
    "ada_ridge.fit(X_train, y_train)\n",
    "print('Train score: {0:0.6f}'.format(ada_ridge.score(X_train,y_train)))\n",
    "print('Test score: {0:0.6f}'.format(ada_ridge.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.831529\n",
      "Test score: 0.861342\n"
     ]
    }
   ],
   "source": [
    "ada_lasso= AdaBoostRegressor(Lasso(alpha=0.5, random_state=10), \n",
    "                             n_estimators=500, learning_rate=0.5, random_state=10)\n",
    "ada_lasso.fit(X_train, y_train)\n",
    "print('Train score: {0:0.6f}'.format(ada_lasso.score(X_train,y_train)))\n",
    "print('Test score: {0:0.6f}'.format(ada_lasso.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.971471\n",
      "Test score: 0.972121\n"
     ]
    }
   ],
   "source": [
    "ada_dtr = AdaBoostRegressor(DecisionTreeRegressor(max_depth=8,max_features=3, random_state=10), \n",
    "                            n_estimators=500, learning_rate=0.5, random_state=10)\n",
    "ada_dtr.fit(X_train, y_train)\n",
    "print('Train score: {0:0.6f}'.format(ada_dtr.score(X_train,y_train)))\n",
    "print('Test score: {0:0.6f}'.format(ada_dtr.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.968933\n",
      "Test score: 0.971593\n"
     ]
    }
   ],
   "source": [
    "gbr = GradientBoostingRegressor(learning_rate = 0.05)\n",
    "gbr.fit(X_train,y_train)\n",
    "print('Train score: {0:0.6f}'.format(gbr.score(X_train,y_train)))\n",
    "print('Test score: {0:0.6f}'.format(gbr.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Classification - Deep Learning\n",
    "- In this section, heating and cooling load are combined and transformed to a single target variable. Dummy transformation is applied for multi-classification in three labels: low, average, and high( three dummy columns accordingly). Feature columns are regressed against this combined target to drop insignificant variables X6 and X8.\n",
    "\n",
    "- GridSearchCV then run Keras Classifier to find the best parameters ( batch 5, epoch 200). Probabilities for each dummy columns( in y_dummy) are generated by prediction. The highest probabilities for each instance will pick the groups accordingly. The results are then labeled and visualized in a frequency chart.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense \n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "np.random.seed(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Processing and Feature Selection - Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = y1+ y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    768.000000\n",
       "mean      46.894961\n",
       "std       19.484947\n",
       "min       16.950000\n",
       "25%       28.750000\n",
       "50%       40.970000\n",
       "75%       64.335000\n",
       "max       89.950000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.912\n",
      "Model:                            OLS   Adj. R-squared:                  0.911\n",
      "Method:                 Least Squares   F-statistic:                     1123.\n",
      "Date:                Wed, 25 Apr 2018   Prob (F-statistic):               0.00\n",
      "Time:                        11:09:49   Log-Likelihood:                -2437.4\n",
      "No. Observations:                 768   AIC:                             4891.\n",
      "Df Residuals:                     760   BIC:                             4928.\n",
      "Df Model:                           7                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const        181.2603     37.701      4.808      0.000     107.249     255.271\n",
      "X1          -135.5617     20.381     -6.651      0.000    -175.572     -95.552\n",
      "X2            -0.1287      0.027     -4.846      0.000      -0.181      -0.077\n",
      "X3             0.0587      0.008      7.693      0.000       0.044       0.074\n",
      "X4            -0.0937      0.015     -6.293      0.000      -0.123      -0.064\n",
      "X5             8.4538      0.669     12.627      0.000       7.140       9.768\n",
      "X6             0.0982      0.188      0.523      0.601      -0.270       0.466\n",
      "X7            34.6497      1.612     21.491      0.000      31.485      37.815\n",
      "X8             0.2445      0.138      1.765      0.078      -0.027       0.516\n",
      "==============================================================================\n",
      "Omnibus:                       31.132   Durbin-Watson:                   0.791\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               54.987\n",
      "Skew:                           0.291   Prob(JB):                     1.15e-12\n",
      "Kurtosis:                       4.174   Cond. No.                     2.85e+16\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 5.61e-25. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "est = sm.OLS(y, Xtemp)\n",
    "est = est.fit()\n",
    "print(est.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_dummy = pd.cut(y, 3, labels=[\"L\",\"A\",\"H\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    L\n",
       "1    L\n",
       "2    L\n",
       "3    L\n",
       "4    A\n",
       "dtype: category\n",
       "Categories (3, object): [L < A < H]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_dummy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Efficiency_L</th>\n",
       "      <th>Efficiency_A</th>\n",
       "      <th>Efficiency_H</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Efficiency_L  Efficiency_A  Efficiency_H\n",
       "0             1             0             0\n",
       "1             1             0             0\n",
       "2             1             0             0\n",
       "3             1             0             0\n",
       "4             0             1             0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb=pd.get_dummies(y_dummy,prefix='Efficiency')\n",
    "emb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Efficiency_L</th>\n",
       "      <th>Efficiency_A</th>\n",
       "      <th>Efficiency_H</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Efficiency_L  Efficiency_A  Efficiency_H\n",
       "0             1             0             0\n",
       "1             1             0             0\n",
       "2             1             0             0\n",
       "3             1             0             0\n",
       "4             0             1             0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_dummy=pd.concat([y_dummy, emb], axis=1)\n",
    "y_dummy.drop(y_dummy.iloc[:,0:1],axis=1,inplace= True)\n",
    "y_dummy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "X_train, X_test, y_train, y_test = train_test_split(X2,y_dummy, random_state = 10)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "def create_model():\n",
    "    #create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(6, input_dim=6, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu')) #hidden layer\n",
    "    model.add(Dense(3,activation='sigmoid')) #output layer\n",
    "    #compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = {'epochs':[100,200] , 'batch_size':[5,20]}\n",
    "keras_classifier = KerasClassifier(build_fn = create_model , verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=<keras.wrappers.scikit_learn.KerasClassifier object at 0x000002472D4C7F98>,\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'epochs': [100, 200], 'batch_size': [5, 20]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "grid_search = GridSearchCV(keras_classifier , param_grid , cv=5 )\n",
    "grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'batch_size': 5, 'epochs': 200}\n",
      "Best cross-validation score: 0.953704\n",
      "Best estimator:\n",
      "<keras.wrappers.scikit_learn.KerasClassifier object at 0x000002474DD20D30>\n",
      "0.928819452826 0.928819448687\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters: {}\".format(grid_search.best_params_)) \n",
    "print(\"Best cross-validation score: {:.6f}\".format(grid_search.best_score_)) \n",
    "print(\"Best estimator:\\n{}\".format(grid_search.best_estimator_))\n",
    "\n",
    "train_score = grid_search.score(X_train,y_train)\n",
    "test_score = grid_search.score(X_test,y_test)\n",
    "print(train_score, test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prediction of Overall Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Sequential() \n",
    "model.add(Dense(6, input_dim=6, activation='relu')) \n",
    "model.add(Dense(8, activation='relu'))             \n",
    "model.add(Dense(3, activation='sigmoid'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile model \n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "576/576 [==============================] - 4s 8ms/step - loss: 0.6603 - acc: 0.7072\n",
      "Epoch 2/200\n",
      "576/576 [==============================] - 1s 895us/step - loss: 0.5432 - acc: 0.8241\n",
      "Epoch 3/200\n",
      "576/576 [==============================] - 0s 728us/step - loss: 0.4051 - acc: 0.8258\n",
      "Epoch 4/200\n",
      "576/576 [==============================] - 0s 625us/step - loss: 0.3286 - acc: 0.8270\n",
      "Epoch 5/200\n",
      "576/576 [==============================] - 0s 648us/step - loss: 0.2971 - acc: 0.8663\n",
      "Epoch 6/200\n",
      "576/576 [==============================] - 0s 677us/step - loss: 0.2828 - acc: 0.8675\n",
      "Epoch 7/200\n",
      "576/576 [==============================] - 0s 649us/step - loss: 0.2757 - acc: 0.8704\n",
      "Epoch 8/200\n",
      "576/576 [==============================] - 0s 695us/step - loss: 0.2711 - acc: 0.8791\n",
      "Epoch 9/200\n",
      "576/576 [==============================] - 0s 677us/step - loss: 0.2679 - acc: 0.8709\n",
      "Epoch 10/200\n",
      "576/576 [==============================] - 0s 670us/step - loss: 0.2648 - acc: 0.8738\n",
      "Epoch 11/200\n",
      "576/576 [==============================] - 0s 695us/step - loss: 0.2618 - acc: 0.8756\n",
      "Epoch 12/200\n",
      "576/576 [==============================] - 0s 691us/step - loss: 0.2585 - acc: 0.8669\n",
      "Epoch 13/200\n",
      "576/576 [==============================] - 0s 688us/step - loss: 0.2548 - acc: 0.8767\n",
      "Epoch 14/200\n",
      "576/576 [==============================] - 0s 660us/step - loss: 0.2515 - acc: 0.8987\n",
      "Epoch 15/200\n",
      "576/576 [==============================] - 0s 696us/step - loss: 0.2478 - acc: 0.9016\n",
      "Epoch 16/200\n",
      "576/576 [==============================] - 0s 693us/step - loss: 0.2441 - acc: 0.9080\n",
      "Epoch 17/200\n",
      "576/576 [==============================] - 0s 679us/step - loss: 0.2399 - acc: 0.9172\n",
      "Epoch 18/200\n",
      "576/576 [==============================] - 0s 667us/step - loss: 0.2373 - acc: 0.9028\n",
      "Epoch 19/200\n",
      "576/576 [==============================] - 0s 747us/step - loss: 0.2321 - acc: 0.9097\n",
      "Epoch 20/200\n",
      "576/576 [==============================] - 0s 679us/step - loss: 0.2281 - acc: 0.9080\n",
      "Epoch 21/200\n",
      "576/576 [==============================] - 0s 703us/step - loss: 0.2254 - acc: 0.9051\n",
      "Epoch 22/200\n",
      "576/576 [==============================] - 0s 681us/step - loss: 0.2202 - acc: 0.9097\n",
      "Epoch 23/200\n",
      "576/576 [==============================] - 0s 681us/step - loss: 0.2157 - acc: 0.9144\n",
      "Epoch 24/200\n",
      "576/576 [==============================] - 0s 693us/step - loss: 0.2119 - acc: 0.9051\n",
      "Epoch 25/200\n",
      "576/576 [==============================] - 0s 693us/step - loss: 0.2073 - acc: 0.9132\n",
      "Epoch 26/200\n",
      "576/576 [==============================] - 0s 707us/step - loss: 0.2041 - acc: 0.9161\n",
      "Epoch 27/200\n",
      "576/576 [==============================] - 0s 700us/step - loss: 0.2011 - acc: 0.9132\n",
      "Epoch 28/200\n",
      "576/576 [==============================] - 0s 665us/step - loss: 0.1992 - acc: 0.9126\n",
      "Epoch 29/200\n",
      "576/576 [==============================] - 0s 689us/step - loss: 0.1968 - acc: 0.9120\n",
      "Epoch 30/200\n",
      "576/576 [==============================] - 0s 702us/step - loss: 0.1940 - acc: 0.9138\n",
      "Epoch 31/200\n",
      "576/576 [==============================] - 0s 696us/step - loss: 0.1915 - acc: 0.9172\n",
      "Epoch 32/200\n",
      "576/576 [==============================] - 0s 703us/step - loss: 0.1902 - acc: 0.9190\n",
      "Epoch 33/200\n",
      "576/576 [==============================] - 0s 695us/step - loss: 0.1888 - acc: 0.9184\n",
      "Epoch 34/200\n",
      "576/576 [==============================] - 0s 686us/step - loss: 0.1875 - acc: 0.9149\n",
      "Epoch 35/200\n",
      "576/576 [==============================] - 0s 689us/step - loss: 0.1847 - acc: 0.9190\n",
      "Epoch 36/200\n",
      "576/576 [==============================] - 0s 696us/step - loss: 0.1835 - acc: 0.9178\n",
      "Epoch 37/200\n",
      "576/576 [==============================] - 0s 681us/step - loss: 0.1823 - acc: 0.9230\n",
      "Epoch 38/200\n",
      "576/576 [==============================] - 0s 648us/step - loss: 0.1810 - acc: 0.9259\n",
      "Epoch 39/200\n",
      "576/576 [==============================] - 0s 644us/step - loss: 0.1799 - acc: 0.9190\n",
      "Epoch 40/200\n",
      "576/576 [==============================] - 0s 665us/step - loss: 0.1784 - acc: 0.9201\n",
      "Epoch 41/200\n",
      "576/576 [==============================] - 0s 757us/step - loss: 0.1771 - acc: 0.9230\n",
      "Epoch 42/200\n",
      "576/576 [==============================] - 0s 787us/step - loss: 0.1772 - acc: 0.9230\n",
      "Epoch 43/200\n",
      "576/576 [==============================] - 0s 752us/step - loss: 0.1752 - acc: 0.9265\n",
      "Epoch 44/200\n",
      "576/576 [==============================] - 0s 661us/step - loss: 0.1743 - acc: 0.9271\n",
      "Epoch 45/200\n",
      "576/576 [==============================] - 0s 656us/step - loss: 0.1730 - acc: 0.9242\n",
      "Epoch 46/200\n",
      "576/576 [==============================] - 0s 651us/step - loss: 0.1725 - acc: 0.9265\n",
      "Epoch 47/200\n",
      "576/576 [==============================] - 0s 667us/step - loss: 0.1720 - acc: 0.9230\n",
      "Epoch 48/200\n",
      "576/576 [==============================] - 0s 665us/step - loss: 0.1704 - acc: 0.9300\n",
      "Epoch 49/200\n",
      "576/576 [==============================] - 0s 674us/step - loss: 0.1697 - acc: 0.9311\n",
      "Epoch 50/200\n",
      "576/576 [==============================] - 0s 653us/step - loss: 0.1700 - acc: 0.9265\n",
      "Epoch 51/200\n",
      "576/576 [==============================] - 0s 660us/step - loss: 0.1683 - acc: 0.9248\n",
      "Epoch 52/200\n",
      "576/576 [==============================] - 0s 656us/step - loss: 0.1676 - acc: 0.9253\n",
      "Epoch 53/200\n",
      "576/576 [==============================] - 0s 705us/step - loss: 0.1673 - acc: 0.9277\n",
      "Epoch 54/200\n",
      "576/576 [==============================] - 0s 696us/step - loss: 0.1663 - acc: 0.9271\n",
      "Epoch 55/200\n",
      "576/576 [==============================] - 0s 707us/step - loss: 0.1659 - acc: 0.9294\n",
      "Epoch 56/200\n",
      "576/576 [==============================] - 0s 698us/step - loss: 0.1645 - acc: 0.9282\n",
      "Epoch 57/200\n",
      "576/576 [==============================] - 0s 740us/step - loss: 0.1631 - acc: 0.9253\n",
      "Epoch 58/200\n",
      "576/576 [==============================] - 0s 712us/step - loss: 0.1624 - acc: 0.9282\n",
      "Epoch 59/200\n",
      "576/576 [==============================] - 0s 775us/step - loss: 0.1616 - acc: 0.9282\n",
      "Epoch 60/200\n",
      "576/576 [==============================] - 0s 714us/step - loss: 0.1604 - acc: 0.9323\n",
      "Epoch 61/200\n",
      "576/576 [==============================] - 0s 674us/step - loss: 0.1590 - acc: 0.9306\n",
      "Epoch 62/200\n",
      "576/576 [==============================] - 0s 708us/step - loss: 0.1591 - acc: 0.9294\n",
      "Epoch 63/200\n",
      "576/576 [==============================] - 0s 644us/step - loss: 0.1583 - acc: 0.9277\n",
      "Epoch 64/200\n",
      "576/576 [==============================] - 0s 648us/step - loss: 0.1573 - acc: 0.9294\n",
      "Epoch 65/200\n",
      "576/576 [==============================] - 0s 655us/step - loss: 0.1559 - acc: 0.9300\n",
      "Epoch 66/200\n",
      "576/576 [==============================] - 0s 651us/step - loss: 0.1551 - acc: 0.9306\n",
      "Epoch 67/200\n",
      "576/576 [==============================] - 0s 742us/step - loss: 0.1543 - acc: 0.9311\n",
      "Epoch 68/200\n",
      "576/576 [==============================] - 0s 689us/step - loss: 0.1532 - acc: 0.9346\n",
      "Epoch 69/200\n",
      "576/576 [==============================] - 0s 668us/step - loss: 0.1525 - acc: 0.9323\n",
      "Epoch 70/200\n",
      "576/576 [==============================] - 0s 695us/step - loss: 0.1528 - acc: 0.9282\n",
      "Epoch 71/200\n",
      "576/576 [==============================] - 0s 710us/step - loss: 0.1519 - acc: 0.9358\n",
      "Epoch 72/200\n",
      "576/576 [==============================] - 0s 595us/step - loss: 0.1498 - acc: 0.9392\n",
      "Epoch 73/200\n",
      "576/576 [==============================] - 0s 583us/step - loss: 0.1498 - acc: 0.9363\n",
      "Epoch 74/200\n",
      "576/576 [==============================] - 0s 604us/step - loss: 0.1492 - acc: 0.9392\n",
      "Epoch 75/200\n",
      "576/576 [==============================] - 0s 597us/step - loss: 0.1485 - acc: 0.9410\n",
      "Epoch 76/200\n",
      "576/576 [==============================] - 0s 602us/step - loss: 0.1472 - acc: 0.9369\n",
      "Epoch 77/200\n",
      "576/576 [==============================] - 0s 609us/step - loss: 0.1465 - acc: 0.9416\n",
      "Epoch 78/200\n",
      "576/576 [==============================] - 0s 688us/step - loss: 0.1454 - acc: 0.9421\n",
      "Epoch 79/200\n",
      "576/576 [==============================] - 0s 679us/step - loss: 0.1454 - acc: 0.9421\n",
      "Epoch 80/200\n",
      "576/576 [==============================] - 0s 639us/step - loss: 0.1439 - acc: 0.9433\n",
      "Epoch 81/200\n",
      "576/576 [==============================] - 0s 677us/step - loss: 0.1430 - acc: 0.9433\n",
      "Epoch 82/200\n",
      "576/576 [==============================] - 0s 806us/step - loss: 0.1409 - acc: 0.9462\n",
      "Epoch 83/200\n",
      "576/576 [==============================] - 0s 796us/step - loss: 0.1402 - acc: 0.9497\n",
      "Epoch 84/200\n",
      "576/576 [==============================] - 0s 695us/step - loss: 0.1392 - acc: 0.9473\n",
      "Epoch 85/200\n",
      "576/576 [==============================] - 0s 625us/step - loss: 0.1391 - acc: 0.9462\n",
      "Epoch 86/200\n",
      "576/576 [==============================] - 0s 689us/step - loss: 0.1376 - acc: 0.9520\n",
      "Epoch 87/200\n",
      "576/576 [==============================] - 0s 613us/step - loss: 0.1363 - acc: 0.9508\n",
      "Epoch 88/200\n",
      "576/576 [==============================] - 0s 588us/step - loss: 0.1359 - acc: 0.9456\n",
      "Epoch 89/200\n",
      "576/576 [==============================] - 0s 714us/step - loss: 0.1351 - acc: 0.9520\n",
      "Epoch 90/200\n",
      "576/576 [==============================] - 0s 608us/step - loss: 0.1347 - acc: 0.9514\n",
      "Epoch 91/200\n",
      "576/576 [==============================] - 0s 621us/step - loss: 0.1339 - acc: 0.9497\n",
      "Epoch 92/200\n",
      "576/576 [==============================] - 0s 700us/step - loss: 0.1344 - acc: 0.9531\n",
      "Epoch 93/200\n",
      "576/576 [==============================] - 0s 580us/step - loss: 0.1331 - acc: 0.9491\n",
      "Epoch 94/200\n",
      "576/576 [==============================] - 0s 567us/step - loss: 0.1329 - acc: 0.9520\n",
      "Epoch 95/200\n",
      "576/576 [==============================] - 0s 581us/step - loss: 0.1313 - acc: 0.9537\n",
      "Epoch 96/200\n",
      "576/576 [==============================] - 0s 580us/step - loss: 0.1312 - acc: 0.9531\n",
      "Epoch 97/200\n",
      "576/576 [==============================] - 0s 587us/step - loss: 0.1312 - acc: 0.9497\n",
      "Epoch 98/200\n",
      "576/576 [==============================] - 0s 562us/step - loss: 0.1304 - acc: 0.9514\n",
      "Epoch 99/200\n",
      "576/576 [==============================] - 0s 601us/step - loss: 0.1297 - acc: 0.9525\n",
      "Epoch 100/200\n",
      "576/576 [==============================] - 0s 590us/step - loss: 0.1288 - acc: 0.9537\n",
      "Epoch 101/200\n",
      "576/576 [==============================] - 0s 594us/step - loss: 0.1299 - acc: 0.9531\n",
      "Epoch 102/200\n",
      "576/576 [==============================] - 0s 660us/step - loss: 0.1290 - acc: 0.9537\n",
      "Epoch 103/200\n",
      "576/576 [==============================] - 0s 719us/step - loss: 0.1283 - acc: 0.9537\n",
      "Epoch 104/200\n",
      "576/576 [==============================] - 0s 653us/step - loss: 0.1273 - acc: 0.9537\n",
      "Epoch 105/200\n",
      "576/576 [==============================] - 0s 597us/step - loss: 0.1269 - acc: 0.9537\n",
      "Epoch 106/200\n",
      "576/576 [==============================] - 0s 642us/step - loss: 0.1269 - acc: 0.9537\n",
      "Epoch 107/200\n",
      "576/576 [==============================] - 0s 597us/step - loss: 0.1256 - acc: 0.9537\n",
      "Epoch 108/200\n",
      "576/576 [==============================] - 0s 611us/step - loss: 0.1262 - acc: 0.9537\n",
      "Epoch 109/200\n",
      "576/576 [==============================] - 0s 616us/step - loss: 0.1252 - acc: 0.9560\n",
      "Epoch 110/200\n",
      "576/576 [==============================] - 0s 698us/step - loss: 0.1268 - acc: 0.9473\n",
      "Epoch 111/200\n",
      "576/576 [==============================] - 0s 623us/step - loss: 0.1255 - acc: 0.9531\n",
      "Epoch 112/200\n",
      "576/576 [==============================] - 0s 702us/step - loss: 0.1241 - acc: 0.9537\n",
      "Epoch 113/200\n",
      "576/576 [==============================] - 0s 642us/step - loss: 0.1231 - acc: 0.9537\n",
      "Epoch 114/200\n",
      "576/576 [==============================] - 0s 646us/step - loss: 0.1233 - acc: 0.9537\n",
      "Epoch 115/200\n",
      "576/576 [==============================] - 0s 625us/step - loss: 0.1220 - acc: 0.9549\n",
      "Epoch 116/200\n",
      "576/576 [==============================] - 0s 616us/step - loss: 0.1212 - acc: 0.9566\n",
      "Epoch 117/200\n",
      "576/576 [==============================] - 0s 567us/step - loss: 0.1215 - acc: 0.9543\n",
      "Epoch 118/200\n",
      "576/576 [==============================] - 0s 625us/step - loss: 0.1213 - acc: 0.9595\n",
      "Epoch 119/200\n",
      "576/576 [==============================] - 0s 637us/step - loss: 0.1241 - acc: 0.9508\n",
      "Epoch 120/200\n",
      "576/576 [==============================] - 0s 715us/step - loss: 0.1201 - acc: 0.9537\n",
      "Epoch 121/200\n",
      "576/576 [==============================] - 0s 705us/step - loss: 0.1201 - acc: 0.9531\n",
      "Epoch 122/200\n",
      "576/576 [==============================] - 0s 620us/step - loss: 0.1200 - acc: 0.9589\n",
      "Epoch 123/200\n",
      "576/576 [==============================] - 0s 611us/step - loss: 0.1195 - acc: 0.9525\n",
      "Epoch 124/200\n",
      "576/576 [==============================] - 0s 762us/step - loss: 0.1196 - acc: 0.9566\n",
      "Epoch 125/200\n",
      "576/576 [==============================] - 0s 867us/step - loss: 0.1183 - acc: 0.9537\n",
      "Epoch 126/200\n",
      "576/576 [==============================] - 1s 898us/step - loss: 0.1191 - acc: 0.9578\n",
      "Epoch 127/200\n",
      "576/576 [==============================] - 0s 836us/step - loss: 0.1177 - acc: 0.9589\n",
      "Epoch 128/200\n",
      "576/576 [==============================] - 0s 797us/step - loss: 0.1170 - acc: 0.9630\n",
      "Epoch 129/200\n",
      "576/576 [==============================] - 0s 802us/step - loss: 0.1168 - acc: 0.9531\n",
      "Epoch 130/200\n",
      "576/576 [==============================] - 0s 808us/step - loss: 0.1175 - acc: 0.9554\n",
      "Epoch 131/200\n",
      "576/576 [==============================] - 0s 789us/step - loss: 0.1167 - acc: 0.9601\n",
      "Epoch 132/200\n",
      "576/576 [==============================] - 0s 773us/step - loss: 0.1167 - acc: 0.9595\n",
      "Epoch 133/200\n",
      "576/576 [==============================] - 0s 698us/step - loss: 0.1163 - acc: 0.9549\n",
      "Epoch 134/200\n",
      "576/576 [==============================] - 0s 745us/step - loss: 0.1159 - acc: 0.9589\n",
      "Epoch 135/200\n",
      "576/576 [==============================] - 0s 780us/step - loss: 0.1153 - acc: 0.9624\n",
      "Epoch 136/200\n",
      "576/576 [==============================] - 0s 749us/step - loss: 0.1149 - acc: 0.9566\n",
      "Epoch 137/200\n",
      "576/576 [==============================] - 0s 698us/step - loss: 0.1158 - acc: 0.9572\n",
      "Epoch 138/200\n",
      "576/576 [==============================] - 0s 688us/step - loss: 0.1158 - acc: 0.9560\n",
      "Epoch 139/200\n",
      "576/576 [==============================] - 0s 755us/step - loss: 0.1150 - acc: 0.9595\n",
      "Epoch 140/200\n",
      "576/576 [==============================] - 0s 684us/step - loss: 0.1144 - acc: 0.9572\n",
      "Epoch 141/200\n",
      "576/576 [==============================] - 0s 729us/step - loss: 0.1141 - acc: 0.9595\n",
      "Epoch 142/200\n",
      "576/576 [==============================] - 0s 675us/step - loss: 0.1145 - acc: 0.9583\n",
      "Epoch 143/200\n",
      "576/576 [==============================] - 0s 679us/step - loss: 0.1146 - acc: 0.9612\n",
      "Epoch 144/200\n",
      "576/576 [==============================] - 0s 759us/step - loss: 0.1146 - acc: 0.9601\n",
      "Epoch 145/200\n",
      "576/576 [==============================] - 0s 757us/step - loss: 0.1127 - acc: 0.9578\n",
      "Epoch 146/200\n",
      "576/576 [==============================] - 0s 679us/step - loss: 0.1132 - acc: 0.9583\n",
      "Epoch 147/200\n",
      "576/576 [==============================] - 0s 724us/step - loss: 0.1127 - acc: 0.9612\n",
      "Epoch 148/200\n",
      "576/576 [==============================] - 0s 712us/step - loss: 0.1128 - acc: 0.9618\n",
      "Epoch 149/200\n",
      "576/576 [==============================] - 0s 735us/step - loss: 0.1139 - acc: 0.9578\n",
      "Epoch 150/200\n",
      "576/576 [==============================] - 0s 726us/step - loss: 0.1122 - acc: 0.9612\n",
      "Epoch 151/200\n",
      "576/576 [==============================] - 0s 670us/step - loss: 0.1118 - acc: 0.9635\n",
      "Epoch 152/200\n",
      "576/576 [==============================] - 0s 715us/step - loss: 0.1121 - acc: 0.9589\n",
      "Epoch 153/200\n",
      "576/576 [==============================] - 0s 703us/step - loss: 0.1119 - acc: 0.9589\n",
      "Epoch 154/200\n",
      "576/576 [==============================] - 0s 731us/step - loss: 0.1125 - acc: 0.9601 0s - loss: 0.1103 - acc: 0.960\n",
      "Epoch 155/200\n",
      "576/576 [==============================] - 0s 710us/step - loss: 0.1110 - acc: 0.9612\n",
      "Epoch 156/200\n",
      "576/576 [==============================] - 0s 729us/step - loss: 0.1111 - acc: 0.9612\n",
      "Epoch 157/200\n",
      "576/576 [==============================] - 0s 682us/step - loss: 0.1112 - acc: 0.9612\n",
      "Epoch 158/200\n",
      "576/576 [==============================] - 0s 778us/step - loss: 0.1109 - acc: 0.9641\n",
      "Epoch 159/200\n",
      "576/576 [==============================] - 0s 802us/step - loss: 0.1102 - acc: 0.9624\n",
      "Epoch 160/200\n",
      "576/576 [==============================] - 0s 827us/step - loss: 0.1104 - acc: 0.9606\n",
      "Epoch 161/200\n",
      "576/576 [==============================] - 0s 705us/step - loss: 0.1102 - acc: 0.9624\n",
      "Epoch 162/200\n",
      "576/576 [==============================] - 0s 834us/step - loss: 0.1100 - acc: 0.9606\n",
      "Epoch 163/200\n",
      "576/576 [==============================] - 0s 849us/step - loss: 0.1112 - acc: 0.9635\n",
      "Epoch 164/200\n",
      "576/576 [==============================] - 1s 917us/step - loss: 0.1112 - acc: 0.9612\n",
      "Epoch 165/200\n",
      "576/576 [==============================] - 0s 785us/step - loss: 0.1100 - acc: 0.9647\n",
      "Epoch 166/200\n",
      "576/576 [==============================] - 0s 602us/step - loss: 0.1102 - acc: 0.9606\n",
      "Epoch 167/200\n",
      "576/576 [==============================] - 0s 689us/step - loss: 0.1095 - acc: 0.9612\n",
      "Epoch 168/200\n",
      "576/576 [==============================] - 0s 658us/step - loss: 0.1103 - acc: 0.9635\n",
      "Epoch 169/200\n",
      "576/576 [==============================] - 0s 768us/step - loss: 0.1088 - acc: 0.9618\n",
      "Epoch 170/200\n",
      "576/576 [==============================] - 0s 752us/step - loss: 0.1089 - acc: 0.9612\n",
      "Epoch 171/200\n",
      "576/576 [==============================] - 0s 775us/step - loss: 0.1094 - acc: 0.9624\n",
      "Epoch 172/200\n",
      "576/576 [==============================] - 0s 822us/step - loss: 0.1079 - acc: 0.9647\n",
      "Epoch 173/200\n",
      "576/576 [==============================] - 0s 790us/step - loss: 0.1086 - acc: 0.9635\n",
      "Epoch 174/200\n",
      "576/576 [==============================] - 0s 787us/step - loss: 0.1078 - acc: 0.9647\n",
      "Epoch 175/200\n",
      "576/576 [==============================] - 0s 698us/step - loss: 0.1078 - acc: 0.9624\n",
      "Epoch 176/200\n",
      "576/576 [==============================] - 0s 736us/step - loss: 0.1072 - acc: 0.9624\n",
      "Epoch 177/200\n",
      "576/576 [==============================] - 0s 670us/step - loss: 0.1076 - acc: 0.9653\n",
      "Epoch 178/200\n",
      "576/576 [==============================] - 0s 649us/step - loss: 0.1069 - acc: 0.9641\n",
      "Epoch 179/200\n",
      "576/576 [==============================] - 0s 677us/step - loss: 0.1076 - acc: 0.9630\n",
      "Epoch 180/200\n",
      "576/576 [==============================] - 0s 766us/step - loss: 0.1067 - acc: 0.9641\n",
      "Epoch 181/200\n",
      "576/576 [==============================] - 0s 695us/step - loss: 0.1068 - acc: 0.9664\n",
      "Epoch 182/200\n",
      "576/576 [==============================] - 0s 749us/step - loss: 0.1067 - acc: 0.9641\n",
      "Epoch 183/200\n",
      "576/576 [==============================] - 0s 818us/step - loss: 0.1060 - acc: 0.9676\n",
      "Epoch 184/200\n",
      "576/576 [==============================] - 0s 665us/step - loss: 0.1055 - acc: 0.9670\n",
      "Epoch 185/200\n",
      "576/576 [==============================] - 0s 658us/step - loss: 0.1057 - acc: 0.9670\n",
      "Epoch 186/200\n",
      "576/576 [==============================] - 0s 672us/step - loss: 0.1067 - acc: 0.9630\n",
      "Epoch 187/200\n",
      "576/576 [==============================] - 0s 653us/step - loss: 0.1052 - acc: 0.9699\n",
      "Epoch 188/200\n",
      "576/576 [==============================] - 0s 681us/step - loss: 0.1060 - acc: 0.9618\n",
      "Epoch 189/200\n",
      "576/576 [==============================] - 0s 672us/step - loss: 0.1044 - acc: 0.9688\n",
      "Epoch 190/200\n",
      "576/576 [==============================] - 0s 695us/step - loss: 0.1045 - acc: 0.9676\n",
      "Epoch 191/200\n",
      "576/576 [==============================] - 0s 698us/step - loss: 0.1044 - acc: 0.9659\n",
      "Epoch 192/200\n",
      "576/576 [==============================] - 0s 738us/step - loss: 0.1049 - acc: 0.9670\n",
      "Epoch 193/200\n",
      "576/576 [==============================] - 0s 719us/step - loss: 0.1046 - acc: 0.9682\n",
      "Epoch 194/200\n",
      "576/576 [==============================] - 0s 661us/step - loss: 0.1045 - acc: 0.9641\n",
      "Epoch 195/200\n",
      "576/576 [==============================] - 0s 681us/step - loss: 0.1040 - acc: 0.9659\n",
      "Epoch 196/200\n",
      "576/576 [==============================] - 0s 675us/step - loss: 0.1037 - acc: 0.9647\n",
      "Epoch 197/200\n",
      "576/576 [==============================] - 0s 717us/step - loss: 0.1041 - acc: 0.9659\n",
      "Epoch 198/200\n",
      "576/576 [==============================] - 0s 667us/step - loss: 0.1041 - acc: 0.9659\n",
      "Epoch 199/200\n",
      "576/576 [==============================] - 0s 679us/step - loss: 0.1055 - acc: 0.9664\n",
      "Epoch 200/200\n",
      "576/576 [==============================] - 0s 832us/step - loss: 0.1032 - acc: 0.9659\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24784504ba8>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "\n",
    "model.fit(X_train, y_train, epochs=200, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192/192 [==============================] - 2s 9ms/step\n",
      "\n",
      "acc: 95.83%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model(?????)\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9.56203163e-01,   2.79161404e-03,   2.95871211e-13],\n",
       "       [  9.50998902e-01,   1.70569727e-03,   4.12668861e-14],\n",
       "       [  9.98900056e-01,   3.12872306e-02,   1.59755369e-11],\n",
       "       [  1.34661784e-02,   9.06583130e-01,   3.40190716e-04],\n",
       "       [  9.56203163e-01,   2.79161404e-03,   2.95871211e-13],\n",
       "       [  9.99887228e-01,   3.36888954e-02,   3.24768776e-13],\n",
       "       [  9.99958754e-01,   1.10707395e-02,   3.67803676e-16],\n",
       "       [  1.18652806e-02,   1.23223811e-01,   9.32618439e-01],\n",
       "       [  7.38432209e-05,   2.32217208e-01,   8.64296436e-01],\n",
       "       [  9.98543739e-01,   4.35443874e-03,   3.89592804e-15],\n",
       "       [  1.57167786e-03,   6.15499496e-01,   4.45810333e-02],\n",
       "       [  9.65727687e-01,   2.02975012e-02,   1.28635069e-09],\n",
       "       [  9.99995351e-01,   9.11552738e-03,   2.86061121e-18],\n",
       "       [  1.18652806e-02,   1.23223811e-01,   9.32618439e-01],\n",
       "       [  1.36150047e-03,   6.56955093e-02,   9.74169850e-01],\n",
       "       [  9.60877419e-01,   4.56569996e-03,   2.12129186e-12],\n",
       "       [  5.85520365e-06,   1.22525338e-02,   9.97852564e-01],\n",
       "       [  9.99959707e-01,   1.82292387e-02,   3.28504619e-15],\n",
       "       [  8.34787643e-05,   9.14194047e-01,   3.25548090e-02],\n",
       "       [  9.99994755e-01,   5.58343483e-03,   3.98987673e-19],\n",
       "       [  1.57167786e-03,   6.15499496e-01,   4.45810333e-02],\n",
       "       [  9.99947786e-01,   4.15291591e-03,   7.15518095e-18],\n",
       "       [  7.38432209e-05,   2.32217208e-01,   8.64296436e-01],\n",
       "       [  1.68408323e-02,   8.83160651e-01,   5.21838432e-04],\n",
       "       [  9.98705268e-01,   7.11462833e-03,   2.79324768e-14],\n",
       "       [  9.37105790e-02,   9.14323986e-01,   1.60153122e-05],\n",
       "       [  9.99947786e-01,   4.15291591e-03,   7.15518095e-18],\n",
       "       [  8.34787643e-05,   9.14194047e-01,   3.25548090e-02],\n",
       "       [  1.57167786e-03,   6.15499496e-01,   4.45810333e-02],\n",
       "       [  9.65830326e-01,   1.23108299e-02,   1.35839215e-10],\n",
       "       [  9.98900056e-01,   3.12872306e-02,   1.59755369e-11],\n",
       "       [  9.99958754e-01,   1.10707395e-02,   3.67803676e-16],\n",
       "       [  9.99947786e-01,   4.15291591e-03,   7.15518095e-18],\n",
       "       [  1.36150047e-03,   6.56955093e-02,   9.74169850e-01],\n",
       "       [  4.47874801e-04,   5.03248513e-01,   5.48540890e-01],\n",
       "       [  1.57167786e-03,   6.15499496e-01,   4.45810333e-02],\n",
       "       [  7.38432209e-05,   2.32217208e-01,   8.64296436e-01],\n",
       "       [  9.99817789e-01,   3.52883562e-02,   9.89971776e-13],\n",
       "       [  9.98362124e-01,   2.66222260e-03,   5.43389900e-16],\n",
       "       [  9.37105790e-02,   9.14323986e-01,   1.60153122e-05],\n",
       "       [  9.65830326e-01,   1.23108299e-02,   1.35839215e-10],\n",
       "       [  9.37105790e-02,   9.14323986e-01,   1.60153122e-05],\n",
       "       [  8.34787643e-05,   9.14194047e-01,   3.25548090e-02],\n",
       "       [  3.28260532e-04,   9.83705401e-01,   9.00337327e-05],\n",
       "       [  1.21810535e-05,   8.65990102e-01,   1.87339947e-01],\n",
       "       [  5.85520365e-06,   1.22525338e-02,   9.97852564e-01],\n",
       "       [  9.98705268e-01,   7.11462833e-03,   2.79324768e-14],\n",
       "       [  7.38432209e-05,   2.32217208e-01,   8.64296436e-01],\n",
       "       [  9.99817789e-01,   3.52883562e-02,   9.89971776e-13],\n",
       "       [  6.88793836e-04,   4.62885648e-02,   9.50336099e-01],\n",
       "       [  9.99942780e-01,   2.65584197e-02,   3.24164591e-14],\n",
       "       [  8.59410193e-06,   7.29686737e-01,   4.18953240e-01],\n",
       "       [  9.99947786e-01,   4.15291591e-03,   7.15518095e-18],\n",
       "       [  9.98362124e-01,   2.66222260e-03,   5.43389900e-16],\n",
       "       [  6.31297225e-05,   1.91353247e-01,   8.98382187e-01],\n",
       "       [  9.60877419e-01,   4.56569996e-03,   2.12129186e-12],\n",
       "       [  9.98734415e-01,   1.17454249e-02,   2.49478552e-13],\n",
       "       [  9.99958754e-01,   1.10707395e-02,   3.67803676e-16],\n",
       "       [  9.99958754e-01,   1.10707395e-02,   3.67803676e-16],\n",
       "       [  1.10971145e-02,   9.96328294e-01,   1.24539056e-07],\n",
       "       [  7.38432209e-05,   2.32217208e-01,   8.64296436e-01],\n",
       "       [  9.98362124e-01,   2.66222260e-03,   5.43389900e-16],\n",
       "       [  9.98362124e-01,   2.66222260e-03,   5.43389900e-16],\n",
       "       [  1.36150047e-03,   6.56955093e-02,   9.74169850e-01],\n",
       "       [  9.50998902e-01,   1.70569727e-03,   4.12668861e-14],\n",
       "       [  9.98705268e-01,   7.11462833e-03,   2.79324768e-14],\n",
       "       [  9.99964595e-01,   2.53442526e-02,   1.06344783e-14],\n",
       "       [  8.34787643e-05,   9.14194047e-01,   3.25548090e-02],\n",
       "       [  9.99942780e-01,   2.65584197e-02,   3.24164591e-14],\n",
       "       [  4.47874801e-04,   5.03248513e-01,   5.48540890e-01],\n",
       "       [  5.85520365e-06,   1.22525338e-02,   9.97852564e-01],\n",
       "       [  9.98362124e-01,   2.66222260e-03,   5.43389900e-16],\n",
       "       [  6.31297225e-05,   1.91353247e-01,   8.98382187e-01],\n",
       "       [  9.99958754e-01,   1.10707395e-02,   3.67803676e-16],\n",
       "       [  7.38432209e-05,   2.32217208e-01,   8.64296436e-01],\n",
       "       [  9.65830326e-01,   1.23108299e-02,   1.35839215e-10],\n",
       "       [  9.37105790e-02,   9.14323986e-01,   1.60153122e-05],\n",
       "       [  9.98543739e-01,   4.35443874e-03,   3.89592804e-15],\n",
       "       [  9.61724222e-01,   7.55005283e-03,   1.89462890e-11],\n",
       "       [  9.50998902e-01,   1.70569727e-03,   4.12668861e-14],\n",
       "       [  3.28260532e-04,   9.83705401e-01,   9.00337327e-05],\n",
       "       [  9.98543739e-01,   4.35443874e-03,   3.89592804e-15],\n",
       "       [  1.36150047e-03,   6.56955093e-02,   9.74169850e-01],\n",
       "       [  1.10971145e-02,   9.96328294e-01,   1.24539056e-07],\n",
       "       [  9.99947786e-01,   4.15291591e-03,   7.15518095e-18],\n",
       "       [  8.59410193e-06,   7.29686737e-01,   4.18953240e-01],\n",
       "       [  9.99947786e-01,   4.15291591e-03,   7.15518095e-18],\n",
       "       [  3.28260532e-04,   9.83705401e-01,   9.00337327e-05],\n",
       "       [  9.37105790e-02,   9.14323986e-01,   1.60153122e-05],\n",
       "       [  9.99947786e-01,   4.15291591e-03,   7.15518095e-18],\n",
       "       [  9.50998902e-01,   1.70569727e-03,   4.12668861e-14],\n",
       "       [  1.57167786e-03,   6.15499496e-01,   4.45810333e-02],\n",
       "       [  9.98362124e-01,   2.66222260e-03,   5.43389900e-16],\n",
       "       [  6.88793836e-04,   4.62885648e-02,   9.50336099e-01],\n",
       "       [  9.60877419e-01,   4.56569996e-03,   2.12129186e-12],\n",
       "       [  9.98874843e-01,   1.91007238e-02,   1.78867956e-12],\n",
       "       [  1.10971145e-02,   9.96328294e-01,   1.24539056e-07],\n",
       "       [  6.31297225e-05,   1.91353247e-01,   8.98382187e-01],\n",
       "       [  8.59410193e-06,   7.29686737e-01,   4.18953240e-01],\n",
       "       [  6.31297225e-05,   1.91353247e-01,   8.98382187e-01],\n",
       "       [  9.99994278e-01,   1.36746326e-02,   2.44586901e-17],\n",
       "       [  6.88793836e-04,   4.62885648e-02,   9.50336099e-01],\n",
       "       [  9.99959707e-01,   1.82292387e-02,   3.28504619e-15],\n",
       "       [  9.99817789e-01,   3.52883562e-02,   9.89971776e-13],\n",
       "       [  6.31297225e-05,   1.91353247e-01,   8.98382187e-01],\n",
       "       [  1.34661784e-02,   9.06583130e-01,   3.40190716e-04],\n",
       "       [  1.34661784e-02,   9.06583130e-01,   3.40190716e-04],\n",
       "       [  9.98543739e-01,   4.35443874e-03,   3.89592804e-15],\n",
       "       [  9.98900056e-01,   3.12872306e-02,   1.59755369e-11],\n",
       "       [  3.63461107e-01,   5.63776135e-01,   1.11187948e-03],\n",
       "       [  9.26639841e-05,   5.95622137e-02,   9.81341362e-01],\n",
       "       [  9.99958754e-01,   1.10707395e-02,   3.67803676e-16],\n",
       "       [  9.65830326e-01,   1.23108299e-02,   1.35839215e-10],\n",
       "       [  5.15672618e-05,   2.41936985e-02,   9.94170189e-01],\n",
       "       [  3.28260532e-04,   9.83705401e-01,   9.00337327e-05],\n",
       "       [  9.99959707e-01,   1.82292387e-02,   3.28504619e-15],\n",
       "       [  9.98734415e-01,   1.17454249e-02,   2.49478552e-13],\n",
       "       [  9.99953508e-01,   6.78623468e-03,   5.13001137e-17],\n",
       "       [  9.61724222e-01,   7.55005283e-03,   1.89462890e-11],\n",
       "       [  9.98900056e-01,   3.12872306e-02,   1.59755369e-11],\n",
       "       [  1.68408323e-02,   8.83160651e-01,   5.21838432e-04],\n",
       "       [  1.34661784e-02,   9.06583130e-01,   3.40190716e-04],\n",
       "       [  9.98874843e-01,   1.91007238e-02,   1.78867956e-12],\n",
       "       [  4.47874801e-04,   5.03248513e-01,   5.48540890e-01],\n",
       "       [  1.10971145e-02,   9.96328294e-01,   1.24539056e-07],\n",
       "       [  9.65830326e-01,   1.23108299e-02,   1.35839215e-10],\n",
       "       [  9.99987841e-01,   1.93685796e-02,   4.41366899e-16],\n",
       "       [  8.34787643e-05,   9.14194047e-01,   3.25548090e-02],\n",
       "       [  4.47874801e-04,   5.03248513e-01,   5.48540890e-01],\n",
       "       [  1.18652806e-02,   1.23223811e-01,   9.32618439e-01],\n",
       "       [  9.65727687e-01,   2.02975012e-02,   1.28635069e-09],\n",
       "       [  9.98874843e-01,   1.91007238e-02,   1.78867956e-12],\n",
       "       [  4.47874801e-04,   5.03248513e-01,   5.48540890e-01],\n",
       "       [  9.50998902e-01,   1.70569727e-03,   4.12668861e-14],\n",
       "       [  5.15672618e-05,   2.41936985e-02,   9.94170189e-01],\n",
       "       [  1.21810535e-05,   8.65990102e-01,   1.87339947e-01],\n",
       "       [  5.15672618e-05,   2.41936985e-02,   9.94170189e-01],\n",
       "       [  9.99942780e-01,   2.65584197e-02,   3.24164591e-14],\n",
       "       [  9.99958754e-01,   1.10707395e-02,   3.67803676e-16],\n",
       "       [  4.47874801e-04,   5.03248513e-01,   5.48540890e-01],\n",
       "       [  9.98362124e-01,   2.66222260e-03,   5.43389900e-16],\n",
       "       [  1.34661784e-02,   9.06583130e-01,   3.40190716e-04],\n",
       "       [  5.15672618e-05,   2.41936985e-02,   9.94170189e-01],\n",
       "       [  6.31297225e-05,   1.91353247e-01,   8.98382187e-01],\n",
       "       [  5.85520365e-06,   1.22525338e-02,   9.97852564e-01],\n",
       "       [  9.56203163e-01,   2.79161404e-03,   2.95871211e-13],\n",
       "       [  8.59410193e-06,   7.29686737e-01,   4.18953240e-01],\n",
       "       [  9.61724222e-01,   7.55005283e-03,   1.89462890e-11],\n",
       "       [  9.99942780e-01,   2.65584197e-02,   3.24164591e-14],\n",
       "       [  9.99817789e-01,   3.52883562e-02,   9.89971776e-13],\n",
       "       [  9.61724222e-01,   7.55005283e-03,   1.89462890e-11],\n",
       "       [  9.37105790e-02,   9.14323986e-01,   1.60153122e-05],\n",
       "       [  9.99959707e-01,   1.82292387e-02,   3.28504619e-15],\n",
       "       [  9.98734415e-01,   1.17454249e-02,   2.49478552e-13],\n",
       "       [  9.99817789e-01,   3.52883562e-02,   9.89971776e-13],\n",
       "       [  9.99942780e-01,   2.65584197e-02,   3.24164591e-14],\n",
       "       [  1.36150047e-03,   6.56955093e-02,   9.74169850e-01],\n",
       "       [  1.19335122e-01,   9.77743864e-01,   1.38873372e-06],\n",
       "       [  1.36150047e-03,   6.56955093e-02,   9.74169850e-01],\n",
       "       [  5.65727711e-01,   9.71984684e-01,   1.28518082e-07],\n",
       "       [  1.21810535e-05,   8.65990102e-01,   1.87339947e-01],\n",
       "       [  9.98705268e-01,   7.11462833e-03,   2.79324768e-14],\n",
       "       [  1.34661784e-02,   9.06583130e-01,   3.40190716e-04],\n",
       "       [  9.98874843e-01,   1.91007238e-02,   1.78867956e-12],\n",
       "       [  9.99959707e-01,   1.82292387e-02,   3.28504619e-15],\n",
       "       [  9.61724222e-01,   7.55005283e-03,   1.89462890e-11],\n",
       "       [  9.99958754e-01,   1.10707395e-02,   3.67803676e-16],\n",
       "       [  4.47874801e-04,   5.03248513e-01,   5.48540890e-01],\n",
       "       [  9.99987841e-01,   1.93685796e-02,   4.41366899e-16],\n",
       "       [  9.99942780e-01,   2.65584197e-02,   3.24164591e-14],\n",
       "       [  9.99942780e-01,   2.65584197e-02,   3.24164591e-14],\n",
       "       [  1.68408323e-02,   8.83160651e-01,   5.21838432e-04],\n",
       "       [  9.98734415e-01,   1.17454249e-02,   2.49478552e-13],\n",
       "       [  9.56203163e-01,   2.79161404e-03,   2.95871211e-13],\n",
       "       [  9.60877419e-01,   4.56569996e-03,   2.12129186e-12],\n",
       "       [  9.56203163e-01,   2.79161404e-03,   2.95871211e-13],\n",
       "       [  6.31297225e-05,   1.91353247e-01,   8.98382187e-01],\n",
       "       [  9.65830326e-01,   1.23108299e-02,   1.35839215e-10],\n",
       "       [  1.19335122e-01,   9.77743864e-01,   1.38873372e-06],\n",
       "       [  9.99947786e-01,   4.15291591e-03,   7.15518095e-18],\n",
       "       [  9.65830326e-01,   1.23108299e-02,   1.35839215e-10],\n",
       "       [  1.68408323e-02,   8.83160651e-01,   5.21838432e-04],\n",
       "       [  9.56203163e-01,   2.79161404e-03,   2.95871211e-13],\n",
       "       [  9.99942780e-01,   2.65584197e-02,   3.24164591e-14],\n",
       "       [  4.47874801e-04,   5.03248513e-01,   5.48540890e-01],\n",
       "       [  9.99817789e-01,   3.52883562e-02,   9.89971776e-13],\n",
       "       [  9.99947786e-01,   4.15291591e-03,   7.15518095e-18],\n",
       "       [  9.60877419e-01,   4.56569996e-03,   2.12129186e-12],\n",
       "       [  9.61724222e-01,   7.55005283e-03,   1.89462890e-11],\n",
       "       [  1.21810535e-05,   8.65990102e-01,   1.87339947e-01],\n",
       "       [  9.98874843e-01,   1.91007238e-02,   1.78867956e-12],\n",
       "       [  4.47874801e-04,   5.03248513e-01,   5.48540890e-01]], dtype=float32)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict = model.predict(X_test)\n",
    "y_predict # the result depends on funciton in last layer( this case is Sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted = np.argmax(y_predict, axis=1)\n",
    "\n",
    "true =y_test.values\n",
    "true = np.argmax(true, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 0, 0, 2, 2, 0, 1, 0, 0, 2, 2, 0, 2, 0, 1, 0, 1, 0, 2,\n",
       "       1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 2, 1, 1, 2, 0, 0, 1, 0, 1, 1, 1, 2, 2,\n",
       "       0, 2, 0, 2, 0, 1, 0, 0, 2, 0, 0, 0, 0, 1, 1, 0, 0, 2, 0, 0, 0, 1, 0,\n",
       "       2, 2, 0, 2, 0, 2, 0, 1, 0, 0, 0, 1, 0, 2, 1, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "       0, 2, 0, 0, 1, 2, 1, 2, 0, 2, 0, 0, 2, 1, 1, 0, 0, 0, 2, 0, 0, 2, 1,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 2, 2, 0, 0, 2, 0, 2, 2, 2, 0,\n",
       "       0, 1, 0, 1, 2, 2, 2, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 1, 2, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 1, 0, 0, 1, 1, 0,\n",
       "       2, 0, 0, 0, 0, 2, 0, 2], dtype=int64)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 0, 2, 2, 0, 1, 0, 0, 2, 2, 0, 2, 0, 1, 0, 1, 0, 2,\n",
       "       1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 2, 2, 1, 2, 0, 0, 1, 0, 1, 1, 1, 1, 2,\n",
       "       0, 2, 0, 2, 0, 1, 0, 0, 2, 0, 0, 0, 0, 1, 2, 0, 0, 2, 0, 0, 0, 1, 0,\n",
       "       2, 2, 0, 2, 0, 2, 0, 1, 0, 0, 0, 1, 0, 2, 1, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "       0, 2, 0, 0, 1, 2, 1, 2, 0, 2, 0, 0, 2, 1, 1, 0, 0, 1, 2, 0, 0, 2, 1,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 2, 1, 0, 0, 1, 2, 2, 0, 0, 2, 0, 2, 1, 2, 0,\n",
       "       0, 2, 0, 1, 2, 2, 2, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 1, 2, 1, 1,\n",
       "       0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 1, 0, 0, 1, 0, 0,\n",
       "       2, 0, 0, 0, 0, 1, 0, 2], dtype=int64)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94270833333333337"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(true, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(predicted,columns=['Energy_Efficiency'])\n",
    "df[df==0]='low'\n",
    "df[df==1]='average'\n",
    "df[df==2]='high'\n",
    "result = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Energy_Efficiency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>average</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Energy_Efficiency\n",
       "0               low\n",
       "1               low\n",
       "2               low\n",
       "3           average\n",
       "4               low"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support.' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option)\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width, fig.canvas.height);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to  previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overriden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        event.shiftKey = false;\n",
       "        // Send a \"J\" for go to next cell\n",
       "        event.which = 74;\n",
       "        event.keyCode = 74;\n",
       "        manager.command_mode();\n",
       "        manager.handle_keydown(event);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"640\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(array([  46.,    0.,    0.,    0.,    0.,   43.,    0.,    0.,    0.,  103.]),\n",
       " array([ 0. ,  0.2,  0.4,  0.6,  0.8,  1. ,  1.2,  1.4,  1.6,  1.8,  2. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.hist(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IV: SUMMARY\n",
    "\n",
    "### (1) Regression in Deep learning ( part I)\n",
    "\n",
    "- After GridSearchCV for the best parameters, test scores are  0.94 in heating load and 0.92 in cooling load.\n",
    "- Keras model has generalized well in both loads for the purpose of prediction.\n",
    "\n",
    "### (2) Overall Performance in R-Squared ( part II)\n",
    "\n",
    "##### Best Performer\n",
    "- Random Forests demonstrates test scores of 0.9974 in heating load and 0.9729 in cooling load.\n",
    "\n",
    "##### Medium Performer( runner-up)\n",
    "- Decision Tree scores 0.9971 in heating load and 0.9723 in cooling load.\n",
    "\n",
    "##### Low Performers\n",
    "- Polynomial Lasso scores 0.7792 in heating load and 0.7798 in cooling load.\n",
    "\n",
    "##### Bagging/ Boosting Ensemble\n",
    "- Boosting improve the most out of used models as a whole. In contrast, Bagging messes up the scores from base regressors.\n",
    "- Adaboost has improved R-squared of cooling load in KNN, Linear Ridge, Linear Lasso. Yet It messes up heating load.\n",
    "\n",
    "\n",
    "### (3) Classification in Deep learning ( part III)\n",
    "\n",
    "- Accuracy of test score is 95.83%. However, there is a small difference in using dummy variables to score 94.27%.\n",
    "- From the frequency chart of classification, more than half of the building models are of medium efficiency ( more than 100).\n",
    "- The rest of these instances are equally distributed between low and high groups of efficiency ( about 45 each)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
